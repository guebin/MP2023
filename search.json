[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "기계학습활용 (2023)",
    "section": "",
    "text": "질문하는 방법\n\n카카오톡: 질문하러 가기 // 학기종료이후 폐쇄함\n이메일: guebin@jbnu.ac.kr\n직접방문: 자연과학대학 본관 205호\nZoom: 카카오톡이나 이메일로 미리 시간을 정할 것\nLMS쪽지: https://ieilms.jbnu.ac.kr/\n\nreferences\n공지사항\n\n강의영상의 경우 시간이 지나면 고화질로 시청할 수 있습니다. (인코딩중에는 유튜브에서 저화질로 송출됩니다)\n\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 5, 2023\n\n\n01wk-001: 캐글소개\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-002: 타이타닉, 데이터의 이해\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-003: 타이타닉, 첫 제출\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-004: 타이타닉, Alexis Cook의 코드\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-005: 타이타닉, Autogluon\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-006: 타이타닉, Autogluon (Fsize)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-007: 타이타닉, Autogluon (Fsize,Drop)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-008: 타이타닉, Autogluon (best_quality)\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-009: 아이스크림, 회귀분석\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-010: 아이스크림(초코/바닐라), 회귀분석\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-011: Medical Cost, 회귀분석\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-012: 취업, 로지스틱\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-013: 타이타닉, 로지스틱\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-014: 결측치 시각화, msno\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-015: 결측치 처리, sklearn.impute\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-016: 타이타닉, 결측치처리+로지스틱\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-017: 취업, 로지스틱을 더 깊게\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-018: Predictor 깊은 이해 + 기호정리\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-019: MinMaxScaler를 이용한 전처리\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-020: StaStandardScaler를 이용한 전처리\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-021: 취업, 오버피팅\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-022: 취업+각종영어점수, 다중공선성\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-023: 취업+각종영어점수, Lasso\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-024: 취업+각종영어점수, LassoCV\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "offline/230907.html",
    "href": "offline/230907.html",
    "title": "2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github",
    "section": "",
    "text": "- 윈도우 컴퓨터인 경우 새로운 계정을 만들어 준다.\n\n한글계정명일 경우는 필수\n영어계정이더라도 계정을 분리하는게 관리에 유리\n\n- 최종적으로는\n\ngithub 로그인 + repository 생성완료\nanaconda 설치완료\ngit 설치완료\nquarto 설치완료\n\n이어야 한다.\n\n\n\ngithub 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리.\n\n\n\n\n\n아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n그림1그림2\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨\n\n\n\n\n\n\n\n\n\ngit 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시\n\n\n\n\n\n\nquarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "offline/230907.html#a.-github",
    "href": "offline/230907.html#a.-github",
    "title": "2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github",
    "section": "",
    "text": "github 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리."
  },
  {
    "objectID": "offline/230907.html#b.-anaconda",
    "href": "offline/230907.html#b.-anaconda",
    "title": "2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github",
    "section": "",
    "text": "아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n그림1그림2\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨"
  },
  {
    "objectID": "offline/230907.html#c.-git",
    "href": "offline/230907.html#c.-git",
    "title": "2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github",
    "section": "",
    "text": "git 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "offline/230907.html#d.-quarto",
    "href": "offline/230907.html#d.-quarto",
    "title": "2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github",
    "section": "",
    "text": "quarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "offline/230926.html",
    "href": "offline/230926.html",
    "title": "2023-09-26 – 코드스페이스, 리눅스명령어, kaggle.json",
    "section": "",
    "text": "1. 코드스페이스\n- 장점:\n\n슈퍼장점: 자동으로 깃헙연결 되어있음 (요즘 깃험쓰기 짜증나서 이게 장점임)\n로칼컴퓨터대비 장점: 내 컴퓨터 깨끗하게 쓸 수 있음 + 내 맘대로 무한 리눅스서버 사용가능\n코랩/kaggle 대비 장점: 한번 정리된 셋팅이 영구적으로 적용됨. (autogluon 맨날 안깔아도 된다)\n\n- 단점:\n\n컴터가 별로임, 사용시간 제한있음 (월 100시간?)\n단점을 극복하려면 결재해야함\n\n- 초기설정: 복붙설정하기, 주피터 extension 설치\n\n\n2. kaggle.json\n\n\n3. 간단한 리눅스 명령어\n- 이동: ls, cd\n- 폴더생성 및 삭제: mkdir, rm -rf\n- 복붙: cp/mv\n- zip압출풀기: unzip\n\n리눅스에 익숙해지면 코랩, 캐글등을 쉽게 사용할 수 있다.\n\n\n코랩 혹은 캐글에서 사용하지 않더라도, 코랩 혹은 캐글에서 사용하는 코드를 쉽게 해석할 수 있다.\n\n\n\n4. 타이타닉 데이터 불러오기"
  },
  {
    "objectID": "posts/03wk-013.html#a.-데이터-정리",
    "href": "posts/03wk-013.html#a.-데이터-정리",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "A. 데이터 정리",
    "text": "A. 데이터 정리\n\nX = pd.get_dummies(df_train.drop(['PassengerId','Survived'],axis=1))\ny = df_train[['Survived']]"
  },
  {
    "objectID": "posts/03wk-013.html#b.-predictor-생성",
    "href": "posts/03wk-013.html#b.-predictor-생성",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()\npredictr \n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-013.html#c.-학습-fit-learn",
    "href": "posts/03wk-013.html#c.-학습-fit-learn",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
  },
  {
    "objectID": "posts/03wk-013.html#a.-데이터정리",
    "href": "posts/03wk-013.html#a.-데이터정리",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "A. 데이터정리",
    "text": "A. 데이터정리\n\nX = pd.get_dummies(df_train[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]])\ny = df_train[[\"Survived\"]]"
  },
  {
    "objectID": "posts/03wk-013.html#b.-predictor-생성-1",
    "href": "posts/03wk-013.html#b.-predictor-생성-1",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()"
  },
  {
    "objectID": "posts/03wk-013.html#c.-학습",
    "href": "posts/03wk-013.html#c.-학습",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X, y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-013.html#d.-예측",
    "href": "posts/03wk-013.html#d.-예측",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "D. 예측",
    "text": "D. 예측\n\n#predictr.predict(X)\ndf_train.assign(Survived_hat=predictr.predict(X)).loc[:,['Survived','Survived_hat']]\n\n\n\n\n\n\n\n\nSurvived\nSurvived_hat\n\n\n\n\n0\n0\n0\n\n\n1\n1\n1\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n0\n0\n\n\n...\n...\n...\n\n\n886\n0\n0\n\n\n887\n1\n1\n\n\n888\n0\n1\n\n\n889\n1\n0\n\n\n890\n0\n0\n\n\n\n\n891 rows × 2 columns"
  },
  {
    "objectID": "posts/03wk-013.html#e.-평가",
    "href": "posts/03wk-013.html#e.-평가",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.8002244668911336"
  },
  {
    "objectID": "posts/03wk-012.html#a.-데이터-정리",
    "href": "posts/03wk-012.html#a.-데이터-정리",
    "title": "03wk-012: 취업, 로지스틱",
    "section": "A. 데이터 정리",
    "text": "A. 데이터 정리\n\nX = pd.get_dummies(df[['toeic','gpa']])\ny = df[['employment']]\n\n\nX\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n...\n...\n...\n\n\n495\n280\n4.288465\n\n\n496\n310\n2.601212\n\n\n497\n225\n0.042323\n\n\n498\n320\n1.041416\n\n\n499\n375\n3.626883\n\n\n\n\n500 rows × 2 columns"
  },
  {
    "objectID": "posts/03wk-012.html#b.-predictor-starstarstar",
    "href": "posts/03wk-012.html#b.-predictor-starstarstar",
    "title": "03wk-012: 취업, 로지스틱",
    "section": "B. Predictor (\\(\\star\\star\\star\\))",
    "text": "B. Predictor (\\(\\star\\star\\star\\))\n- 여기가 중요함. \\(y\\)가 연속형이 아니라 범주형으로 이루어진 경우는 sklearn.linear_model.LogisticRegression() 이용하여 predictor를 만들 것\n\npredictr = sklearn.linear_model.LogisticRegression()"
  },
  {
    "objectID": "posts/03wk-012.html#c.-학습",
    "href": "posts/03wk-012.html#c.-학습",
    "title": "03wk-012: 취업, 로지스틱",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\n/home/cgb2/anaconda3/envs/r/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-012.html#d.-예측",
    "href": "posts/03wk-012.html#d.-예측",
    "title": "03wk-012: 취업, 로지스틱",
    "section": "D. 예측",
    "text": "D. 예측\n\npredictr.predict(X) \n\narray([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1])"
  },
  {
    "objectID": "posts/03wk-012.html#e.-평가",
    "href": "posts/03wk-012.html#e.-평가",
    "title": "03wk-012: 취업, 로지스틱",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.882\n\n\n\n(predictr.predict(X) == y.employment).mean()\n\n0.882\n\n\n\nplt.plot(df.toeic,df.gpa,'o')\ndf_filtered = df[predictr.predict(X)==1]\nplt.plot(df_filtered.toeic,df_filtered.gpa,'o') \n\n\n\n\n\n이 정도면 합리적임"
  },
  {
    "objectID": "posts/01wk-002.html#a.-gender_submission.csv",
    "href": "posts/01wk-002.html#a.-gender_submission.csv",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "A. gender_submission.csv",
    "text": "A. gender_submission.csv\n- 왜 여자만 살까?\n\n\n\n\n\n\nGTP 답변\n\n\n\ngender_submission.csv파일은 단순히 모든 여성 승객이 생존한다고 가정한 예측을 담은 파일로, 실제 데이터에 기반한 예측이 아닙니다. 이 파일은 제출 파일의 형식과 구성을 보여주기 위한 예시입니다. 실제로는 성별이나 다른 특성에 따라 생존 여부가 결정되는 것이 아닌, 여러 다양한 변수들이 생존에 영향을 미치게 됩니다.\n실제로 타이타닉호에서는 여성들이 생존율이 높았습니다. 타이타닉호 침몰 사건 당시 여성과 아이들을 먼저 구조하고 구명보트에 태우는 등의 우선순위가 있었기 때문입니다. 따라서 “gender_submission.csv” 파일은 예시로서 제공되었고, 실제 예측 모델을 개발할 때에는 여러 다른 변수들도 고려하여 예측해야 합니다."
  },
  {
    "objectID": "posts/01wk-002.html#b.-embarked",
    "href": "posts/01wk-002.html#b.-embarked",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "B. embarked",
    "text": "B. embarked\n- 항구는 승객의 배경과 사회적 상황을 반영할 수 있다.\n\n\n\n\n\n\nGPT 답변\n\n\n\n네, 항구별로 승객들의 배경과 사회적 상황이 다를 수 있습니다. 다양한 항구에서 탑승한 승객들은 지역적, 경제적, 문화적인 차이를 반영할 수 있습니다. 몇 가지 예시를 들어보겠습니다:\n\n경제적 차이: 서로 다른 항구에서 탑승한 승객들은 다른 경제적 배경을 가지고 있을 수 있습니다. 상류층 승객이나 부유한 승객들이 탑승한 항구일수록 1등급 티켓을 선택한 경우가 많을 것입니다.\n문화적 차이: 다른 항구에서 탑승한 승객들은 다른 문화적 배경을 가지고 있을 수 있습니다. 이로 인해 승객들의 습관, 언어, 관습 등이 다를 수 있습니다.\n지역적 차이: 항구의 위치와 지역적 특성에 따라 승객들의 배경이 달라질 수 있습니다. 해당 지역의 주요 산업이나 경제 활동에 따라 승객들의 직업이나 사회적 지위가 다를 수 있습니다.\n가족 관계: 특정 항구에서 가족들이 함께 탑승한 경우, 해당 지역의 가족 구조와 관련된 차이가 반영될 수 있습니다.\n\n이러한 다양한 차이를 분석하여 항구별로 생존율이나 다른 변수들과의 상관 관계를 파악할 수 있습니다. 이는 모델링 과정에서 추가적인 특성 엔지니어링이나 분석에 활용될 수 있습니다.\n\n\n- 항구는 국적에 대한 정보를 내포할 수도 있다.\n\n\n\n\n\n\nGPT 답변\n\n\n\n타이타닉호의 데이터에서는 승객들의 국적에 대한 직접적인 정보는 제공되지 않습니다. 따라서 승객들의 국적에 대한 정보를 확인하는 것은 어렵습니다. 그러나 항구별로 승객들의 출발지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수는 있습니다.\n예를 들어, Cherbourg 항구가 프랑스에 위치하며, Queenstown 항구가 아일랜드에 위치해 있습니다. 따라서 해당 항구에서 탑승한 승객들 중 일부는 해당 국가의 국적을 가질 가능성이 높을 것입니다. 그러나 국적은 항상 탑승 항구와 일치하지 않을 수 있으며, 탑승한 승객들의 다양한 출신지와 배경을 고려해야 합니다.\n요약하면, 타이타닉호 데이터에서는 직접적인 국적 정보는 제공되지 않지만, 항구별로 승객들의 출신지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수 있습니다."
  },
  {
    "objectID": "posts/01wk-002.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "href": "posts/01wk-002.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "C. sibsp와 parch에 대한 피처엔지니어링 아이디어",
    "text": "C. sibsp와 parch에 대한 피처엔지니어링 아이디어\n- 두 변수 sibsp와 parch를 합쳐 family_size라는 하나의 변수를 만들고, 이를 활용하여 y를 예측할 수 있다.\n\n데이터를 봐야 하겠지만 가족단위로 보트를 줬을 확률도 큼\n가족단위로 보트를 실제로 주었을 경우 family_size라는 변수는 매우 의미있는 해석이 됨\n\n\n\n\n\n\n\nGPT 답변\n\n\n\n가족 규모 특성 생성: ‘sibsp’와 ’parch’ 변수를 활용하여 승객의 가족 규모를 나타내는 새로운 변수를 만들 수 있습니다. 예를 들어, ‘sibsp’와 ’parch’ 값을 더한 후 1을 더해 ‘family_size’ 변수로 만들 수 있습니다. 이를 통해 가족 규모가 큰 승객이 생존 여부에 영향을 미칠 수 있는지 확인할 수 있습니다."
  },
  {
    "objectID": "posts/01wk-002.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "href": "posts/01wk-002.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "D. embarked와 class에 대한 피처엔지니어링의 아이디어",
    "text": "D. embarked와 class에 대한 피처엔지니어링의 아이디어\n- 경우에 따라서 두가지 범주를 하나도 합쳐서 만든 새로운 변수가 매우 의미있을 수 있음\n\n경우에 따라서 특정 국적 (프랑스)의 1등급 손님이 구명보트를 많이 얻었을 수도 있음.\n이럴경우 embarked와 class를 독립적으로 생각하는것보다 둘을 합친 변수가 훨씬 유의미한 변수임\n\n\n\n\n\n\n\nGPT 답변\n\n\n\n물론, 자세한 설명을 드리겠습니다. “항구와 클래스의 조합 특성 생성”이라는 개념은 ‘embarked’ (탑승 항구)와 ‘pclass’ (티켓 클래스) 두 변수를 활용하여 새로운 특성을 만들어 분석하는 것을 의미합니다.\n예를 들어, “embarked” 변수에는 ‘C’, ‘Q’, ‘S’와 같은 항구 정보가 있으며, “pclass” 변수에는 ’1’, ‘2’, ’3’과 같은 티켓 클래스 정보가 있습니다. 이 두 변수를 조합하여 새로운 변수를 만들어보겠습니다.\n생성할 새로운 변수의 예시:\n“embarked_pclass”: ‘C_1’, ‘C_2’, ‘C_3’, ‘Q_1’, ‘Q_2’, ‘Q_3’, ‘S_1’, ‘S_2’, ‘S_3’ 위와 같이 “embarked”와 “pclass”를 조합하여 새로운 변수 값을 만듭니다. 예를 들어 ’embarked’가 ’C’이고 ’pclass’가 ’1’인 경우, “embarked_pclass” 변수 값은 ’C_1’이 됩니다. 이렇게 조합된 변수는 해당 승객이 어떤 항구에서 탑승했고 어떤 티켓 클래스를 사용했는지를 동시에 나타내는 특성입니다.\n이렇게 생성된 “embarked_pclass” 변수를 사용하여 특정 항구와 클래스의 조합이 생존 여부에 어떤 영향을 미치는지를 분석할 수 있습니다. 예를 들어, ‘C_1’ 조합이 생존율과 관련이 있는지, ‘Q_3’ 조합이 생존 여부에 영향을 미치는지 등을 확인하여 해당 특성이 모델링에 유용한 정보를 제공하는지 알아볼 수 있습니다.\n\n\n\n\n\n\n\n\nGPT 답변 2 – 그냥 “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것과 무슨차이냐는 질문에 대한 답변\n\n\n\n네, “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것도 가능합니다. 그러나 새로운 변수를 만드는 것은 데이터를 조금 더 유용하게 활용하는 방법 중 하나입니다. 여기에 몇 가지 이유가 있습니다:\n상호작용 효과 강조: 새로운 변수를 만들면 “embarked”와 “pclass”의 조합이 생존 여부에 미치는 영향을 강조할 수 있습니다. 단순히 각각의 변수를 개별적으로 사용하는 것보다 조합된 변수를 통해 서로 간의 상호작용 효과를 쉽게 확인할 수 있습니다.\n더 풍부한 패턴 탐색: 새로운 변수를 만들면 서로 다른 항구와 클래스 조합들 간의 생존률 차이를 더 세부적으로 분석할 수 있습니다. 이를 통해 특정 항구와 클래스의 승객들이 생존 여부에 미치는 영향을 더 자세히 이해할 수 있습니다.\n다양한 조합 고려: 조합된 변수를 만들면 “embarked”와 “pclass”의 가능한 모든 조합을 커버할 수 있습니다. 이는 분석의 전체적인 시각을 확장하고, 어떤 조합이 중요한지를 더 효과적으로 파악할 수 있도록 도와줍니다.\n모델 학습에 더 적합한 데이터 생성: 몇몇 머신 러닝 알고리즘은 상호작용 변수를 활용하여 모델의 성능을 향상시킬 수 있습니다. 이러한 알고리즘들은 새로운 변수를 활용하여 더 복잡한 패턴을 학습할 수 있습니다.\n새로운 변수를 만들어 분석하는 것은 분석의 창의성과 도메인 지식을 활용하여 데이터로부터 더 많은 의미를 도출하려는 노력의 일부입니다. 분석의 목적에 따라 각 변수를 개별적으로 사용하거나 조합하여 새로운 변수를 만들 수 있습니다."
  },
  {
    "objectID": "posts/01wk-001.html",
    "href": "posts/01wk-001.html",
    "title": "01wk-001: 캐글소개",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. 타이타닉 경진대회 찾기\n- 방법1: 검색을 통하여 접속하기\n\n\n\n그림1: 타이타닉 경진대회 검색\n\n\n- 방법2: 직접주소를 입력 https://www.kaggle.com/competitions/titanic\n\n\n3. 데이터구경\n- 데이터의 설명을 빠르게 파악하는 방법\n\n방법1: 변수위주로 Kaggle 홈페이지에서 파악\n방법2: 구글번역기 활용\n방법3: \bChatGPT이용\n\n- 제 생각\n\nChatGPT가 옳지 않은 소리를 할 떄도 있음. 하지만 처음에 데이터에 대한 개념을 빠르게 정리하고자 할 때 도움이 된다.\n변수이름이 약어로 된 경우가 많은데 이럴경우 GPT가 유용하다.\n\n\n\n4. 메뉴소개\n- 캐글 웹사이트의 메뉴탭 소개\n\nOverview (개요):\n\n\n경진대회 주최자가 경진대회의 배경, 목표, 데이터셋 설명 등을 설명\n\n\nData (데이터):\n\n\n경진대회에 사용되는 데이터셋에 관한 정보를 찾을 수 있음.\n데이터의 구성, 변수 설명, 예시 데이터 등이 제공되며, 데이터를 이해하고 분석할 수 있는데 필요한 정보들이 여기에 포함됨.\n\n\nCode (코드):\n\n\n경진대회 참가자들이 코드를 공유하고 토론하는 공간.\n주로 주어진 문제에 대한 데이터 분석 및 모델링 코드, 데이터 전처리 방법, 모델 학습 등에 관련된 내용이 이곳에 포함됨.\n\n\nDiscussion (토론):\n\n\n참가자들이 서로 의견을 교환하고 질문을 주고받을 수 있는 공간.\n데이터 분석 방법, 모델 구축 전략, 문제 해결 과정 등에 대한 토론이 이루어짐.\n\n\nLeaderboard (리더보드):\n\n\n경진대회 참가자들의 성능 평가 지표와 순위가 나열.\n참가자들의 모델 성능을 비교하고 경쟁 상황을 실시간으로 확인할 수 있음.\n\n\nRules (규칙):\n\n\n참가자들이 따라야 할 규칙, 데이터 사용 방법, 평가 지표 등이 명시되어 있음.\n\n- 체크하면 좋은 것들\n\nOveview: 평가지표(metric), 상(prize), 시간(timeline)\nRules: 평가지표(metric), 외부데이터 사용여부, 하루 최대 제출 수, 최종선택가능한 솔루션 수 (limit)\n\n- 대회의 유형 (ref: https://github.com/PacktPublishing/The-Kaggle-Book)\n\nGetting Started: 상을 제공하지 않음. 튜토리얼용. 매우 협력적인 환경.\nFeatured: 가장 일반적인 유형. 스폰서 회사의 비지니스 관련문제. 상금이 후함. 솔루션을 소개하는 자세한 리포트를 준비해야하고 발표할 것을 요구받을 수 있음.\nAnalytics: 질적 평가. 참가자의 PPT를 제출로 받음."
  },
  {
    "objectID": "posts/02wk-003.html#a.-test-답을-모름-제출해야-알-수-있음",
    "href": "posts/02wk-003.html#a.-test-답을-모름-제출해야-알-수-있음",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "A. test – 답을 모름, 제출해야 알 수 있음",
    "text": "A. test – 답을 모름, 제출해야 알 수 있음\n- 제출결과는 리더보드에서 확인할 수 있음."
  },
  {
    "objectID": "posts/02wk-003.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "href": "posts/02wk-003.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "B. train – 스스로 풀어보고 채점할 수 있음",
    "text": "B. train – 스스로 풀어보고 채점할 수 있음\n- 캐글에서 code \\(\\to\\) New Notebook 클릭\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n\ntr=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntst=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# 예비학습 – accuracy의 계산\n\nimport pandas as pd \n\n\ndf = pd.DataFrame({'Surv':[1,0,1,1,0],'Sex':['f','m','f','m','m']})\n\n- Surv 열의 선택\n\ndf.Surv\n\n0    1\n1    0\n2    1\n3    1\n4    0\nName: Surv, dtype: int64\n\n\n- Sex 열의 선택\n\ndf.Sex\n\n0    f\n1    m\n2    f\n3    m\n4    m\nName: Sex, dtype: object\n\n\n- Sex == f이면 생존(1), 그렇지 않으면 사망(0)이라고 예측\n\n(df.Sex == 'f')*1\n\n0    1\n1    0\n2    1\n3    0\n4    0\nName: Sex, dtype: int64\n\n\n- 결과를 정리하면 아래와 같음\n\npd.DataFrame({'real': df.Surv, 'estimate': (df.Sex == 'f')*1})\n\n\n\n\n\n\n\n\nreal\nestimate\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n1\n0\n\n\n4\n0\n0\n\n\n\n\n\n\n\n- accuracy를 손으로 계산하면 \\(\\frac{4}{5}=0.8\\).\n- 컴퓨터로 accuracy를 계산한다면\n\n(df.Surv == (df.Sex == 'f')*1).sum()/5 # 방법1\n\n0.8\n\n\n\n(df.Surv == (df.Sex == 'f')*1).mean() # 방법2\n\n0.8\n\n\n#\n- 실제자료의 accuracy를 구해보자.\n\ntr = pd.read_csv('./titanic/train.csv')\n\n\n(tr.Survived == (tr.Sex == 'female')).mean()\n\n0.7867564534231201"
  },
  {
    "objectID": "posts/04wk-017.html",
    "href": "posts/04wk-017.html",
    "title": "04wk-017: 취업, 로지스틱을 더 깊게",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\n\n\n\n3. 데이터 불러오기 \\(\\to\\) 학습\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\n\n\n\n\n0\n135\n0.051535\n0\n\n\n1\n935\n0.355496\n0\n\n\n2\n485\n2.228435\n0\n\n\n3\n65\n1.179701\n0\n\n\n4\n445\n3.962356\n1\n\n\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n\n\n496\n310\n2.601212\n1\n\n\n497\n225\n0.042323\n0\n\n\n498\n320\n1.041416\n0\n\n\n499\n375\n3.626883\n1\n\n\n\n\n500 rows × 3 columns\n\n\n\n\nX = df[['toeic','gpa']]\ny = df[['employment']]\npredictr = sklearn.linear_model.LogisticRegression()\npredictr.fit(X,y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n\n4. yhat이 나오는 방식?\n- 확인: 무슨 수식에 의하여 나오긴함\n\npredictr.coef_, predictr.intercept_\n\n(array([[0.00571598, 2.46520018]]), array([-8.45433334]))\n\n\n\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n\n0      0.000523\n1      0.096780\n2      0.453003\n3      0.005627\n4      0.979312\n         ...   \n495    0.976295\n496    0.432939\n497    0.000855\n498    0.016991\n499    0.932777\nLength: 500, dtype: float64\n\n\n\n((v &gt; 0.5) == predictr.predict(X)).mean()\n\n1.0\n\n\n- 하여튼 아래와 같은 구조임\n(구조1)\n\n\n\n\nflowchart LR\n  A[\"predictr (not learned)\"]\n  B[\"predictr (learned)\"]\n  C(\"v (=prob)\")\n  D(\"yhat\")\n  A --&gt; |\"`.fit(X,y)`\"| B --&gt; |\"`????`\" | C --&gt; |\"`v&gt;0.5`\"| D\n\n\n\n\n\n(구조2) – 단순화\n\n\n\n\nflowchart LR\n  A[\"predictr (not learned)\"]\n  B[\"predictr (learned)\"]\n  C(\"yhat\")\n  A --&gt; |\"`.fit(X,y)`\"| B --&gt; |\"`.predict(X)`\" | C\n\n\n\n\n\n- v 값을 알고 싶다면 어쩌지?\n\nv[:5].round(3)\n\n0    0.001\n1    0.097\n2    0.453\n3    0.006\n4    0.979\ndtype: float64\n\n\n\npredictr.predict_proba(X)[:5].round(3) \n\narray([[0.999, 0.001],\n       [0.903, 0.097],\n       [0.547, 0.453],\n       [0.994, 0.006],\n       [0.021, 0.979]])"
  },
  {
    "objectID": "posts/04wk-014.html#a.-df.info",
    "href": "posts/04wk-014.html#a.-df.info",
    "title": "04wk-014: 결측치 시각화, msno",
    "section": "A. df.info()",
    "text": "A. df.info()\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   A       668 non-null    float64\n 1   B       656 non-null    float64\n 2   C       608 non-null    float64\n 3   D       668 non-null    float64\n 4   E       660 non-null    float64\ndtypes: float64(5)\nmemory usage: 39.2 KB"
  },
  {
    "objectID": "posts/04wk-014.html#b.-msno.bar",
    "href": "posts/04wk-014.html#b.-msno.bar",
    "title": "04wk-014: 결측치 시각화, msno",
    "section": "B. msno.bar()",
    "text": "B. msno.bar()\n\nmsno.bar(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-014.html#a.-msno.matrix",
    "href": "posts/04wk-014.html#a.-msno.matrix",
    "title": "04wk-014: 결측치 시각화, msno",
    "section": "A. msno.matrix()",
    "text": "A. msno.matrix()\n\nmsno.matrix(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-014.html#b.-msno.heatmap",
    "href": "posts/04wk-014.html#b.-msno.heatmap",
    "title": "04wk-014: 결측치 시각화, msno",
    "section": "B. msno.heatmap()",
    "text": "B. msno.heatmap()\n\nmsno.heatmap(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-014.html#c.-msno.dendrogram",
    "href": "posts/04wk-014.html#c.-msno.dendrogram",
    "title": "04wk-014: 결측치 시각화, msno",
    "section": "C. msno.dendrogram()",
    "text": "C. msno.dendrogram()\n\nmsno.dendrogram(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/05wk-020.html",
    "href": "posts/05wk-020.html",
    "title": "05wk-020: StaStandardScaler를 이용한 전처리",
    "section": "",
    "text": "1. 강의영상\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.preprocessing \n\n\n\n3. StandardScaler()\n- 예제자료 로드\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 스케일러를 생성\n\nsclr = sklearn.preprocessing.StandardScaler()\nsclr.fit_transform(df)\n\narray([[-0.8680409 , -0.98104887],\n       [ 1.81575704, -0.73905505],\n       [ 0.3061207 ,  0.75205327],\n       [-1.10287322, -0.08287854],\n       [ 0.17193081,  2.13248542],\n       [-1.10287322,  0.44828929],\n       [-0.34805505, -0.77533368],\n       [ 1.12803382, -0.75451182]])\n\n\n- 계산식\n\n(df.toeic - df.toeic.mean())/df.toeic.std(ddof=0) # 계산식\n\n0   -0.868041\n1    1.815757\n2    0.306121\n3   -1.102873\n4    0.171931\n5   -1.102873\n6   -0.348055\n7    1.128034\nName: toeic, dtype: float64\n\n\n\n\n4. 비교\n- MinMaxScaler와 StandardScaler는 데이터의 스케일을 조정하는 두 가지 일반적인 방법이다.\n\nMinMaxScaler:\n\n작동 원리: 데이터를 0과 1 사이의 값으로 조정\n장점: 원하는 범위 내로 데이터를 조정할 때 유용. 특히 신경망에서는 활성화 함수의 범위와 일치하도록 입력 값을 조정하는 데 유용.1\n단점: 이상치에 매우 민감하다. 이상치 때문에 전체 데이터의 스케일이 크게 영향받을 수 있음.\n\nStandardScaler:\n\n작동 원리: 데이터의 평균을 0, 표준편차를 1로 만드는 방식으로 조정.\n장점: 이상치에 MinMaxScaler보다 덜 민감함. 많은 통계적 기법들, 특히 PCA 같은 선형 알고리즘에서 잘 작동함.2\n단점: MinMaxScaler와 달리, 표준화된 데이터의 값이 특정 범위 내에 있음을 보장하지 않음.3\n\n\n1 sigmoid, tanh와 같은 활성화 함수의 출력값과 맞추는 용도2 그야 PCA는 정규분포를 가정하고 만든 알고리즘이라~3 MinMaxScaler도 딱히 엄격하게 보장하는건 아니야- 무식한 설명 (1)\n\nMinMaxScaler: 컴퓨터공학과, 전자공학과 느낌\nStandardScaler: 통계학과 느낌\n\n- 무식한 설명 (2)\n\nMinMaxScaler: 데이터가 기본적으로 0$\\(1 혹은 -1\\)$1 사이의 범위에 있다고 가정한다.\nStandardScaler: 데이터가 기본적으로 정규분포를 따른다고 가정하는 모형들과 잘 맞는다.\n\n- 둘 중 어느 것을 선택할지 결정하기 위한 고려사항:\n\n이상치가 많으면 StandardScaler가 더 적합할 수 있다.\n모델의 알고리즘과 특성에 따라 선택해야 한다. 예를 들어, 신경망은 일반적으로 0과 1 사이의 값이나 -1과 1 사이의 값으로 입력을 받는 활성화 함수를 사용하므로 MinMaxScaler가 적합할 수 있다.\n\n결론적으로, 두 스케일링 방법 중 어느 것이 더 좋은지는 사용 사례와 데이터의 특성에 따라 다르기 때문에, 가능한 경우 둘 다 시도해보고 모델의 성능을 비교하는 것이 좋다.\n- 둘 중 어느 것을 선택할지 결정하기 위한 고려사항 – 무식한 설명\n\n보통은 아무거나 해도 큰일 안남.\n아주 특수한 경우4를 제외하고는 어차피 이론적인 선택기준은 없음.\n\n\n\n4 Classical PCA"
  },
  {
    "objectID": "posts/03wk-011.html#a.-data-정리",
    "href": "posts/03wk-011.html#a.-data-정리",
    "title": "03wk-011: Medical Cost, 회귀분석",
    "section": "A. Data 정리",
    "text": "A. Data 정리\n\ndf.columns\n\nIndex(['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges'], dtype='object')\n\n\n\nX = pd.get_dummies(df.drop(['charges'],axis=1))\ny = df[['charges']]\n\n\nX\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\nsex_female\nsex_male\nsmoker_no\nsmoker_yes\nregion_northeast\nregion_northwest\nregion_southeast\nregion_southwest\n\n\n\n\n0\n19\n27.900\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n1\n18\n33.770\n1\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n28\n33.000\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n33\n22.705\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n32\n28.880\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\n30.970\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1334\n18\n31.920\n0\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1335\n18\n36.850\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1336\n21\n25.800\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1337\n61\n29.070\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n1338 rows × 11 columns\n\n\n\n\ny\n\n\n\n\n\n\n\n\ncharges\n\n\n\n\n0\n16884.92400\n\n\n1\n1725.55230\n\n\n2\n4449.46200\n\n\n3\n21984.47061\n\n\n4\n3866.85520\n\n\n...\n...\n\n\n1333\n10600.54830\n\n\n1334\n2205.98080\n\n\n1335\n1629.83350\n\n\n1336\n2007.94500\n\n\n1337\n29141.36030\n\n\n\n\n1338 rows × 1 columns"
  },
  {
    "objectID": "posts/03wk-011.html#b.-predictor-생성",
    "href": "posts/03wk-011.html#b.-predictor-생성",
    "title": "03wk-011: Medical Cost, 회귀분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression()"
  },
  {
    "objectID": "posts/03wk-011.html#c.-학습",
    "href": "posts/03wk-011.html#c.-학습",
    "title": "03wk-011: Medical Cost, 회귀분석",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-011.html#d.-예측",
    "href": "posts/03wk-011.html#d.-예측",
    "title": "03wk-011: Medical Cost, 회귀분석",
    "section": "D. 예측",
    "text": "D. 예측\n\ndf.assign(yhat = predictr.predict(X))\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\nyhat\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n25293.713028\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n3448.602834\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n6706.988491\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n3754.830163\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n5592.493386\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n12351.323686\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n3511.930809\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n4149.132486\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n1246.584939\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n37085.623268\n\n\n\n\n1338 rows × 8 columns"
  },
  {
    "objectID": "posts/03wk-011.html#e.-평가",
    "href": "posts/03wk-011.html#e.-평가",
    "title": "03wk-011: Medical Cost, 회귀분석",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y) # R^2\n\n0.7509130345985207\n\n\n\n0.7 이상이면 망한모형까지는 아님 (대회용으로는 부적절할 수 있으나 대충 쓸 수는 있는 정도)"
  },
  {
    "objectID": "posts/06wk-023.html",
    "href": "posts/06wk-023.html",
    "title": "05wk-023: 취업+각종영어점수, Lasso",
    "section": "",
    "text": "1. 강의영상\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\n\n\n\n3. Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_manytoeic.csv\")\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n-0.949314\n0.051535\n135\n134.889567\n132.466381\n134.328746\n133.692285\n134.490387\n133.482738\n135.805902\n...\n134.568925\n135.248135\n135.557690\n134.307587\n136.115518\n136.895841\n135.599610\n134.083179\n135.125554\n134.320048\n\n\n1\n4.861725\n0.355496\n935\n933.897710\n934.928312\n935.665096\n934.115679\n934.204277\n935.213653\n935.551083\n...\n932.507220\n934.449154\n935.213485\n935.216839\n935.039467\n935.400347\n934.172674\n935.189259\n936.578955\n937.278620\n\n\n2\n5.582663\n2.228435\n485\n483.020037\n484.161879\n486.041920\n482.954745\n485.891348\n484.849052\n485.410756\n...\n484.863750\n485.516249\n484.523790\n486.036836\n485.011074\n485.637154\n484.001982\n485.544729\n485.815829\n485.042914\n\n\n3\n3.919694\n1.179701\n65\n66.780387\n67.388852\n65.622595\n62.763278\n65.935953\n67.288798\n66.314210\n...\n64.151163\n64.677268\n66.340589\n64.291130\n64.540950\n66.728237\n65.474820\n65.725247\n65.015707\n65.399658\n\n\n4\n8.286851\n3.962356\n445\n444.348116\n445.520801\n446.419087\n442.715198\n445.043974\n444.849464\n446.178144\n...\n444.993117\n444.999768\n445.773163\n444.924819\n442.703987\n445.118233\n445.119561\n446.214723\n444.983397\n444.455462\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n7.002081\n4.288465\n280\n280.520486\n281.060157\n281.291354\n280.816827\n280.867509\n279.748219\n280.956388\n...\n280.864668\n280.261168\n278.871055\n280.267526\n279.792068\n279.931181\n280.606405\n280.710435\n279.602746\n280.472047\n\n\n496\n5.431050\n2.601212\n310\n310.015294\n310.003728\n308.680788\n309.179518\n310.908038\n308.580682\n308.720718\n...\n310.670717\n310.872724\n309.631197\n309.723108\n309.624440\n310.207927\n308.519863\n309.858127\n310.602814\n310.841082\n\n\n497\n0.088812\n0.042323\n225\n225.226422\n225.738373\n223.342848\n226.416036\n223.478492\n225.080199\n224.896846\n...\n223.252383\n226.178697\n225.494945\n225.154573\n225.254684\n224.711397\n225.259744\n223.700222\n225.700224\n225.120175\n\n\n498\n3.806388\n1.041416\n320\n321.135794\n319.880713\n319.759936\n321.071771\n320.085155\n320.657430\n320.802495\n...\n319.875882\n321.006234\n319.054499\n320.498798\n320.407021\n323.002351\n319.496038\n320.011256\n319.304070\n320.418582\n\n\n499\n6.739095\n3.626883\n375\n376.032149\n373.131026\n374.800140\n374.541924\n375.363948\n375.758260\n375.045161\n...\n375.452139\n374.790209\n374.247366\n372.755630\n373.711254\n376.621000\n373.979317\n374.788932\n374.153385\n373.678255\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n\n4. 분석\n- 적용할 방법: Lasso\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n\nX = df_train.drop(['employment_score'],axis=1)\nXX = df_test.drop(['employment_score'],axis=1)\ny = df_train[['employment_score']]\nyy = df_test[['employment_score']]\n\n\nprdtr = sklearn.linear_model.Lasso()\n\n\nprdtr.fit(X,y)\n#prdtr.coef_\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.574e-01, tolerance: 1.988e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso()\n\n\n\nprdtr.score(X,y)\n\n0.7090183447263774\n\n\n\nprdtr.score(XX,yy)\n\n0.7302315058096471\n\n\n\n\n5. Lasso의 비법\n- LinearRegression vs Lasso\n\nprdtr_list = [sklearn.linear_model.LinearRegression(), sklearn.linear_model.Lasso()]\n\n\nfor prdtr in prdtr_list:\n    prdtr.fit(X,y)\n\n\nfig,ax = plt.subplots(2)\nax[0].plot(prdtrs[0].coef_.reshape(-1)[1:],label='Toeic Coefs of LinearRegression',color='C0')\nax[0].legend()\nax[1].plot(prdtrs[1].coef_.reshape(-1)[1:],label='Toeic Coef of Lasso',color='C1')\nax[1].legend()\n\n&lt;matplotlib.legend.Legend at 0x7f11fd15cfd0&gt;\n\n\n\n\n\n- Lasso는 훨씬 Spare한 Coef를 가진다.\n\n(abs(prdtr_list[0].coef_)&gt;0).sum() # 502개 모두 값이 있음..\n\n502\n\n\n\n(abs(prdtr_list[1].coef_)&gt;0).sum() # 대부분0이고 7개정도만 0이 아님\n\n7\n\n\n\n\n6. 하이퍼파라메터\n\nalphas = np.linspace(0,1,100)\nalphas\n\narray([0.        , 0.01010101, 0.02020202, 0.03030303, 0.04040404,\n       0.05050505, 0.06060606, 0.07070707, 0.08080808, 0.09090909,\n       0.1010101 , 0.11111111, 0.12121212, 0.13131313, 0.14141414,\n       0.15151515, 0.16161616, 0.17171717, 0.18181818, 0.19191919,\n       0.2020202 , 0.21212121, 0.22222222, 0.23232323, 0.24242424,\n       0.25252525, 0.26262626, 0.27272727, 0.28282828, 0.29292929,\n       0.3030303 , 0.31313131, 0.32323232, 0.33333333, 0.34343434,\n       0.35353535, 0.36363636, 0.37373737, 0.38383838, 0.39393939,\n       0.4040404 , 0.41414141, 0.42424242, 0.43434343, 0.44444444,\n       0.45454545, 0.46464646, 0.47474747, 0.48484848, 0.49494949,\n       0.50505051, 0.51515152, 0.52525253, 0.53535354, 0.54545455,\n       0.55555556, 0.56565657, 0.57575758, 0.58585859, 0.5959596 ,\n       0.60606061, 0.61616162, 0.62626263, 0.63636364, 0.64646465,\n       0.65656566, 0.66666667, 0.67676768, 0.68686869, 0.6969697 ,\n       0.70707071, 0.71717172, 0.72727273, 0.73737374, 0.74747475,\n       0.75757576, 0.76767677, 0.77777778, 0.78787879, 0.7979798 ,\n       0.80808081, 0.81818182, 0.82828283, 0.83838384, 0.84848485,\n       0.85858586, 0.86868687, 0.87878788, 0.88888889, 0.8989899 ,\n       0.90909091, 0.91919192, 0.92929293, 0.93939394, 0.94949495,\n       0.95959596, 0.96969697, 0.97979798, 0.98989899, 1.        ])\n\n\n\nprdtr_list = [sklearn.linear_model.Lasso(alpha=a) for a in alphas]\nfor prdtr in prdtr_list:\n    prdtr.fit(X,y)\n\n\nnonzero_coefs = [abs(prdtr.coef_&gt;0).sum() for prdtr in prdtr_list] \n\n\nplt.plot(alphas, nonzero_coefs)\n\n\n\n\n\nalpha가 커질수록 0인 coef가 많아짐"
  },
  {
    "objectID": "posts/02wk-008.html#a.-데이터",
    "href": "posts/02wk-008.html#a.-데이터",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")\n\nLoaded data from: ./titanic/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./titanic/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418"
  },
  {
    "objectID": "posts/02wk-008.html#b.-predictor-생성",
    "href": "posts/02wk-008.html#b.-predictor-생성",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230912_120554/\""
  },
  {
    "objectID": "posts/02wk-008.html#c.-적합fit",
    "href": "posts/02wk-008.html#c.-적합fit",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr,presets='best_quality') # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230912_120554/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   293.94 GB / 490.57 GB (59.9%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    125495.28 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.1s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.16s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6319   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6364   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.835    = Validation score   (accuracy)\n    0.46s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    0.65s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8373   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8361   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8608   = Validation score   (accuracy)\n    1.85s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8294   = Validation score   (accuracy)\n    0.58s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8328   = Validation score   (accuracy)\n    0.3s     = Training   runtime\n    0.07s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8507   = Validation score   (accuracy)\n    2.85s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    1.09s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    4.25s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    1.36s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8608   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 23.18s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230912_120554/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f46e583f8e0&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.860831       0.026401  1.851676                0.026401           1.851676            1       True          7\n1       WeightedEnsemble_L2   0.860831       0.027539  2.251745                0.001138           0.400069            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.850730       0.093112  2.852662                0.093112           2.852662            1       True         10\n3      LightGBMLarge_BAG_L1   0.840629       0.029128  1.360948                0.029128           1.360948            1       True         13\n4           LightGBM_BAG_L1   0.840629       0.031956  0.654864                0.031956           0.654864            1       True          4\n5            XGBoost_BAG_L1   0.840629       0.048379  1.094147                0.048379           1.094147            1       True         11\n6     NeuralNetTorch_BAG_L1   0.840629       0.102560  4.247738                0.102560           4.247738            1       True         12\n7   RandomForestGini_BAG_L1   0.837262       0.088506  0.341183                0.088506           0.341183            1       True          5\n8   RandomForestEntr_BAG_L1   0.836139       0.081470  0.341543                0.081470           0.341543            1       True          6\n9         LightGBMXT_BAG_L1   0.835017       0.027167  0.462362                0.027167           0.462362            1       True          3\n10    ExtraTreesEntr_BAG_L1   0.832772       0.073440  0.297061                0.073440           0.297061            1       True          9\n11    ExtraTreesGini_BAG_L1   0.829405       0.097789  0.580898                0.097789           0.580898            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.001740  0.003219                0.001740           0.003219            1       True          2\n13    KNeighborsUnif_BAG_L1   0.631874       0.001945  0.003195                0.001945           0.003195            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.860831\n0.026401\n1.851676\n0.026401\n1.851676\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.860831\n0.027539\n2.251745\n0.001138\n0.400069\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.850730\n0.093112\n2.852662\n0.093112\n2.852662\n1\nTrue\n10\n\n\n3\nLightGBMLarge_BAG_L1\n0.840629\n0.029128\n1.360948\n0.029128\n1.360948\n1\nTrue\n13\n\n\n4\nLightGBM_BAG_L1\n0.840629\n0.031956\n0.654864\n0.031956\n0.654864\n1\nTrue\n4\n\n\n5\nXGBoost_BAG_L1\n0.840629\n0.048379\n1.094147\n0.048379\n1.094147\n1\nTrue\n11\n\n\n6\nNeuralNetTorch_BAG_L1\n0.840629\n0.102560\n4.247738\n0.102560\n4.247738\n1\nTrue\n12\n\n\n7\nRandomForestGini_BAG_L1\n0.837262\n0.088506\n0.341183\n0.088506\n0.341183\n1\nTrue\n5\n\n\n8\nRandomForestEntr_BAG_L1\n0.836139\n0.081470\n0.341543\n0.081470\n0.341543\n1\nTrue\n6\n\n\n9\nLightGBMXT_BAG_L1\n0.835017\n0.027167\n0.462362\n0.027167\n0.462362\n1\nTrue\n3\n\n\n10\nExtraTreesEntr_BAG_L1\n0.832772\n0.073440\n0.297061\n0.073440\n0.297061\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.829405\n0.097789\n0.580898\n0.097789\n0.580898\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.001740\n0.003219\n0.001740\n0.003219\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.631874\n0.001945\n0.003195\n0.001945\n0.003195\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-008.html#d.-예측-predict",
    "href": "posts/02wk-008.html#d.-예측-predict",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.898989898989899\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(best_quality)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/06wk-024.html",
    "href": "posts/06wk-024.html",
    "title": "05wk-024: 취업+각종영어점수, LassoCV",
    "section": "",
    "text": "1. 강의영상\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\n\n\n\n3. Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_manytoeic.csv\")\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n-0.949314\n0.051535\n135\n134.889567\n132.466381\n134.328746\n133.692285\n134.490387\n133.482738\n135.805902\n...\n134.568925\n135.248135\n135.557690\n134.307587\n136.115518\n136.895841\n135.599610\n134.083179\n135.125554\n134.320048\n\n\n1\n4.861725\n0.355496\n935\n933.897710\n934.928312\n935.665096\n934.115679\n934.204277\n935.213653\n935.551083\n...\n932.507220\n934.449154\n935.213485\n935.216839\n935.039467\n935.400347\n934.172674\n935.189259\n936.578955\n937.278620\n\n\n2\n5.582663\n2.228435\n485\n483.020037\n484.161879\n486.041920\n482.954745\n485.891348\n484.849052\n485.410756\n...\n484.863750\n485.516249\n484.523790\n486.036836\n485.011074\n485.637154\n484.001982\n485.544729\n485.815829\n485.042914\n\n\n3\n3.919694\n1.179701\n65\n66.780387\n67.388852\n65.622595\n62.763278\n65.935953\n67.288798\n66.314210\n...\n64.151163\n64.677268\n66.340589\n64.291130\n64.540950\n66.728237\n65.474820\n65.725247\n65.015707\n65.399658\n\n\n4\n8.286851\n3.962356\n445\n444.348116\n445.520801\n446.419087\n442.715198\n445.043974\n444.849464\n446.178144\n...\n444.993117\n444.999768\n445.773163\n444.924819\n442.703987\n445.118233\n445.119561\n446.214723\n444.983397\n444.455462\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n7.002081\n4.288465\n280\n280.520486\n281.060157\n281.291354\n280.816827\n280.867509\n279.748219\n280.956388\n...\n280.864668\n280.261168\n278.871055\n280.267526\n279.792068\n279.931181\n280.606405\n280.710435\n279.602746\n280.472047\n\n\n496\n5.431050\n2.601212\n310\n310.015294\n310.003728\n308.680788\n309.179518\n310.908038\n308.580682\n308.720718\n...\n310.670717\n310.872724\n309.631197\n309.723108\n309.624440\n310.207927\n308.519863\n309.858127\n310.602814\n310.841082\n\n\n497\n0.088812\n0.042323\n225\n225.226422\n225.738373\n223.342848\n226.416036\n223.478492\n225.080199\n224.896846\n...\n223.252383\n226.178697\n225.494945\n225.154573\n225.254684\n224.711397\n225.259744\n223.700222\n225.700224\n225.120175\n\n\n498\n3.806388\n1.041416\n320\n321.135794\n319.880713\n319.759936\n321.071771\n320.085155\n320.657430\n320.802495\n...\n319.875882\n321.006234\n319.054499\n320.498798\n320.407021\n323.002351\n319.496038\n320.011256\n319.304070\n320.418582\n\n\n499\n6.739095\n3.626883\n375\n376.032149\n373.131026\n374.800140\n374.541924\n375.363948\n375.758260\n375.045161\n...\n375.452139\n374.790209\n374.247366\n372.755630\n373.711254\n376.621000\n373.979317\n374.788932\n374.153385\n373.678255\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n\n4. 분석\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n\nX = df_train.drop(['employment_score'],axis=1)\nXX = df_test.drop(['employment_score'],axis=1)\ny = df_train[['employment_score']]\nyy = df_test[['employment_score']]\n\n\nprdtr = sklearn.linear_model.LassoCV()\n\n\nprdtr.fit(X,y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:1563: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.024e-01, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.661e+00, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.013e+00, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.340e+00, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.626e+00, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.863e+00, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.044e+00, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.164e+00, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.022e+01, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.122e+01, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.216e+01, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.289e+01, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.306e+01, tolerance: 1.637e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.055e+00, tolerance: 1.601e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.572e+00, tolerance: 1.601e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.309e+00, tolerance: 1.601e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.189e+00, tolerance: 1.601e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.152e+00, tolerance: 1.601e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.015e+01, tolerance: 1.601e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.214e+01, tolerance: 1.601e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.410e+01, tolerance: 1.601e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.365e-01, tolerance: 1.612e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.845e-01, tolerance: 1.612e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.296e-01, tolerance: 1.612e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.701e-01, tolerance: 1.612e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.262e-01, tolerance: 1.988e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\nLassoCV()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoCVLassoCV()\n\n\n\nprdtr.score(X,y)\n\n0.8019360935778972\n\n\n\nprdtr.score(XX,yy)\n\n0.8380536028790841\n\n\n\nprdtr.alpha_ # 선택된 알파\n\n0.3746854844061495\n\n\n\n\n5. 확인\n\nprdtr = sklearn.linear_model.Lasso(alpha=0.3746854844061495)\n\n\nprdtr.fit(X,y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.262e-01, tolerance: 1.988e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\nLasso(alpha=0.3746854844061495)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.3746854844061495)\n\n\n\nprdtr.score(X,y)\n\n0.8019360935778972\n\n\n\nprdtr.score(XX,yy)\n\n0.8380536028790841"
  },
  {
    "objectID": "posts/02wk-007.html#a.-데이터",
    "href": "posts/02wk-007.html#a.-데이터",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")\n\nLoaded data from: ./titanic/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./titanic/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418\n\n\n- 피처엔지니어링\n\n_tr = tr.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)\n_tst = tst.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)"
  },
  {
    "objectID": "posts/02wk-007.html#b.-predictor-생성",
    "href": "posts/02wk-007.html#b.-predictor-생성",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230912_120900/\""
  },
  {
    "objectID": "posts/02wk-007.html#c.-적합fit",
    "href": "posts/02wk-007.html#c.-적합fit",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(_tr) # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230912_120900/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   293.85 GB / 490.57 GB (59.9%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    124991.67 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    10 features in original data used to generate 27 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.19s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8101   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8268   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.21s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.66s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.7989   = Validation score   (accuracy)\n    0.35s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8268   = Validation score   (accuracy)\n    0.46s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8212   = Validation score   (accuracy)\n    0.15s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.838    = Validation score   (accuracy)\n    1.55s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8268   = Validation score   (accuracy)\n    0.44s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 6.75s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230912_120900/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f463532b730&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.210158  3.510778                0.000527           0.246271            2       True         14\n1        NeuralNetTorch   0.837989       0.008443  1.547239                0.008443           1.547239            1       True         12\n2         LightGBMLarge   0.826816       0.003162  0.443558                0.003162           0.443558            1       True         13\n3              CatBoost   0.826816       0.003559  0.661513                0.003559           0.661513            1       True          7\n4              LightGBM   0.826816       0.003659  0.283715                0.003659           0.283715            1       True          4\n5       NeuralNetFastAI   0.826816       0.007515  0.462052                0.007515           0.462052            1       True         10\n6               XGBoost   0.821229       0.004702  0.148207                0.004702           0.148207            1       True         11\n7      RandomForestEntr   0.821229       0.214481  0.381199                0.214481           0.381199            1       True          6\n8      RandomForestGini   0.815642       0.091963  0.328555                0.091963           0.328555            1       True          5\n9            LightGBMXT   0.810056       0.003035  0.204797                0.003035           0.204797            1       True          3\n10       ExtraTreesGini   0.804469       0.184527  0.518099                0.184527           0.518099            1       True          8\n11       ExtraTreesEntr   0.798883       0.179535  0.353106                0.179535           0.353106            1       True          9\n12       KNeighborsUnif   0.653631       0.002189  0.009097                0.002189           0.009097            1       True          1\n13       KNeighborsDist   0.653631       0.002325  0.009845                0.002325           0.009845            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.210158\n3.510778\n0.000527\n0.246271\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.008443\n1.547239\n0.008443\n1.547239\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.826816\n0.003162\n0.443558\n0.003162\n0.443558\n1\nTrue\n13\n\n\n3\nCatBoost\n0.826816\n0.003559\n0.661513\n0.003559\n0.661513\n1\nTrue\n7\n\n\n4\nLightGBM\n0.826816\n0.003659\n0.283715\n0.003659\n0.283715\n1\nTrue\n4\n\n\n5\nNeuralNetFastAI\n0.826816\n0.007515\n0.462052\n0.007515\n0.462052\n1\nTrue\n10\n\n\n6\nXGBoost\n0.821229\n0.004702\n0.148207\n0.004702\n0.148207\n1\nTrue\n11\n\n\n7\nRandomForestEntr\n0.821229\n0.214481\n0.381199\n0.214481\n0.381199\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.815642\n0.091963\n0.328555\n0.091963\n0.328555\n1\nTrue\n5\n\n\n9\nLightGBMXT\n0.810056\n0.003035\n0.204797\n0.003035\n0.204797\n1\nTrue\n3\n\n\n10\nExtraTreesGini\n0.804469\n0.184527\n0.518099\n0.184527\n0.518099\n1\nTrue\n8\n\n\n11\nExtraTreesEntr\n0.798883\n0.179535\n0.353106\n0.179535\n0.353106\n1\nTrue\n9\n\n\n12\nKNeighborsUnif\n0.653631\n0.002189\n0.009097\n0.002189\n0.009097\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n0.653631\n0.002325\n0.009845\n0.002325\n0.009845\n1\nTrue\n2\n\n\n\n\n\n\n\n- validation set의 의미:"
  },
  {
    "objectID": "posts/02wk-007.html#d.-예측-predict",
    "href": "posts/02wk-007.html#d.-예측-predict",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(_tr)).mean()\n\n0.9539842873176206\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(_tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(Fsize,Drop)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/05wk-021.html#a.-학부12학년-수준의-설명",
    "href": "posts/05wk-021.html#a.-학부12학년-수준의-설명",
    "title": "05wk-021: 취업+밸런스게임, 오버피팅",
    "section": "A. 학부1~2학년 수준의 설명",
    "text": "A. 학부1~2학년 수준의 설명\n- 과적합(Overfitting): 머신러닝과 통계에서 자주 나타나는 문제로, 모델이 학습데이터에 과도하게 최적화가 되어서 실제로 새로운 데이터나 테스트데이터에서 성능이 저하되는 현상을 말함.\n- 오버피팅의 원인:\n\n불필요한 특징: 불필요한 특징이 데이터에 포함되어 있다면 오버피팅이 발생할 수 있음."
  },
  {
    "objectID": "posts/05wk-021.html#b.-일반인-수준의-설명",
    "href": "posts/05wk-021.html#b.-일반인-수준의-설명",
    "title": "05wk-021: 취업+밸런스게임, 오버피팅",
    "section": "B. 일반인 수준의 설명",
    "text": "B. 일반인 수준의 설명\n- 시험 공부(1): 공부를 하랬더니 외우고 있음..\n- 시험 공부(2): (시험 하루 전날에) 공부 그만하고 술이나 먹으러 가자.. 더 공부하면 train error만 줄일 뿐이야..\n- 운전: 특정도로에서만 운전연습을 했음. 그래서 그 도로의 구멍, 곡률, 신호등의 위치까지 완벽하게 숙지하였음. 그 결과 그 도로에서는 잘하게 되었지만, 그 도로 이외의 다른도로에서 운전을 한다면 문제가 발생함.\n- 언어: 특정 주제나 특정 상황에 대한 대화만을 반복적으로 연습하여, 그 상황에서는 완벽한 대화가 가능하지만 그 외의 상황에서는 대화를 제대로 이어나갈 수 없음."
  },
  {
    "objectID": "posts/04wk-016.html#a.-결측치-체크",
    "href": "posts/04wk-016.html#a.-결측치-체크",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "A. 결측치 체크",
    "text": "A. 결측치 체크\n- 결측치확인\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB"
  },
  {
    "objectID": "posts/04wk-016.html#b.-시각화",
    "href": "posts/04wk-016.html#b.-시각화",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "B. 시각화",
    "text": "B. 시각화\n\nmsno.matrix(df_train) \n# msno.bar(df_train) # 큰 의미 X \n# msno.dendrogram(df_train) # 큰 의미 X \n# msno.heatmap(df_train) # 큰 의미 X \n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-016.html#c.-결측치-처리",
    "href": "posts/04wk-016.html#c.-결측치-처리",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "C. 결측치 처리",
    "text": "C. 결측치 처리\n\ndf_train.select_dtypes(include=\"number\")\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\n27.0\n0\n0\n13.0000\n\n\n887\n888\n1\n1\n19.0\n0\n0\n30.0000\n\n\n888\n889\n0\n3\nNaN\n1\n2\n23.4500\n\n\n889\n890\n1\n1\n26.0\n0\n0\n30.0000\n\n\n890\n891\n0\n3\n32.0\n0\n0\n7.7500\n\n\n\n\n891 rows × 7 columns\n\n\n\n\ndef impute_missing(df):\n    df_imputed = df.copy()\n    df_num = df.select_dtypes(include=\"number\")\n    df_cat = df.select_dtypes(exclude=\"number\")\n    df_imputed[df_num.columns] = sklearn.impute.SimpleImputer().fit_transform(df_num) \n    df_imputed[df_cat.columns] = sklearn.impute.SimpleImputer(strategy='most_frequent').fit_transform(df_cat) \n    return df_imputed\n\n\nimpute_missing(df_test)\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892.0\n3.0\nKelly, Mr. James\nmale\n34.50000\n0.0\n0.0\n330911\n7.8292\nB57 B59 B63 B66\nQ\n\n\n1\n893.0\n3.0\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.00000\n1.0\n0.0\n363272\n7.0000\nB57 B59 B63 B66\nS\n\n\n2\n894.0\n2.0\nMyles, Mr. Thomas Francis\nmale\n62.00000\n0.0\n0.0\n240276\n9.6875\nB57 B59 B63 B66\nQ\n\n\n3\n895.0\n3.0\nWirz, Mr. Albert\nmale\n27.00000\n0.0\n0.0\n315154\n8.6625\nB57 B59 B63 B66\nS\n\n\n4\n896.0\n3.0\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.00000\n1.0\n1.0\n3101298\n12.2875\nB57 B59 B63 B66\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305.0\n3.0\nSpector, Mr. Woolf\nmale\n30.27259\n0.0\n0.0\nA.5. 3236\n8.0500\nB57 B59 B63 B66\nS\n\n\n414\n1306.0\n1.0\nOliva y Ocana, Dona. Fermina\nfemale\n39.00000\n0.0\n0.0\nPC 17758\n108.9000\nC105\nC\n\n\n415\n1307.0\n3.0\nSaether, Mr. Simon Sivertsen\nmale\n38.50000\n0.0\n0.0\nSOTON/O.Q. 3101262\n7.2500\nB57 B59 B63 B66\nS\n\n\n416\n1308.0\n3.0\nWare, Mr. Frederick\nmale\n30.27259\n0.0\n0.0\n359309\n8.0500\nB57 B59 B63 B66\nS\n\n\n417\n1309.0\n3.0\nPeter, Master. Michael J\nmale\n30.27259\n1.0\n1.0\n2668\n22.3583\nB57 B59 B63 B66\nC\n\n\n\n\n418 rows × 11 columns"
  },
  {
    "objectID": "posts/04wk-016.html#a.-자료의-정리",
    "href": "posts/04wk-016.html#a.-자료의-정리",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "A. 자료의 정리",
    "text": "A. 자료의 정리\n\n{c:len(set(df_train[c])) for c in df_train.select_dtypes(include=\"object\").columns}\n\n{'Name': 891, 'Sex': 2, 'Ticket': 681, 'Cabin': 148, 'Embarked': 4}\n\n\n\nX = pd.get_dummies(impute_missing(df_train).drop(['Survived','Name','Ticket','Cabin'],axis=1))\ny = impute_missing(df_train)[['Survived']]"
  },
  {
    "objectID": "posts/04wk-016.html#b.-predictor-생성",
    "href": "posts/04wk-016.html#b.-predictor-생성",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()"
  },
  {
    "objectID": "posts/04wk-016.html#c.-학습",
    "href": "posts/04wk-016.html#c.-학습",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/04wk-016.html#d.-예측",
    "href": "posts/04wk-016.html#d.-예측",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "D. 예측",
    "text": "D. 예측\n\npredictr.predict(X)\n\narray([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n       0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n       0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n       0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n       0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n       1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n       0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n       1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n       1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n       1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n       1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n       0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n       1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n       1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n       0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n       0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n       0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n       0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n       0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n       1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n       1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n       1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n       1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n       0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n       1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n       1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n       0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n       0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n       0., 1., 0., 1., 1., 0., 0.])"
  },
  {
    "objectID": "posts/04wk-016.html#e.-평가",
    "href": "posts/04wk-016.html#e.-평가",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.8024691358024691"
  },
  {
    "objectID": "posts/03wk-010.html#a.-데이터",
    "href": "posts/03wk-010.html#a.-데이터",
    "title": "03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "A. 데이터",
    "text": "A. 데이터\n\nX = df[['temp','type']] # 독립변수, 설명변수, 피쳐\ny = df[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nX = X.assign(type = [type == 'choco' for type in X.type])"
  },
  {
    "objectID": "posts/03wk-010.html#b.-predictor-생성",
    "href": "posts/03wk-010.html#b.-predictor-생성",
    "title": "03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression() \npredictr \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-010.html#c.-학습-fit-learn",
    "href": "posts/03wk-010.html#c.-학습-fit-learn",
    "title": "03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-010.html#d.-예측-predict",
    "href": "posts/03wk-010.html#d.-예측-predict",
    "title": "03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n\nyhat = predictr.predict(X)\n\n\nplt.plot(df.temp,df.sales,'o',alpha=0.5)\nplt.plot(df.temp,yhat, 'x',alpha=0.5)\n\n\n\n\n- 어떻게 맞춘거지?\n\\[\\textup{아이스크림 판매량} = 40 + \\textup{아이스크림종류} \\times (-20) + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\npredictr.coef_, predictr.intercept_\n\n(array([[  2.52239574, -20.54021854]]), array([40.16877158]))\n\n\n- 온도가 -2이고, type이 초코라면? 예측값은?\n\nXnew = pd.DataFrame({'temp':[-2.0],'type':[1]})\nXnew\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n-2.0\n1\n\n\n\n\n\n\n\n\npredictr.predict(Xnew)\n\narray([[14.58376156]])\n\n\n- 온도가 -2이고, type이 바닐라라면? 예측값은?\n\nXnew = pd.DataFrame({'temp':[-2.0],'type':[0]})\nXnew\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n-2.0\n0\n\n\n\n\n\n\n\n\npredictr.predict(Xnew)\n\narray([[35.1239801]])"
  },
  {
    "objectID": "posts/02wk-004.html#part-1-get-started",
    "href": "posts/02wk-004.html#part-1-get-started",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 1: Get started",
    "text": "Part 1: Get started\nIn this section, you’ll learn more about the competition and make your first submission.\n\nJoin the competition!\nThe first thing to do is to join the competition! Open a new window with the competition page, and click on the “Join Competition” button, if you haven’t already. (If you see a “Submit Predictions” button instead of a “Join Competition” button, you have already joined the competition, and don’t need to do so again.)\n\nThis takes you to the rules acceptance page. You must accept the competition rules in order to participate. These rules govern how many submissions you can make per day, the maximum team size, and other competition-specific details. Then, click on “I Understand and Accept” to indicate that you will abide by the competition rules.\n\n\nThe challenge\nThe competition is simple: we want you to use the Titanic passenger data (name, age, price of ticket, etc) to try to predict who will survive and who will die.\n\n\nThe data\nTo take a look at the competition data, click on the Data tab at the top of the competition page. Then, scroll down to find the list of files.\nThere are three files in the data: (1) train.csv, (2) test.csv, and (3) gender_submission.csv.\n\n(1) train.csv\ntrain.csv contains the details of a subset of the passengers on board (891 passengers, to be exact – where each passenger gets a different row in the table). To investigate this data, click on the name of the file on the left of the screen. Once you’ve done this, you can view all of the data in the window.\n\nThe values in the second column (“Survived”) can be used to determine whether each passenger survived or not: - if it’s a “1”, the passenger survived. - if it’s a “0”, the passenger died.\nFor instance, the first passenger listed in train.csv is Mr. Owen Harris Braund. He was 22 years old when he died on the Titanic.\n\n\n(2) test.csv\nUsing the patterns you find in train.csv, you have to predict whether the other 418 passengers on board (in test.csv) survived.\nClick on test.csv (on the left of the screen) to examine its contents. Note that test.csv does not have a “Survived” column - this information is hidden from you, and how well you do at predicting these hidden values will determine how highly you score in the competition!\n\n\n(3) gender_submission.csv\nThe gender_submission.csv file is provided as an example that shows how you should structure your predictions. It predicts that all female passengers survived, and all male passengers died. Your hypotheses regarding survival will probably be different, which will lead to a different submission file. But, just like this file, your submission should have: - a “PassengerId” column containing the IDs of each passenger from test.csv. - a “Survived” column (that you will create!) with a “1” for the rows where you think the passenger survived, and a “0” where you predict that the passenger died."
  },
  {
    "objectID": "posts/02wk-004.html#part-2-your-coding-environment",
    "href": "posts/02wk-004.html#part-2-your-coding-environment",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 2: Your coding environment",
    "text": "Part 2: Your coding environment\nIn this section, you’ll train your own machine learning model to improve your predictions. If you’ve never written code before or don’t have any experience with machine learning, don’t worry! We don’t assume any prior experience in this tutorial.\n\nThe Notebook\nThe first thing to do is to create a Kaggle Notebook where you’ll store all of your code. You can use Kaggle Notebooks to getting up and running with writing code quickly, and without having to install anything on your computer. (If you are interested in deep learning, we also offer free GPU access!)\nBegin by clicking on the Code tab on the competition page. Then, click on “New Notebook”.\n\nYour notebook will take a few seconds to load. In the top left corner, you can see the name of your notebook – something like “kernel2daed3cd79”.\n\nYou can edit the name by clicking on it. Change it to something more descriptive, like “Getting Started with Titanic”.\n\n\n\nYour first lines of code\nWhen you start a new notebook, it has two gray boxes for storing code. We refer to these gray boxes as “code cells”.\n\nThe first code cell already has some code in it. To run this code, put your cursor in the code cell. (If your cursor is in the right place, you’ll notice a blue vertical line to the left of the gray box.) Then, either hit the play button (which appears to the left of the blue line), or hit [Shift] + [Enter] on your keyboard.\nIf the code runs successfully, three lines of output are returned. Below, you can see the same code that you just ran, along with the output that you should see in your notebook.\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nThis shows us where the competition data is stored, so that we can load the files into the notebook. We’ll do that next.\n\n\nLoad the data\nThe second code cell in your notebook now appears below the three lines of output with the file locations.\n\nType the two lines of code below into your second code cell. Then, once you’re done, either click on the blue play button, or hit [Shift] + [Enter].\n\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nYour code should return the output above, which corresponds to the first five rows of the table in train.csv. It’s very important that you see this output in your notebook before proceeding with the tutorial! &gt; If your code does not produce this output, double-check that your code is identical to the two lines above. And, make sure your cursor is in the code cell before hitting [Shift] + [Enter].\nThe code that you’ve just written is in the Python programming language. It uses a Python “module” called pandas (abbreviated as pd) to load the table from the train.csv file into the notebook. To do this, we needed to plug in the location of the file (which we saw was /kaggle/input/titanic/train.csv).\n&gt; If you’re not already familiar with Python (and pandas), the code shouldn’t make sense to you – but don’t worry! The point of this tutorial is to (quickly!) make your first submission to the competition. At the end of the tutorial, we suggest resources to continue your learning.\nAt this point, you should have at least three code cells in your notebook.\n\nCopy the code below into the third code cell of your notebook to load the contents of the test.csv file. Don’t forget to click on the play button (or hit [Shift] + [Enter])!\n\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nAs before, make sure that you see the output above in your notebook before continuing.\nOnce all of the code runs successfully, all of the data (in train.csv and test.csv) is loaded in the notebook. (The code above shows only the first 5 rows of each table, but all of the data is there – all 891 rows of train.csv and all 418 rows of test.csv!)"
  },
  {
    "objectID": "posts/02wk-004.html#part-3-your-first-submission",
    "href": "posts/02wk-004.html#part-3-your-first-submission",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 3: Your first submission",
    "text": "Part 3: Your first submission\nRemember our goal: we want to find patterns in train.csv that help us predict whether the passengers in test.csv survived.\nIt might initially feel overwhelming to look for patterns, when there’s so much data to sort through. So, we’ll start simple.\n\nExplore a pattern\nRemember that the sample submission file in gender_submission.csv assumes that all female passengers survived (and all male passengers died).\nIs this a reasonable first guess? We’ll check if this pattern holds true in the data (in train.csv).\nCopy the code below into a new code cell. Then, run the cell.\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n\n% of women who survived: 0.7420382165605095\n\n\n\n여성의 생존률을 구하는 코드입니다, 이전에 accuracy를 구하던 테크닉을 활용하면 아래의 코드도 가능합니다\n\ntrain_data[train_data.Sex == 'female'].Survived.mean()\n\n0.7420382165605095\n\n\n\nBefore moving on, make sure that your code returns the output above. The code above calculates the percentage of female passengers (in train.csv) who survived.\nThen, run the code below in another code cell:\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n\n% of men who survived: 0.18890814558058924\n\n\nThe code above calculates the percentage of male passengers (in train.csv) who survived.\nFrom this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in gender_submission.csv is not a bad first guess!\nBut at the end of the day, this gender-based submission bases its predictions on only a single column. As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions. Since it is quite difficult to consider several columns at once (or, it would take a long time to consider all possible patterns in many different columns simultaneously), we’ll use machine learning to automate this for us."
  },
  {
    "objectID": "posts/02wk-004.html#your-first-machine-learning-model",
    "href": "posts/02wk-004.html#your-first-machine-learning-model",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Your first machine learning model",
    "text": "Your first machine learning model\nWe’ll build what’s known as a random forest model. This model is constructed of several “trees” (there are three trees in the picture below, but we’ll construct 100!) that will individually consider each passenger’s data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\n\nThe code cell below looks for patterns in four different columns (“Pclass”, “Sex”, “SibSp”, and “Parch”) of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file submission.csv.\nCopy this code into your notebook, and run it in a new code cell.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\nMake sure that your notebook outputs the same message above (Your submission was successfully saved!) before moving on. &gt; Again, don’t worry if this code doesn’t make sense to you! For now, we’ll focus on how to generate and submit predictions.\nOnce you’re ready, click on the “Save Version” button in the top right corner of your notebook. This will generate a pop-up window.\n- Ensure that the “Save and Run All” option is selected, and then click on the “Save” button. - This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the “Save Version” button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (…) to the right of the most recent version, and select Open in Viewer.\n- Click on the Data tab on the top of the screen. Then, click on the “Submit” button to submit your results.\n\nCongratulations for making your first submission to a Kaggle competition! Within ten minutes, you should receive a message providing your spot on the leaderboard. Great work!"
  },
  {
    "objectID": "posts/02wk-004.html#part-4-learn-more",
    "href": "posts/02wk-004.html#part-4-learn-more",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 4: Learn more!",
    "text": "Part 4: Learn more!\nIf you’re interested in learning more, we strongly suggest our (3-hour) Intro to Machine Learning course, which will help you fully understand all of the code that we’ve presented here. You’ll also know enough to generate even better predictions!"
  },
  {
    "objectID": "posts/02wk-004.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "href": "posts/02wk-004.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?",
    "text": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?\n- 원래코드\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\n\nlen(predictions), len(X_test)\n\n(418, 418)\n\n\n- 이렇게 수정하면 될 듯\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\nlen(predictions),len(y)\n\n(891, 891)\n\n\n\n(predictions == y).mean()\n\n0.8159371492704826"
  },
  {
    "objectID": "posts/02wk-004.html#b.-alexis-cook의-코드를-수정해보자",
    "href": "posts/02wk-004.html#b.-alexis-cook의-코드를-수정해보자",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "B. Alexis Cook의 코드를 수정해보자!",
    "text": "B. Alexis Cook의 코드를 수정해보자!\n- 코드를 수정해보자.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=5000, max_depth=1000, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\n(predictions == y).mean()\n\n0.8170594837261503\n\n\n\n내가 만든게 더 좋은데??\n\n- 이것도 제출결과로 만들어보자.\n\npredictions = model.predict(X_test)\n\n- 아래와 같이 제출하면 에러가 발생\n\npd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정_submission.csv\")\n\n- 아래와 같이 제출파일을 저장해야 한다.\n\npd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정2_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/02wk-005.html#a.-데이터",
    "href": "posts/02wk-005.html#a.-데이터",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")"
  },
  {
    "objectID": "posts/02wk-005.html#b.-predictor-생성",
    "href": "posts/02wk-005.html#b.-predictor-생성",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230910_162629/\""
  },
  {
    "objectID": "posts/02wk-005.html#c.-적합fit",
    "href": "posts/02wk-005.html#c.-적합fit",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr) \n# 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230910_162629/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   306.27 GB / 490.57 GB (62.4%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    126262.11 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.2s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.09s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.16s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.16s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.85s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8268   = Validation score   (accuracy)\n    1.33s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8101   = Validation score   (accuracy)\n    0.14s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8324   = Validation score   (accuracy)\n    1.49s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.22s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 7.58s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230910_162629/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fa18f4444f0&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.193071  4.142821                0.000378           0.218161            2       True         14\n1         LightGBMLarge   0.832402       0.003180  0.422871                0.003180           0.422871            1       True         13\n2        NeuralNetTorch   0.832402       0.009243  1.488933                0.009243           1.488933            1       True         12\n3              CatBoost   0.826816       0.003432  0.526762                0.003432           0.526762            1       True          7\n4       NeuralNetFastAI   0.826816       0.007648  1.326021                0.007648           1.326021            1       True         10\n5              LightGBM   0.821229       0.003042  0.157547                0.003042           0.157547            1       True          4\n6            LightGBMXT   0.815642       0.003350  0.197612                0.003350           0.197612            1       True          3\n7        ExtraTreesGini   0.815642       0.050255  0.848478                0.050255           0.848478            1       True          8\n8      RandomForestEntr   0.815642       0.091255  0.525193                0.091255           0.525193            1       True          6\n9      RandomForestGini   0.815642       0.162890  0.340251                0.162890           0.340251            1       True          5\n10              XGBoost   0.810056       0.004424  0.141125                0.004424           0.141125            1       True         11\n11       ExtraTreesEntr   0.810056       0.060458  0.398653                0.060458           0.398653            1       True          9\n12       KNeighborsDist   0.653631       0.001958  0.007848                0.001958           0.007848            1       True          2\n13       KNeighborsUnif   0.653631       0.004457  0.087323                0.004457           0.087323            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.193071\n4.142821\n0.000378\n0.218161\n2\nTrue\n14\n\n\n1\nLightGBMLarge\n0.832402\n0.003180\n0.422871\n0.003180\n0.422871\n1\nTrue\n13\n\n\n2\nNeuralNetTorch\n0.832402\n0.009243\n1.488933\n0.009243\n1.488933\n1\nTrue\n12\n\n\n3\nCatBoost\n0.826816\n0.003432\n0.526762\n0.003432\n0.526762\n1\nTrue\n7\n\n\n4\nNeuralNetFastAI\n0.826816\n0.007648\n1.326021\n0.007648\n1.326021\n1\nTrue\n10\n\n\n5\nLightGBM\n0.821229\n0.003042\n0.157547\n0.003042\n0.157547\n1\nTrue\n4\n\n\n6\nLightGBMXT\n0.815642\n0.003350\n0.197612\n0.003350\n0.197612\n1\nTrue\n3\n\n\n7\nExtraTreesGini\n0.815642\n0.050255\n0.848478\n0.050255\n0.848478\n1\nTrue\n8\n\n\n8\nRandomForestEntr\n0.815642\n0.091255\n0.525193\n0.091255\n0.525193\n1\nTrue\n6\n\n\n9\nRandomForestGini\n0.815642\n0.162890\n0.340251\n0.162890\n0.340251\n1\nTrue\n5\n\n\n10\nXGBoost\n0.810056\n0.004424\n0.141125\n0.004424\n0.141125\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.060458\n0.398653\n0.060458\n0.398653\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.001958\n0.007848\n0.001958\n0.007848\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.004457\n0.087323\n0.004457\n0.087323\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-005.html#d.-예측-predict",
    "href": "posts/02wk-005.html#d.-예측-predict",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.9349046015712682\n\n\n\n(tr.Survived == (tr.Sex == \"female\")).mean() # 예전점수와 비교\n\n0.7867564534231201\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/03wk-009.html#a.-질문",
    "href": "posts/03wk-009.html#a.-질문",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "A. 질문",
    "text": "A. 질문\n- 질문: 기온이 \\(x=-2.0\\) 일 때 아이스크림을 얼마정도 판다고 보는게 타당할까?"
  },
  {
    "objectID": "posts/03wk-009.html#b.-답1",
    "href": "posts/03wk-009.html#b.-답1",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "B. 답1",
    "text": "B. 답1\n- \\(x=-2.0\\) 근처의 데이터를 살펴보자.\n\ndf[(-3.0 &lt; df.temp) & (df.temp &lt; -1.0)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n3\n-1.3\n17.673681\n\n\n\n\n\n\n\n대충 17.67 근처이지 않을까?.."
  },
  {
    "objectID": "posts/03wk-009.html#c.-답2",
    "href": "posts/03wk-009.html#c.-답2",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "C. 답2",
    "text": "C. 답2\n- 자료를 바탕으로 그림을 그려보자.\n\nplt.plot(df.temp,df.sales,'o')\nplt.plot([-2.0],[17.67],'x')\n\n\n\n\n\n저거 보다 못팔 것 같은데?"
  },
  {
    "objectID": "posts/03wk-009.html#d.-아이디어",
    "href": "posts/03wk-009.html#d.-아이디어",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "D. 아이디어",
    "text": "D. 아이디어\n- 선을 기가 막히게 그어서 추세선을 만들고, 그 추세선 위의 점으로 예측하자.\n- 속마음: 사실 추세선을 알고 있긴함\n\nplt.plot(df.temp,df.sales,'o')\nplt.plot(df.temp,20+df.temp*2.5,'--')\n\n\n\n\n- 사실 \\(y=20+2.5x\\) 라는 추세선을 그으면 된다는 것을 알고 있다.\n- 그래서 \\(x=-2\\) 이라면 \\(y=20-2.5\\times 2=15\\) 라고 보는게 합리적임. (물론 오차가 있을 수 있지만 그건 운이므로 어쩔수 없는것임, 랜덤으로 뭐가 나올지까지 맞출 수는 없음1)\n1 만약 그렇다면 랜덤이 아니겠지?- 그렇지만 우리는 사실 \\(20, 2.5\\) 라는 숫자를 모른다. (이 숫자만 안다면 임의의 \\(x\\)에 대한 \\(y\\)값을 알 수 있을 텐데…)\n- 게임셋팅\n\n원래게임: 임의의 \\(x\\)에 대하여 합리적인 \\(y\\)를 잘 찾는 게임\n변형된게임: \\(20,2.5\\) 라는 숫자를 잘 찾는 게임, 즉 데이터를 보고 최대한 \\(y_i \\approx ax_i+b\\) 이 되도록 \\(a,b\\)를 잘 선택하는 게임"
  },
  {
    "objectID": "posts/03wk-009.html#a.-데이터",
    "href": "posts/03wk-009.html#a.-데이터",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 변수 설정\n\nX = df[['temp']] # 독립변수, 설명변수, 피쳐\ny = df[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nplt.plot(X,y,'o')\n\n\n\n\n- 질문: 기온이 \\(x=-2.0\\) 일 때 아이스크림을 얼마정도 판다고 보는게 타당할까?\n\ndf[(-3.0 &lt; df.temp) & (df.temp &lt; -1.0)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n3\n-1.3\n17.673681\n\n\n\n\n\n\n\n- 답1: 대충 17.67 근처이지 않을까?..\n- 답2: 17.67 보다 작지 않을까?\n- 아이디어: 추세선을 그리고 거기서 예측해보면 어떨까?\n- 데이터를 학습하여 추세선을 적절히 그릴 수 있고, 그려진 추세선으로 예측까지 해줄수 있는 아이(predictor)를 만들자."
  },
  {
    "objectID": "posts/03wk-009.html#b.-predictor-생성",
    "href": "posts/03wk-009.html#b.-predictor-생성",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression() \npredictr \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlinear regression 이라는 방법으로 추세선을 만들고 예측하는 아이(predictor)를 만드는 코드"
  },
  {
    "objectID": "posts/03wk-009.html#c.-학습-fit-learn",
    "href": "posts/03wk-009.html#c.-학습-fit-learn",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-009.html#d.-예측-predict",
    "href": "posts/03wk-009.html#d.-예측-predict",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- Predictor: 데이터를 살펴보니 원래 true는 이럴것 같아요\n\nyhat = predictr.predict(X)\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat, 'o--',alpha=0.5)\n\n\n\n\n- 최규빈: 저런 추세선을 그었다면, \\(y=ax+b\\) 꼴의 식에서 \\(a\\), \\(b\\)를 적당한 값으로 찾았다는 의미인데, 그 값은 어디있지?\n- Predictor: 아래에 있어요\n\na = predictr.coef_,\nb = predictr.intercept_\n\n\na,b\n\n((array([[2.51561216]]),), array([19.66713127]))\n\n\n- 최규빈: 확인해보자..\n\n(df.temp * 2.51561216 + 19.66713127)[:5], yhat[:5]\n\n(0     9.353121\n 1    10.359366\n 2    12.120295\n 3    16.396835\n 4    18.409325\n Name: temp, dtype: float64,\n array([[ 9.35312141],\n        [10.35936628],\n        [12.12029479],\n        [16.39683546],\n        [18.40932519]]))\n\n\n- 새로운 데이터 \\(x=-2\\) 에 대한 예측 (1) – 수식위주로\n\n2.51561216*(-2) + 19.66713127\n\n14.635906949999999\n\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat,'--',alpha=0.5)\nplt.plot([-2],[14.635906949999999],'xr')\n\n\n\n\n- 새로운 데이터 \\(x=-2\\) 에 대한 예측 (2) – 코드위주로 (\\(\\star\\))\n\nXnew = pd.DataFrame({'temp':[-2.0]})\n\n\npredictr.predict(Xnew)\n\narray([[14.63590695]])\n\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat,'--',alpha=0.5)\nplt.plot(Xnew, predictr.predict(Xnew),'xr')"
  },
  {
    "objectID": "posts/02wk-006.html#a.-데이터",
    "href": "posts/02wk-006.html#a.-데이터",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")\n\n- 피처엔지니어링\n\ntr.eval('Fsize = SibSp + Parch')\ntst.eval('Fsize = SibSp + Parch')\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n0\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n1\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n0\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n0\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\nSpector, Mr. Woolf\nmale\nNaN\n0\n0\nA.5. 3236\n8.0500\nNaN\nS\n0\n\n\n414\n1306\n1\nOliva y Ocana, Dona. Fermina\nfemale\n39.0\n0\n0\nPC 17758\n108.9000\nC105\nC\n0\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\n0\n0\nSOTON/O.Q. 3101262\n7.2500\nNaN\nS\n0\n\n\n416\n1308\n3\nWare, Mr. Frederick\nmale\nNaN\n0\n0\n359309\n8.0500\nNaN\nS\n0\n\n\n417\n1309\n3\nPeter, Master. Michael J\nmale\nNaN\n1\n1\n2668\n22.3583\nNaN\nC\n2\n\n\n\n\n418 rows × 12 columns"
  },
  {
    "objectID": "posts/02wk-006.html#b.-predictor-생성",
    "href": "posts/02wk-006.html#b.-predictor-생성",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230912_115814/\""
  },
  {
    "objectID": "posts/02wk-006.html#c.-적합fit",
    "href": "posts/02wk-006.html#c.-적합fit",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr.eval('Fsize = SibSp + Parch'))\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230912_115814/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   294.00 GB / 490.57 GB (59.9%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    125936.3 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    12 features in original data used to generate 29 features in processed data.\n    Train Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.21s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.648    = Validation score   (accuracy)\n    0.09s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.18s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.88s    = Training   runtime\n    0.12s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.51s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8324   = Validation score   (accuracy)\n    1.32s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.15s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.838    = Validation score   (accuracy)\n    1.38s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.24s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 7.68s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230912_115814/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f46f0228310&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.016608  2.934600                0.000400           0.236214            2       True         14\n1        NeuralNetTorch   0.837989       0.008671  1.382263                0.008671           1.382263            1       True         12\n2         LightGBMLarge   0.832402       0.003077  0.377827                0.003077           0.377827            1       True         13\n3       NeuralNetFastAI   0.832402       0.007537  1.316123                0.007537           1.316123            1       True         10\n4              CatBoost   0.826816       0.003649  0.528946                0.003649           0.528946            1       True          7\n5               XGBoost   0.826816       0.004545  0.150251                0.004545           0.150251            1       True         11\n6              LightGBM   0.821229       0.003294  0.180805                0.003294           0.180805            1       True          4\n7      RandomForestEntr   0.821229       0.194986  0.478185                0.194986           0.478185            1       True          6\n8            LightGBMXT   0.815642       0.003372  0.198229                0.003372           0.198229            1       True          3\n9      RandomForestGini   0.815642       0.097445  0.306731                0.097445           0.306731            1       True          5\n10       ExtraTreesEntr   0.810056       0.076015  0.512563                0.076015           0.512563            1       True          9\n11       ExtraTreesGini   0.804469       0.118582  0.881634                0.118582           0.881634            1       True          8\n12       KNeighborsDist   0.648045       0.001869  0.007722                0.001869           0.007722            1       True          2\n13       KNeighborsUnif   0.648045       0.004271  0.092099                0.004271           0.092099            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.016608\n2.934600\n0.000400\n0.236214\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.008671\n1.382263\n0.008671\n1.382263\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.832402\n0.003077\n0.377827\n0.003077\n0.377827\n1\nTrue\n13\n\n\n3\nNeuralNetFastAI\n0.832402\n0.007537\n1.316123\n0.007537\n1.316123\n1\nTrue\n10\n\n\n4\nCatBoost\n0.826816\n0.003649\n0.528946\n0.003649\n0.528946\n1\nTrue\n7\n\n\n5\nXGBoost\n0.826816\n0.004545\n0.150251\n0.004545\n0.150251\n1\nTrue\n11\n\n\n6\nLightGBM\n0.821229\n0.003294\n0.180805\n0.003294\n0.180805\n1\nTrue\n4\n\n\n7\nRandomForestEntr\n0.821229\n0.194986\n0.478185\n0.194986\n0.478185\n1\nTrue\n6\n\n\n8\nLightGBMXT\n0.815642\n0.003372\n0.198229\n0.003372\n0.198229\n1\nTrue\n3\n\n\n9\nRandomForestGini\n0.815642\n0.097445\n0.306731\n0.097445\n0.306731\n1\nTrue\n5\n\n\n10\nExtraTreesEntr\n0.810056\n0.076015\n0.512563\n0.076015\n0.512563\n1\nTrue\n9\n\n\n11\nExtraTreesGini\n0.804469\n0.118582\n0.881634\n0.118582\n0.881634\n1\nTrue\n8\n\n\n12\nKNeighborsDist\n0.648045\n0.001869\n0.007722\n0.001869\n0.007722\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.648045\n0.004271\n0.092099\n0.004271\n0.092099\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-006.html#d.-예측-predict",
    "href": "posts/02wk-006.html#d.-예측-predict",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr.eval('Fsize = SibSp + Parch'))).mean()\n\n0.9169472502805837\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst.eval('Fsize = SibSp + Parch'))).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(Fsize)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/06wk-022.html#a.-toeic과-gpa가-유의미한-변수라는걸-눈치챘다면",
    "href": "posts/06wk-022.html#a.-toeic과-gpa가-유의미한-변수라는걸-눈치챘다면",
    "title": "05wk-022: 취업+각종영어점수, 다중공선성",
    "section": "A. toeic과 gpa가 유의미한 변수라는걸 눈치챘다면?",
    "text": "A. toeic과 gpa가 유의미한 변수라는걸 눈치챘다면?\n\nX = df_train[['toeic','gpa']]\ny = df_train[['employment_score']]\nXX = df_test[['toeic','gpa']]\nyy = df_test[['employment_score']]\n\n\npredictr = sklearn.linear_model.LinearRegression()\n\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\npredictr.coef_,predictr.intercept_\n\n(array([[0.00517485, 1.30448417]]), array([-0.08665847]))\n\n\n\npredictr.score(X,y)\n\n0.8249262562780773\n\n\n\npredictr.score(XX,yy)\n\n0.8347244826096045"
  },
  {
    "objectID": "posts/06wk-022.html#b.-하다못해-toeic0과-gpa로-적합했다면",
    "href": "posts/06wk-022.html#b.-하다못해-toeic0과-gpa로-적합했다면",
    "title": "05wk-022: 취업+각종영어점수, 다중공선성",
    "section": "B. 하다못해 toeic0과 gpa로 적합했다면?",
    "text": "B. 하다못해 toeic0과 gpa로 적합했다면?\n\nX = df_train[['toeic0','gpa']]\ny = df_train[['employment_score']]\nXX = df_test[['toeic0','gpa']]\nyy = df_test[['employment_score']]\n\n\npredictr = sklearn.linear_model.LinearRegression()\n\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\npredictr.coef_,predictr.intercept_\n\n(array([[0.00517489, 1.30422841]]), array([-0.08635412]))\n\n\n\npredictr.score(X,y)\n\n0.8249991103247842\n\n\n\npredictr.score(XX,yy)\n\n0.8346691714793039"
  },
  {
    "objectID": "posts/04wk-018.html#a.-학습-이후에-예측평가-가능",
    "href": "posts/04wk-018.html#a.-학습-이후에-예측평가-가능",
    "title": "04wk-018: Predictor 깊은 이해 + 기호정리",
    "section": "A. 학습 이후에 예측/평가 가능",
    "text": "A. 학습 이후에 예측/평가 가능\n- Predictor의 list생성\n\npredictors = [sklearn.linear_model.LinearRegression() for i in range(2)]\npredictors\n\n[LinearRegression(), LinearRegression()]\n\n\n\n두개의 predictor를 만들어서 리스트로 정리함\n\n- 첫번째 predictor에 접근\n\npredictors[0]\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 두번째 predictor에 접근\n\npredictors[1]\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 첫번째 predictor를 학습\n\npredictors[0].fit(df_train_X,df_train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 학습이후에는 .coef_, .intercept_ 값이 생성됨\n\npredictors[0].coef_, predictors[0].intercept_\n\n(array([[-0.99420514]]), array([20.86235044]))\n\n\n\npredictors[1].coef_, predictors[1].intercept_\n\nAttributeError: 'LinearRegression' object has no attribute 'coef_'\n\n\n- .coef_와 .intercept_값이 생겨야 .predict(X)를 통하여 예측을 할 수 있음\n\npredictors[0].predict(df_train_X)\n\narray([[ 0.97824769],\n       [-0.01595745],\n       [-1.01016259],\n       [-2.00436772],\n       [-2.99857286],\n       [-3.992778  ],\n       [-4.98698314],\n       [-5.98118827]])\n\n\n\npredictors[1].predict(df_train_X)\n\nNotFittedError: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n\n\n- 예측을 해야 평가를 할 수 있음\n\npredictors[0].score(df_train_X,df_train_y)\n\n0.9996857763812423\n\n\n\npredictors[1].score(df_train_X,df_train_y)\n\nNotFittedError: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
  },
  {
    "objectID": "posts/04wk-018.html#b.-.fitxy에서-xy의-형식",
    "href": "posts/04wk-018.html#b.-.fitxy에서-xy의-형식",
    "title": "04wk-018: Predictor 깊은 이해 + 기호정리",
    "section": "B. .fit(X,y)에서 X,y의 형식",
    "text": "B. .fit(X,y)에서 X,y의 형식\n\npredictr = predictors[0]\n\n\nXs = {'DataFrame(2d)': df_train_X, \n      'Seires(1d)': df_train_X.X,\n      'ndarray(2d)': np.array(df_train_X),\n      'ndarray(1d)': np.array(df_train_X).reshape(-1),\n      'list(2d)': np.array(df_train_X).tolist(),\n      'list(1d)': np.array(df_train_X).reshape(-1).tolist()}\n\n\nys = {'DataFrame(2d)': df_train_y, \n      'Seires(1d)': df_train_y.y,\n      'ndarray(2d)': np.array(df_train_y),\n      'ndarray(1d)': np.array(df_train_y).reshape(-1),\n      'list(2d)': np.array(df_train_y).tolist(),\n      'list(1d)': np.array(df_train_y).reshape(-1).tolist()}\n\n\ndef test(X,y):\n    try: \n        predictr.fit(X,y)\n        return 'no error'\n    except:\n        return 'error'\n\n\n{('X='+i,'y='+j): test(Xs[i],ys[j]) for i,j in itertools.product(Xs.keys(),ys.keys())}\n\n{('X=DataFrame(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=Seires(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(1d)'): 'no error',\n ('X=Seires(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=Seires(1d)', 'y=Seires(1d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(2d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(1d)'): 'error',\n ('X=Seires(1d)', 'y=list(2d)'): 'error',\n ('X=Seires(1d)', 'y=list(1d)'): 'error',\n ('X=ndarray(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=Seires(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(1d)'): 'no error',\n ('X=ndarray(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=ndarray(1d)', 'y=Seires(1d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(2d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(1d)'): 'error',\n ('X=ndarray(1d)', 'y=list(2d)'): 'error',\n ('X=ndarray(1d)', 'y=list(1d)'): 'error',\n ('X=list(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=list(2d)', 'y=Seires(1d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=list(2d)', 'y=list(2d)'): 'no error',\n ('X=list(2d)', 'y=list(1d)'): 'no error',\n ('X=list(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=list(1d)', 'y=Seires(1d)'): 'error',\n ('X=list(1d)', 'y=ndarray(2d)'): 'error',\n ('X=list(1d)', 'y=ndarray(1d)'): 'error',\n ('X=list(1d)', 'y=list(2d)'): 'error',\n ('X=list(1d)', 'y=list(1d)'): 'error'}\n\n\n- 결론: X는 2d만 가능, y는 2d,1d 모두 가능"
  },
  {
    "objectID": "posts/05wk-019.html#a.-모티브",
    "href": "posts/05wk-019.html#a.-모티브",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 모티브",
    "text": "A. 모티브\n- 예제자료 로드\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 모형을 돌려보고 해석한 결과\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n\n토익이 중요해? 아니면 학점이 중요해?\n얼만큼 중요해?\n\n- 모티브: 토익점수를 0-1사이로 맞추고 gpa도 0-1사이로 맞추면 해석이 쉽지 않을까?"
  },
  {
    "objectID": "posts/05wk-019.html#b.-사용방법",
    "href": "posts/05wk-019.html#b.-사용방법",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. 사용방법",
    "text": "B. 사용방법\n- 스케일러 생성\n\nsclr = sklearn.preprocessing.MinMaxScaler()\n\n- fit, transform\n\nsclr.fit(df)\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nsclr.transform(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n- fit_transform\n\nsclr.fit_transform(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])"
  },
  {
    "objectID": "posts/05wk-019.html#c.-잘못된-사용",
    "href": "posts/05wk-019.html#c.-잘못된-사용",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "C. 잘못된 사용",
    "text": "C. 잘못된 사용\n- sclr.fit()와 sclr.fit_transform()은 입력으로 2차원 자료구조를 기대한다. (그중에서도 은근히 numpy array를 기대함)\n\nsclr.fit_transform(df['toeic']) # df['toeic']는 1차원 자료구조\n\nValueError: Expected 2D array, got 1D array instead:\narray=[135. 935. 485.  65. 445.  65. 290. 730.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\n\n\nsclr.fit_transform(df[['toeic']]) # df[['toeic']]는 2차원 자료구조\n\narray([[0.08045977],\n       [1.        ],\n       [0.48275862],\n       [0.        ],\n       [0.43678161],\n       [0.        ],\n       [0.25862069],\n       [0.76436782]])"
  },
  {
    "objectID": "posts/05wk-019.html#a.-사용방법",
    "href": "posts/05wk-019.html#a.-사용방법",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 사용방법",
    "text": "A. 사용방법\n- scaler를 오브젝트로 따로 만들지 않고 함수형으로 구현\n\nsklearn.preprocessing.minmax_scale(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n- 이것은 심지어 1차원 자료구조에도 적용가능하다.\n\nsklearn.preprocessing.minmax_scale(df['toeic'])\n\narray([0.08045977, 1.        , 0.48275862, 0.        , 0.43678161,\n       0.        , 0.25862069, 0.76436782])\n\n\n- 열별로 스케일링을 하는게 아니라 행별로 스케일링을 하는 것도 가능하다. (여기서는 필요없지만..)\n\nsklearn.preprocessing.minmax_scale(df,axis=1)\n\narray([[1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.]])"
  },
  {
    "objectID": "posts/05wk-019.html#b.-discussions",
    "href": "posts/05wk-019.html#b.-discussions",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. discussions",
    "text": "B. discussions\n- 언뜻 보기에는 MinMaxScaler 보다 minmax_scale이 좋아보이는데, 생각보다 일반적으로 minmax_scale을 사용하지는 않음. 이유는 아래와 같음.\n\n파이썬을 쓰는 사람들이 함수형 접근방식보다 객체지향 접근방식을 선호한다. (이건 제생각)\n학습데이터와 테스트데이터의 스케일링시 동일한 변환을 유지하는 상황에서는 MinMaxScaler 가 유리함.\ninverse_transform 메서드를 같은 부가기능을 제공함."
  },
  {
    "objectID": "posts/05wk-019.html#a.-잘못된-스케일링-방법-비효율의-문제",
    "href": "posts/05wk-019.html#a.-잘못된-스케일링-방법-비효율의-문제",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 잘못된 스케일링 방법 – 비효율의 문제",
    "text": "A. 잘못된 스케일링 방법 – 비효율의 문제\n\nsklearn.preprocessing.minmax_scale(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsklearn.preprocessing.minmax_scale(XX)\n\narray([[0. ],\n       [0.5],\n       [1. ]])\n\n\n- 이 방법은 전략적으로 비효율적인 문제이지, 치팅과 관련된 치명적인 잘못은 아니다.\n\n만약에 어떠한 경우에 이러한 전처리 방식이 오히려 전략적이라고 판단될 경우 사용할수도 있음."
  },
  {
    "objectID": "posts/05wk-019.html#b.-올바른-스케일링-방법",
    "href": "posts/05wk-019.html#b.-올바른-스케일링-방법",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. 올바른 스케일링 방법",
    "text": "B. 올바른 스케일링 방법\n- 방법1\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit(X) # \n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nsclr.transform(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsclr.transform(XX)\n\narray([[0.125],\n       [0.375],\n       [0.625]])\n\n\n- 방법2\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit_transform(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsclr.transform(XX)\n\narray([[0.125],\n       [0.375],\n       [0.625]])"
  },
  {
    "objectID": "posts/05wk-019.html#c.-scaled_value-in-01",
    "href": "posts/05wk-019.html#c.-scaled_value-in-01",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "C. scaled_value \\(\\in\\) [0,1]?",
    "text": "C. scaled_value \\(\\in\\) [0,1]?\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit_transform(X)\nsclr.transform(XX)\n\narray([[0.16666667],\n       [0.5       ],\n       [1.33333333]])\n\n\n\n스케일링된 값이 1보다 클 수도 있다."
  },
  {
    "objectID": "posts/05wk-019.html#d.-아주-잘못된-스케일링-방법-정보누수",
    "href": "posts/05wk-019.html#d.-아주-잘못된-스케일링-방법-정보누수",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "D. 아주 잘못된 스케일링 방법 – 정보누수",
    "text": "D. 아주 잘못된 스케일링 방법 – 정보누수\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n- 데이터를 합친다.. (미쳤어??)\n\nnp.concatenate([X,XX])\n\narray([[1. ],\n       [2. ],\n       [3. ],\n       [4. ],\n       [3.5],\n       [1.5],\n       [2.5],\n       [5. ]])\n\n\n- 합친데이터에서 스케일링\n\nsklearn.preprocessing.minmax_scale(np.concatenate([X,XX]))[:5]\n\narray([[0.   ],\n       [0.25 ],\n       [0.5  ],\n       [0.75 ],\n       [0.625]])\n\n\n- 이러한 전처리 방식을 정보누수라고 한다. (대회 규정에 따라서 탈락사유에 해당함)"
  },
  {
    "objectID": "posts/04wk-015.html#a.-숫자형자료의-impute",
    "href": "posts/04wk-015.html#a.-숫자형자료의-impute",
    "title": "04wk-015: 결측치 처리, sklearn.impute",
    "section": "A. 숫자형자료의 impute",
    "text": "A. 숫자형자료의 impute\n- 주어진자료\n\ndf = pd.DataFrame({'A':[2.1,1.9,2.2,np.nan,1.9], 'B':[0,0,np.nan,0,0]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n- 빈칸은 대충 아래와 같이 추정하면 되지 않을까?\n\ndf.loc[3,'A'] = df.A.mean()\ndf.loc[2,'B'] = df.B.mean()\n\n\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.100\n0.0\n\n\n1\n1.900\n0.0\n\n\n2\n2.200\n0.0\n\n\n3\n2.025\n0.0\n\n\n4\n1.900\n0.0\n\n\n\n\n\n\n\n- 자동으로 하려면?\n\ndf = pd.DataFrame({'A':[2.1,1.9,2.2,np.nan,1.9], 'B':[0,0,np.nan,0,0]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n(방법1)\n\nimputer = sklearn.impute.SimpleImputer()\nimputer.fit(df)\nimputer.transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법2)\n\nimputer = sklearn.impute.SimpleImputer()\nimputer.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n- 다른방식으로 결측값 대체\n(방법1) – 평균으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='mean')\nimputer.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법2) – 중앙값으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='median')\nimputer.fit_transform(df)\n\narray([[2.1, 0. ],\n       [1.9, 0. ],\n       [2.2, 0. ],\n       [2. , 0. ],\n       [1.9, 0. ]])\n\n\n(방법3) – 최빈값으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimputer.fit_transform(df)\n\narray([[2.1, 0. ],\n       [1.9, 0. ],\n       [2.2, 0. ],\n       [1.9, 0. ],\n       [1.9, 0. ]])\n\n\n(방법4) – 상수대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='constant',fill_value=-999)\nimputer.fit_transform(df)\n\narray([[   2.1,    0. ],\n       [   1.9,    0. ],\n       [   2.2, -999. ],\n       [-999. ,    0. ],\n       [   1.9,    0. ]])"
  },
  {
    "objectID": "posts/04wk-015.html#b.-범주형자료의-impute",
    "href": "posts/04wk-015.html#b.-범주형자료의-impute",
    "title": "04wk-015: 결측치 처리, sklearn.impute",
    "section": "B. 범주형자료의 impute",
    "text": "B. 범주형자료의 impute\n- 자료\n\ndf = pd.DataFrame({'A':['Y','N','Y','Y',np.nan], 'B':['stat','math',np.nan,'stat','bio']})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n- 최빈값 혹은 상수대체만 가능\n(방법1) – 최빈값을 이용\n\nimptr = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimptr.fit_transform(df)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)\n\n\n(방법2) – 상수로 대체함\n\nimptr1 = sklearn.impute.SimpleImputer(strategy='constant',fill_value='Y')\nimptr1.fit_transform(df[['A']])\nimptr2 = sklearn.impute.SimpleImputer(strategy='constant',fill_value='math')\nimptr2.fit_transform(df[['B']])\n\narray([['stat'],\n       ['math'],\n       ['math'],\n       ['stat'],\n       ['bio']], dtype=object)\n\n\n\nnp.concatenate([imptr1.fit_transform(df[['A']]),imptr2.fit_transform(df[['B']])],axis=1)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)"
  },
  {
    "objectID": "posts/04wk-015.html#c.-혼합형자료의-impute-1-모두-최빈값으로-impute",
    "href": "posts/04wk-015.html#c.-혼합형자료의-impute-1-모두-최빈값으로-impute",
    "title": "04wk-015: 결측치 처리, sklearn.impute",
    "section": "C. 혼합형자료의 impute – (1) 모두 최빈값으로 impute",
    "text": "C. 혼합형자료의 impute – (1) 모두 최빈값으로 impute\n# 예제: 아래의 df에서 결측치를 모두 최빈값으로 impute하라.\n\ndf = pd.DataFrame(\n    {'A':[2.1,1.9,2.2,np.nan,1.9],\n     'B':[0,0,np.nan,0,0],\n     'C':['Y','N','Y','Y',np.nan], \n     'D':['stat','math',np.nan,'stat','bio']}\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n(풀이)\n\nimptr = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimptr.fit_transform(df)\n\narray([[2.1, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'N', 'math'],\n       [2.2, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'Y', 'bio']], dtype=object)\n\n\n#"
  },
  {
    "objectID": "posts/04wk-015.html#d.-혼합형자료의-impute-2-숫자형은-평균값으로-범주는-최빈값으로-impute",
    "href": "posts/04wk-015.html#d.-혼합형자료의-impute-2-숫자형은-평균값으로-범주는-최빈값으로-impute",
    "title": "04wk-015: 결측치 처리, sklearn.impute",
    "section": "D. 혼합형자료의 impute – (2) 숫자형은 평균값으로, 범주는 최빈값으로 impute",
    "text": "D. 혼합형자료의 impute – (2) 숫자형은 평균값으로, 범주는 최빈값으로 impute\n# 예제: 아래의 df를 숫자형일 경우는 평균대치, 문자형일 경우는 최빈값으로 대치하라.\n\ndf = pd.DataFrame(\n    {'A':[2.1,1.9,2.2,np.nan,1.9],\n     'B':[0,0,np.nan,0,0],\n     'C':['Y','N','Y','Y',np.nan], \n     'D':['stat','math',np.nan,'stat','bio']}\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n(풀이)\n- step1: 복사본 생성\n\ndf_imputed = df.copy()\ndf_imputed\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n- step2: 데이터프레임 분리\n\ndf_num = df.select_dtypes(include=\"number\")\ndf_num\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n\ndf_cat = df.select_dtypes(exclude=\"number\")\ndf_cat \n\n\n\n\n\n\n\n\nC\nD\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n- step3: impute\n\ndf_imputed[df_num.columns] = sklearn.impute.SimpleImputer(strategy='mean').fit_transform(df_num)\ndf_imputed[df_cat.columns] = sklearn.impute.SimpleImputer(strategy='most_frequent').fit_transform(df_cat)\n\n\ndf_imputed\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.100\n0.0\nY\nstat\n\n\n1\n1.900\n0.0\nN\nmath\n\n\n2\n2.200\n0.0\nY\nstat\n\n\n3\n2.025\n0.0\nY\nstat\n\n\n4\n1.900\n0.0\nY\nbio"
  },
  {
    "objectID": "offline.html",
    "href": "offline.html",
    "title": "대면수업",
    "section": "",
    "text": "기계학습활용은 비대면수업과 대면수업을 병행합니다. 본 페이지는 대면수업을 위한 강의자료입니다.\n공지사항\n\n대면수업은 학점과 무관한 수업입니다. (대면수업에 참여하지 않아도 학점에 불이익이 없습니다)\n9월7일 대면수업: Quarto를 이용한 기술블로그 만들기 + 타이타닉 자료에 대한 발표/토론\n9월12일 대면수업: 9월12일 대면수업은 Quarto를 이용한 기술블로그 만들기를 이어서 설명하겠습니다. 9월7일 대면수업 내용을 따라오신 상태이어야 합니다. (가능하면 지난시간에 실습했던 자리를 이어서 하는게 좋을거에요)\n9월21일 대면수업: Anaconda를 이용한 개발환경만들기\n\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 26, 2023\n\n\n2023-09-26 – 코드스페이스, 리눅스명령어, kaggle.json\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n2023-09-21 – Anaconda를 이용하여 autogluon 설치하기\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n2023-09-12 – Quarto Blog 만들기\n\n\n최규빈 \n\n\n\n\nSep 7, 2023\n\n\n2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "offline/230912.html",
    "href": "offline/230912.html",
    "title": "2023-09-12 – Quarto Blog 만들기",
    "section": "",
    "text": "0. 꼭 해야할까?\n1. https://www.youtube.com/watch?v=SZcjvjrdomE\n\nrepo도 확인함\ncommit 은 거의 기본임\n\n2. https://www.youtube.com/watch?v=Yc56NpYW1DM&t=23s\n\n3:00 - 3:40: 깃헙과 블로그가 중요함 / commit 을 빼곡하게, repo 확인\n3:40 - : 블로그\n4:00 - : commit 링크가 없으면 X, 필수라고 해서 넣은 느낌 (영상에서는 필수 아니라고 하지만 반 필수임)\n\n\n\n1. 블로그 생성\n- 깃헙레포지토리로 이동한뒤에 아래의 명령어를 입력:\nquarto create-project --type website --template blog\n위의 명령어를 실행하면 여러가지 설정파일들이 생성된다.\n- 블로그 publish\nquarto publish gh-pages\n- 깃헙으로 이동하여 확인\n\nwindow는 자동으로 퍼블리쉬 된다.\nmac에서는 수동으로 gh-pages로 바꿔야함.\n\n\n\n2. 새로운 포스트 생성\n- 아무 노트북이나 만들어본다. (혹은 이미 만들어진 노트북을 넣어본다.)\n\n예시1: https://guebin.github.io/PP2023/ 에서 아무거나 다운로드\n예시2: git clone https://github.com/guebin/PP2023.git\n\n- 노트북을 posts 폴더에 넣는다.\n- 아래를 통하여 preview 이미지를 확인.\nquarto preview \n- 퍼블리쉬\nquarto publish --no-prompt\n\n\n3. 블로그 설정변경\n- index.qmd 메인에 보이는 화면\n---\ntitle: \"기계학습활용 (2023)\"\nlisting:\n  contents: posts\n  sort: \"date\"\n  type: table\n  categories: true\n  sort-ui: false\n  filter-ui: false\npage-layout: full\ntitle-block-banner: false\n---\n\n**질문하는 방법**\n\n- 카카오톡: [질문하러 가기](http://pf.kakao.com/_skxnxdG/chat) // 학기종료이후 폐쇄함\n- 이메일: &lt;guebin@jbnu.ac.kr&gt;\n- 직접방문: 자연과학대학 본관 205호 \n- Zoom: *카카오톡이나 이메일로 미리 시간을 정할 것*\n- LMS쪽지: &lt;https://ieilms.jbnu.ac.kr/&gt;\n\n**references**\n\n**공지사항** \n\n- 강의영상의 경우 시간이 지나면 고화질로 시청할 수 있습니다. (인코딩중에는 유튜브에서 저화질로 송출됩니다)\n- **9월7일 대면수업**: Quarto를 이용한 기술블로그 만들기 + 타이타닉 자료에 대한 발표/토론 \n\n**강의노트** \n- _quarto.yml\nproject:\n  type: website\n\nwebsite:\n  title: \"MP2023\"\n  navbar:\n    right:\n      - icon: github    \n        href: https://github.com/guebin/MP2023\n      - icon: youtube\n        href: https://www.youtube.com/channel/UCQk9RyBNgXc7ORIsYlOfQrg/playlists?view=50&sort=dd&shelf_id=2\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n- posts 폴더에서 _metadata.yml\n# options specified here will apply to all posts in this folder\n\n# freeze computational output\n# (see https://quarto.org/docs/projects/code-execution.html#freeze)\nexecute:\n  freeze: auto  # re-render only when source changes\n\n# Enable banner style title blocks\ntitle-block-banner: false\n\nnotebook-links: true\ncap-location: margin\nreference-location: margin\ncitation-location: margin\n\nformat:\n  html:\n    toc: true\n    code-fold: false\n    code-line-numbers: false\n    code-copy: true\n  ipynb: default\n- 기타 자세한 옵션은 아래에서 뒤져볼 것\n\nref: https://quarto.org/\n\n- 다른 사람들의 블로그 소개 (설정파일 구경용)\n\nhttps://seoyeonc.github.io/sy_hub/\nhttps://boram-coco.github.io/coco/\nhttps://pinkocto.github.io/Quarto-Blog/\nhttps://pinkocto.github.io/noteda/\n\n\n\n4. 이해\n- 아래를 입력\ngit add .\ngit commit -m .\ngit switch gh-pages\n\n폴더내용 확인\n\n- 다시 원래대로 돌아오기\ngit switch main\n- quarto publish 의 역할\n\n.ipynb 파일을 어떠한 규칙에 따라 변환하여 .html파일로 바꿈\n바꾼 파일들을 gh-pages라는 이름의 브랜치로 옮김\ngh-pages 브랜치를 github 으로 push\n이후에는 github이 알아서 gh-pages의 내용을 웹이 뿌려줌.\n\n- 블로그가 깨졌을때 대응하는 방법\n\ngh-pages로 이동하여 모든 파일 및 폴더 삭제\ngit add ., git commit -m . git push를 순서대로 입력\n다시 main으로 돌아와서 quarto publish --no-prompt 입력\n\n\n\n5. 추천하는 루틴\ngit add .\ngit commit -m .\ngit push \nquarto publish --no-prompt"
  },
  {
    "objectID": "offline/230921.html",
    "href": "offline/230921.html",
    "title": "2023-09-21 – Anaconda를 이용하여 autogluon 설치하기",
    "section": "",
    "text": "1. 아나콘다 설치\n\n아나콘다 설치: https://www.anaconda.com/download\n\n\n\n2. 가상환경 mp 생성 + activate\nconda create -n mp python=3.10 \nconda activate mp \n\n\n3. autogluon 설치\nconda install -c conda-forge mamba\nmamba install -c conda-forge autogluon\n\n\n4. jupyterlab 설치 + 실행\nconda install -c conda-forge jupyterlab \njupyter lab \n\n\n5. pip install vs conda install\n- pip vs conda\n\npip은 특정 패키지만 설치, conda는 관련된 패키지를 모두 설치.\npip은 패키지간의 충돌이 발생할 수 있으며 conda는 이러한 충돌이 발생하지 않음.\npip은 최신버전을 설치하는 경향이 있으며 conda는 최신버전보다 약간 낮은 버전이 설치되는 경향이 있음 (최신버전의 경우 호환성여부가 조사되지 않았으므로).\n공식문서에는 pip과 conda를 이용한 설치가 모두 나와있으며, 보통은 conda를 이용하여 설치하는게 좋다. (scikit-learn, autogluon)\n\n\n\n6. 가상환경\n- 보통은 가상환경을 따로 만들고 패키지를 설치하는게 좋다.\n\n하나의 환경에 여러가지 패키지를 동시에 까는 경우, 패키지 숫자가 많아질수록 결국에는 충돌이 일어남.\n따라서 프로젝트별로 가상환경을 분리하여 관련 패키지를 설치하는 것이 좋다.\n\n- 요약\n\n본인컴퓨터로 작업하는 경우: 아나콘다 or 도커1를 이용하여 가상환경에서 작업할 것! 그렇지 않으면 컴퓨터가 더러워짐.2\n\n아나콘다나 도커를 사용하고 싶지 않다면, 코랩이나 캐글노트북 등 클라우드 컴퓨터에서 작업하는걸 추천\n클라우드 컴퓨터에서 작업하면 본인의 컴퓨터를 정리된 상태로 유지될 수 있다는 장점이 있지만, 새로운 컴퓨터를 할당받을때 마다 autogluon같은 패키지를 계속 설치해야하는 불편함이 있음.\n\n\n\n1 이건 아직 설명안했어요2 패키지 간의 종속성때문에 결국 고장남"
  }
]