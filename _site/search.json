[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "기계학습활용 (2023)",
    "section": "",
    "text": "질문하는 방법\n\n카카오톡: 질문하러 가기 // 학기종료이후 폐쇄함\n이메일: guebin@jbnu.ac.kr\n직접방문: 자연과학대학 본관 205호\nZoom: 카카오톡이나 이메일로 미리 시간을 정할 것\nLMS쪽지: https://ieilms.jbnu.ac.kr/\n\nreferences\n공지사항\n\n강의영상의 경우 시간이 지나면 고화질로 시청할 수 있습니다. (인코딩중에는 유튜브에서 저화질로 송출됩니다)\n\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nNov 21, 2023\n\n\n13wk-57: House Prices / 자료분석(Autogluon)\n\n\n최규빈 \n\n\n\n\nDec 1, 2023\n\n\n13wk-56: 타이타닉 / 자료분석(Autogluon)\n\n\n최규빈 \n\n\n\n\nDec 1, 2023\n\n\n13wk-55: Medical Cost / 자료분석(Autogluon)\n\n\n최규빈 \n\n\n\n\nDec 1, 2023\n\n\n13wk-54: 체중감량(교호작용) / 자료분석(Autogluon)\n\n\n최규빈 \n\n\n\n\nDec 1, 2023\n\n\n13wk-53: 취업(다중공선성) / 자료분석(Autogluon)\n\n\n최규빈 \n\n\n\n\nDec 1, 2023\n\n\n13wk-52: 취업(오버피팅) / 자료분석(Autogluon)\n\n\n최규빈 \n\n\n\n\nNov 21, 2023\n\n\n13wk-51: 아이스크림(type무의미) / 자료분석(Autogluon)\n\n\n최규빈 \n\n\n\n\nNov 21, 2023\n\n\n13wk-50: 아이스크림(이상치) / 자료분석(Autogluon)\n\n\n최규빈 \n\n\n\n\nDec 1, 2023\n\n\n13wk-49: 키와 몸무게 (결측치, 성별교호작용) / 자료분석(Autogluon)\n\n\n최규빈 \n\n\n\n\nDec 1, 2023\n\n\n13wk-48: 아이스크림 / 자료분석(Autogluon)\n\n\n최규빈 \n\n\n\n\nDec 1, 2023\n\n\n13wk-47: 평가지표의 계산\n\n\n최규빈 \n\n\n\n\nNov 21, 2023\n\n\n12wk-46: 평가지표의 이해\n\n\n최규빈 \n\n\n\n\nNov 21, 2023\n\n\n12wk-45: 아이스크림 / 부스팅\n\n\n최규빈 \n\n\n\n\nNov 21, 2023\n\n\n12wk-44: Medical Cost / 랜덤포레스트\n\n\n최규빈 \n\n\n\n\nNov 16, 2023\n\n\n11wk-43: 아이스크림 판매량 / 배깅\n\n\n최규빈 \n\n\n\n\nNov 16, 2023\n\n\n11wk-42: Weighted_Data / 의사결정나무 weights\n\n\n최규빈 \n\n\n\n\nNov 16, 2023\n\n\n11wk-41: Medical Cost / 의사결정나무 max_feature,random_state\n\n\n최규빈 \n\n\n\n\nNov 16, 2023\n\n\n11wk-40: Medical Cost / 의사결정나무의 시각화\n\n\n최규빈 \n\n\n\n\nNov 10, 2023\n\n\n10wk-39: 의사결정나무 Discussion\n\n\n최규빈 \n\n\n\n\nNov 10, 2023\n\n\n10wk-38: 아이스크림 – 의사결정나무 원리\n\n\n최규빈 \n\n\n\n\nNov 10, 2023\n\n\n10wk-37: 아이스크림 – 의사결정나무, max_depth\n\n\n최규빈 \n\n\n\n\nNov 10, 2023\n\n\n10wk-36: 애니메이션\n\n\n최규빈 \n\n\n\n\nNov 1, 2023\n\n\n09wk-mid (ver 1.0) – 풀이업로드\n\n\n최규빈 \n\n\n\n\nOct 24, 2023\n\n\n08wk-supp1: 중간점검\n\n\n최규빈 \n\n\n\n\nOct 16, 2023\n\n\n07wk-35: 아이스크림(이상치) / 의사결정나무\n\n\n최규빈 \n\n\n\n\nOct 16, 2023\n\n\n07wk-34: 취업(오버피팅) / 의사결정나무\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-33: 취업(다중공선성) / 의사결정나무\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-32: 아이스크림(교호작용) / 의사결정나무\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-31: 체중감량(교호작용) / 의사결정나무\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-30: 아이스크림(교호작용) / 회귀분석\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-29: 체중감량(교호작용) / 회귀분석 – 추가해설\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-28: 선형모형의 적\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-27: 아이스크림(이상치) / 회귀분석 – 추가해설\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n06wk-26: 취업(다중공선성) / LassoCV\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n06wk-25: 취업(다중공선성) / Lasso\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n06wk-24: 취업(다중공선성) / RidgeCV\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n06wk-23: 취업(다중공선성) / Ridge – 추가해설\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n06wk-22: 취업(다중공선성) / 다중공선성의 개념 – 추가해설\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-21: 취업(오버피팅) / 오버피팅의 개념 – 추가해설\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-20: StandardScaler를 이용한 전처리\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-19: MinMaxScaler를 이용한 전처리\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-18: Predictor의 깊은 이해, 기호정리\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-17: 취업 / 로지스틱의 깊은 이해\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-16: 타이타닉 / 결측치처리+로지스틱 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-15: sklearn.impute를 이용한 결측치 처리\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-14: msno를 이용한 결측치 시각화\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-13: 타이타닉 / 로지스틱 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-12: 취업 / 로지스틱 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-11: Medical Cost / 회귀분석\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-10: 아이스크림(범주) / 회귀분석 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-09: 아이스크림 / 회귀분석 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-08: 타이타닉 / Autogluon (best_quality)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-07: 타이타닉 / Autogluon (Fsize,Drop)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-06: 타이타닉 / Autogluon (Fsize)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-05: 타이타닉 / Autogluon\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-04: 타이타닉 / Alexis Cook의 코드\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-03: 타이타닉 / 첫 제출\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-02: 타이타닉 데이터의 이해 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-01: 캐글소개\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "offline/230907.html",
    "href": "offline/230907.html",
    "title": "2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github",
    "section": "",
    "text": "- 윈도우 컴퓨터인 경우 새로운 계정을 만들어 준다.\n\n한글계정명일 경우는 필수\n영어계정이더라도 계정을 분리하는게 관리에 유리\n\n- 최종적으로는\n\ngithub 로그인 + repository 생성완료\nanaconda 설치완료\ngit 설치완료\nquarto 설치완료\n\n이어야 한다.\n\n\n\ngithub 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리.\n\n\n\n\n\n아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n그림1그림2\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨\n\n\n\n\n\n\n\n\n\ngit 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시\n\n\n\n\n\n\nquarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "offline/230907.html#a.-github",
    "href": "offline/230907.html#a.-github",
    "title": "2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github",
    "section": "",
    "text": "github 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리."
  },
  {
    "objectID": "offline/230907.html#b.-anaconda",
    "href": "offline/230907.html#b.-anaconda",
    "title": "2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github",
    "section": "",
    "text": "아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n그림1그림2\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨"
  },
  {
    "objectID": "offline/230907.html#c.-git",
    "href": "offline/230907.html#c.-git",
    "title": "2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github",
    "section": "",
    "text": "git 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "offline/230907.html#d.-quarto",
    "href": "offline/230907.html#d.-quarto",
    "title": "2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github",
    "section": "",
    "text": "quarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "offline/230926.html",
    "href": "offline/230926.html",
    "title": "2023-09-26 – 코드스페이스, 리눅스명령어, kaggle.json",
    "section": "",
    "text": "1. 코드스페이스\n- 장점:\n\n슈퍼장점: 자동으로 깃헙연결 되어있음 (요즘 깃헙쓰기 짜증나서 이게 장점임)\n로칼컴퓨터대비 장점: 내 컴퓨터 깨끗하게 쓸 수 있음 + 내 맘대로 무한 리눅스서버 사용가능\n코랩/kaggle 대비 장점: 한번 정리된 셋팅이 영구적으로 적용됨. (autogluon 맨날 안깔아도 된다)\n\n- 단점:\n\n컴터가 별로임, 사용시간 제한있음 (월 100시간?)\n단점을 극복하려면 결재해야함\n\n- 초기설정: 복붙설정하기, 주피터 extension 설치\n\n\n2. kaggle.json\n\n\n3. 간단한 리눅스 명령어\n- 이동: ls, cd\n- 폴더생성 및 삭제: mkdir, rm -rf\n- 복붙: cp/mv\n- zip압출풀기: unzip\n\n리눅스에 익숙해지면 코랩, 캐글등을 쉽게 사용할 수 있다.\n\n\n코랩 혹은 캐글에서 사용하지 않더라도, 코랩 혹은 캐글에서 사용하는 코드를 쉽게 해석할 수 있다.\n\n\n\n4. 타이타닉 데이터 불러오기"
  },
  {
    "objectID": "offline/231130.html",
    "href": "offline/231130.html",
    "title": "2023-11-30 – 원격컴퓨터에서 개발환경 셋팅하기 (2)",
    "section": "",
    "text": "1. CodeSpace설치\n- 아래를 이용하여 codespace 설치\napt-get install curl\ncurl -Lk 'https://code.visualstudio.com/sha/download?build=stable&os=cli-alpine-x64' --output vscode_cli.tar.gz\ntar -xf vscode_cli.tar.gz\n- 아래를 이용하여 초기설정 (깃헙아이디로 인증)\n./code tunnel \n- 초기설정 이후 아래를 이용하여 백그라운드에서 실행\nnohup ./code tunnel &gt; /dev/null 2&gt;&1 &\n- 아래의 익스텐션을 설치하면 편리함"
  },
  {
    "objectID": "offline/231114.html",
    "href": "offline/231114.html",
    "title": "2023-11-14 – 개발환경의 변천사",
    "section": "",
    "text": "주의사항!!\n\n\n\n본 강의노트는 작성자의 상상에 근거하여 작성되었으며, 사실이 아닌 내용이 포함되어 있을 수 있습니다. (특히 옛날사람들의 코딩습관들)"
  },
  {
    "objectID": "offline/231114.html#a.-0세대-프로그래머-프롬프트",
    "href": "offline/231114.html#a.-0세대-프로그래머-프롬프트",
    "title": "2023-11-14 – 개발환경의 변천사",
    "section": "A. 0세대 프로그래머 (프롬프트)",
    "text": "A. 0세대 프로그래머 (프롬프트)\n- 특징: 프롬프트만 쓴다..\n# 실습1 – python 사용\n- 윈도우에서 anaconda prompt 실행 -&gt; python\n(base) C:\\Users\\python&gt;python\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; [1,2,3]+[4]\n[1, 2, 3, 4]\n&gt;&gt;&gt; a=[1,2,3]+[4]\n&gt;&gt;&gt; a\n[1, 2, 3, 4]\n- 2개를 실행할 수도 있음. (두 환경은 각각 서로 독립적인 파이썬, 변수가 공유되지 않음) \\(\\star\\)\n- 아쉬운점: `?list’와 같이 도움말 기능이 동작하지 않음\n&gt;&gt;&gt; ?list\n  File \"&lt;stdin&gt;\", line 1\n    ?list\n    ^\nSyntaxError: invalid syntax\n&gt;&gt;&gt; \n#\n# 실습2 – ipython 사용\n- 윈도우에서 anaconda prompt 실행 -&gt; ipython\n(base) C:\\Users\\python&gt;ipython\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.29.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: a=[1,2,3]\n\nIn [2]: a\nOut[2]: [1, 2, 3]\n\nIn [3]: a+[4]\nOut[3]: [1, 2, 3, 4]\n- ?list가 가능\nIn [4]: ?list\nInit signature: list(iterable=(), /)\nDocstring:\nBuilt-in mutable sequence.\n\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\nType:           type\nSubclasses:     _HashedSeq, StackSummary, DeferredConfigList, SList, _ImmutableLineList, FormattedText, NodeList, _ExplodedList, Stack, _Accumulator, ...\n\n- 색깔이 알록달록해서 문법을 보기 편하다. (구문강조)\n#\n# 실습3 – 0세대 프로그래머의 삶 with python\n- 1부터 10까지 합을 구하는 프로그램을 만들고 싶음\n- 시도1: python을 키고 아래와 같이 실행\n(base) C:\\Users\\python&gt;python\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; total = 0\n&gt;&gt;&gt; for i in range(10):\n...     total=total+i\n...\n&gt;&gt;&gt; total\n45\n&gt;&gt;&gt;\n- 반성: 정답은 55인데 45가 출력되었다! \\(\\to\\) range(10)을 range(1,11)으로 바꿔야겠다!\n- 시도2: range(1,11)을 바꿔야겠다고 생각하고 다시 입력하다가 오타가 발생\n&gt;&gt;&gt; total =0\n&gt;&gt;&gt; for i in range(1,11):\n...     total = totla +i\n...\n\n앗 totla이라고 잘못쳤다.\n\n- 반성: 다음에는 정신을 똑바로 차려야겠다.\n- 불편한점: … 다..\n#\n# 실습4 – 1세대 프로그래머의 삶 with ipython\n- ipython을 사용한 프로그래머는 좀더 상황이 낫다\n(base) C:\\Users\\python&gt;ipython\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.29.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: total = 0\n\nIn [2]: for i in range(1,11):\n   ...:     total = total + i\n   ...:\n\nIn [3]: total\nOut[3]: 55\n\n편한점1: 자동으로 들여쓰기가 되어서 편함\n편한점2: 화살표를 이용해서 for문을 쓰는 도중에 위아래로 이동가능\n불편한점1: 화살표로 이동할수는 있는데 마우스로는 이동할 수 없다.\n불편한점2: 내가 작성한 코드를 관리하기 어렵다.\n\n#"
  },
  {
    "objectID": "offline/231114.html#b.-1세대-프로그래머-스크립트활용",
    "href": "offline/231114.html#b.-1세대-프로그래머-스크립트활용",
    "title": "2023-11-14 – 개발환경의 변천사",
    "section": "B. 1세대 프로그래머 (스크립트활용)",
    "text": "B. 1세대 프로그래머 (스크립트활용)\n- 특징: 스크립트 + 프롬프트를 사용\n# 실습1 – python + .py\n- 메모장을 키고 아래의 내용을 적는다.\ntotal = 0 \nfor i in range(1,11): \n    total = total + i\nprint(total)\n- 파일이름을 mysum.py로 저장한다.\n- anaconda prompt에서 mysum.py파일이 저장된 폴더로 이동 -&gt; 실행\n(base) C:\\Users\\python&gt;cd Desktop\n\n(base) C:\\Users\\python\\Desktop&gt;dir\n C 드라이브의 볼륨에는 이름이 없습니다.\n 볼륨 일련 번호: 9AFD-A05F\n\n C:\\Users\\python\\Desktop 디렉터리\n\n2022-03-27  오전 11:32    &lt;DIR&gt;          .\n2022-03-27  오전 11:32    &lt;DIR&gt;          ..\n2022-03-27  오전 12:01             2,306 Chrome.lnk\n2022-03-26  오후 08:32             2,332 Microsoft Edge.lnk\n2022-03-27  오전 11:33                71 mysum.py\n               3개 파일               4,709 바이트\n               2개 디렉터리  743,643,467,776 바이트 남음\n\n(base) C:\\Users\\python\\Desktop&gt;python mysum.py\n55\n\n(base) C:\\Users\\python\\Desktop&gt;\n- 소감\n\n편한점1: 마우스를 이용하여 이동가능\n편한점2: 내가 작업한 내용은 바탕화면의 메모장에 저장이 되어있음\n아쉬운점: ipython의 장점은 활용못함 (구문강조, 도움말기능)\n\n#\n# 실습2 – ipython + .py\n- 전체적인 개발방식\n\n메모장: 코드를 편집, 저장\nipython: anaconda prompt처럼 메모장의 코드를 실행하고 결과를 확인 + 구문강조, 도움말확인기능 등을 이용하여 짧은 코드를 빠르게 작성\n\n- 기능\n\nipython에서 !python mysum.py를 입력하면 anaconda prompt에서 python mysum.py를 입력한 것과 같은 효과\nipython에서 %run mysum을 입력하면 메모장에서 mysum.py에 입력된 내용을 복사해서 ipython에 붙여넣어 실행한것과 같은 효과\n\n#"
  },
  {
    "objectID": "offline/231114.html#c.-2세대-프로그래머-ide-활용",
    "href": "offline/231114.html#c.-2세대-프로그래머-ide-활용",
    "title": "2023-11-14 – 개발환경의 변천사",
    "section": "C. 2세대 프로그래머 (IDE 활용)",
    "text": "C. 2세대 프로그래머 (IDE 활용)\n- 메모장과 ipython을 하나로 통합한 프로그램이 등장!\n\njupyter notebook, jupyter lab\nspyder\nidle\nVScode\n…\n\n- 주피터의 트릭 (실제로 주피터는 ipython에 기생할 뿐 아무런 역할도 안해요)\n\n주피터를 실행\n새노트북을 생성 (파이썬으로 선택)\n\n\n컴퓨터는 내부적으로 ipython을 실행하고 그 ipython이랑 여러분이 방금만든 그 노트북과 연결\n\n\n처음보이는 cell에 1+1을 입력 -&gt; 쉬프트엔터 -&gt; 결과2가 출력\n\n\n처음보이는 cell하나 = 자동으로 열린 하나의 메모장\ncell 1+1을 입력 = 메모장에 1+1을 적음\n쉬프트+엔터후 결과2를 출력 = cell의 내용을 복사 -&gt; ipython에 붙여넣음 -&gt; ipython 계산된 결과를 복사 -&gt; cell로 돌아와 붙여넣기\n\n\n새로운 cell을 추가하고 2+2을 입력 -&gt; 쉬프트엔터 -&gt; 결과4가 출력\n\n\n새로운 cell을 추가 = 새로운 메모장 추가\ncell 2+2을 입력 = 새로운 메모장에 2+2를 적음\n쉬프트+엔터후 결과4를 출력 = cell의 내용을 복사 -&gt; ipython에 붙여넣음 -&gt; ipython 계산된 결과를 복사 -&gt; cell로 돌아와 붙여넣기\n\n- 중요한 사실들\n\nIDE는 내부적으로 연산을 수행하는 능력이 없다. (생각해볼것: 왜 R을 꼭 설치하고 Rstudio를 설치해야 했을까?)\n주피터에서 커널을 재시작한다는 의미는 메모장이 열린채로 ipython을 껐다가 다시 실행한다는 의미\n주피터는 단순히 ’메모장의 내용을 복사하여 붙여넣는 기계’라고 볼 수 있다. 이렇게 생각하면 주피터는 꼭 ipython에 연결할 이유는 없다. 실제로 주피터에 R을 연결해서 쓸 수 있다. 즉 하나의 IDE가 여러개의 언어와 연결될 수 있다.\nJupyterlab이라는 프로그램은 크롬에 있는 내용과 ipython간의 통신을 제어하는 프로그램일 뿐이다."
  },
  {
    "objectID": "offline/231114.html#d.-3세대-프로그래머-가상환경",
    "href": "offline/231114.html#d.-3세대-프로그래머-가상환경",
    "title": "2023-11-14 – 개발환경의 변천사",
    "section": "D. 3세대 프로그래머 (가상환경)",
    "text": "D. 3세대 프로그래머 (가상환경)"
  },
  {
    "objectID": "offline/231114.html#e.-4세대-프로그래머-원격컴퓨터",
    "href": "offline/231114.html#e.-4세대-프로그래머-원격컴퓨터",
    "title": "2023-11-14 – 개발환경의 변천사",
    "section": "E. 4세대 프로그래머 (원격컴퓨터)",
    "text": "E. 4세대 프로그래머 (원격컴퓨터)"
  },
  {
    "objectID": "posts/13wk-47.html",
    "href": "posts/13wk-47.html",
    "title": "13wk-47: 평가지표의 계산",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport sklearn.metrics\n\n\n\n3. 12wk-46 숙제풀이\n밀실안에 100명의 사람이 있다고 하자. 이중 이중휴민트는 2명이 있다고 하자. 이중휴민트를 잡기위해서 3명을 사살했다고 하자. 사살된 사람중 실제 이중휴민트는 1명이었다고 하자. 이 경우\n\naccurary\nTPR(=recall)\nprecision\nFPR\n\n값을 계산하라.\n- y,yhat 생성\n\ny=np.array([1]*2+[0]*98)\ny\n\narray([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nyhat = np.array([0,1,1,1]+[0]*96)\nyhat\n\narray([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n- 혼동행렬(=confusion matrix) 구하기\n\nsklearn.metrics.confusion_matrix(y,yhat)\n\narray([[96,  2],\n       [ 1,  1]])\n\n\n\n(tn,fp), (fn,tp) = sklearn.metrics.confusion_matrix(y,yhat)\n\n- accuracy\n\n(tp+tn)/100 # accuracy\n\n0.97\n\n\n\nsklearn.metrics.accuracy_score(y,yhat)\n\n0.97\n\n\n- recall\n\ntp/(tp+fn)\n\n0.5\n\n\n\nsklearn.metrics.recall_score(y,yhat)\n\n0.5\n\n\n- precision\n\ntp/(tp+fp)\n\n0.3333333333333333\n\n\n\nsklearn.metrics.precision_score(y,yhat)\n\n0.3333333333333333\n\n\n- FPR\n\nfp / (fp + tn)\n\n0.02040816326530612"
  },
  {
    "objectID": "posts/02wk-08.html#a.-데이터",
    "href": "posts/02wk-08.html#a.-데이터",
    "title": "02wk-08: 타이타닉 / Autogluon (best_quality)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\n!kaggle competitions download -c titanic\n!unzip titanic.zip -d ./titanic\ndf_train = TabularDataset('titanic/train.csv')\ndf_test = TabularDataset('titanic/test.csv')\n!rm titanic.zip\n!rm -rf titanic/\n\nDownloading titanic.zip to /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\n  0%|                                               | 0.00/34.1k [00:00&lt;?, ?B/s]\n100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 5.34MB/s]\nArchive:  titanic.zip\n  inflating: ./titanic/gender_submission.csv  \n  inflating: ./titanic/test.csv      \n  inflating: ./titanic/train.csv"
  },
  {
    "objectID": "posts/02wk-08.html#b.-predictor-생성",
    "href": "posts/02wk-08.html#b.-predictor-생성",
    "title": "02wk-08: 타이타닉 / Autogluon (best_quality)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231024_084544\""
  },
  {
    "objectID": "posts/02wk-08.html#c.-적합fit",
    "href": "posts/02wk-08.html#c.-적합fit",
    "title": "02wk-08: 타이타닉 / Autogluon (best_quality)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(df_train,presets='best_quality') # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nPresets specified: ['best_quality']\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231024_084544\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   265.20 GB / 490.57 GB (54.1%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    123275.96 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2417.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.21s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6308   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6364   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\nWill use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install ray==2.6.3`\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.835    = Validation score   (accuracy)\n    1.39s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8406   = Validation score   (accuracy)\n    1.18s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8373   = Validation score   (accuracy)\n    0.6s     = Training   runtime\n    0.09s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8361   = Validation score   (accuracy)\n    0.36s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8608   = Validation score   (accuracy)\n    4.16s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8294   = Validation score   (accuracy)\n    0.69s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8328   = Validation score   (accuracy)\n    0.65s    = Training   runtime\n    0.11s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\nNo improvement since epoch 7: early stopping\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\nNo improvement since epoch 6: early stopping\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\nNo improvement since epoch 7: early stopping\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n    0.853    = Validation score   (accuracy)\n    10.32s   = Training   runtime\n    0.14s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n    0.8406   = Validation score   (accuracy)\n    1.14s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8462   = Validation score   (accuracy)\n    31.34s   = Training   runtime\n    0.15s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8406   = Validation score   (accuracy)\n    3.43s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8608   = Validation score   (accuracy)\n    0.61s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 57.87s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231024_084544\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fd0b87ab370&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.860831       0.028274   4.163593                0.028274           4.163593            1       True          7\n1       WeightedEnsemble_L2   0.860831       0.029458   4.777558                0.001185           0.613964            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.852974       0.140585  10.323885                0.140585          10.323885            1       True         10\n3     NeuralNetTorch_BAG_L1   0.846240       0.148010  31.344297                0.148010          31.344297            1       True         12\n4           LightGBM_BAG_L1   0.840629       0.022044   1.180209                0.022044           1.180209            1       True          4\n5      LightGBMLarge_BAG_L1   0.840629       0.025207   3.430948                0.025207           3.430948            1       True         13\n6            XGBoost_BAG_L1   0.840629       0.032869   1.136343                0.032869           1.136343            1       True         11\n7   RandomForestGini_BAG_L1   0.837262       0.093387   0.595516                0.093387           0.595516            1       True          5\n8   RandomForestEntr_BAG_L1   0.836139       0.074692   0.359487                0.074692           0.359487            1       True          6\n9         LightGBMXT_BAG_L1   0.835017       0.023820   1.392500                0.023820           1.392500            1       True          3\n10    ExtraTreesEntr_BAG_L1   0.832772       0.105990   0.649323                0.105990           0.649323            1       True          9\n11    ExtraTreesGini_BAG_L1   0.829405       0.071771   0.686410                0.071771           0.686410            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.014100   0.004636                0.014100           0.004636            1       True          2\n13    KNeighborsUnif_BAG_L1   0.630752       0.031787   0.003633                0.031787           0.003633            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.860831\n0.028274\n4.163593\n0.028274\n4.163593\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.860831\n0.029458\n4.777558\n0.001185\n0.613964\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.852974\n0.140585\n10.323885\n0.140585\n10.323885\n1\nTrue\n10\n\n\n3\nNeuralNetTorch_BAG_L1\n0.846240\n0.148010\n31.344297\n0.148010\n31.344297\n1\nTrue\n12\n\n\n4\nLightGBM_BAG_L1\n0.840629\n0.022044\n1.180209\n0.022044\n1.180209\n1\nTrue\n4\n\n\n5\nLightGBMLarge_BAG_L1\n0.840629\n0.025207\n3.430948\n0.025207\n3.430948\n1\nTrue\n13\n\n\n6\nXGBoost_BAG_L1\n0.840629\n0.032869\n1.136343\n0.032869\n1.136343\n1\nTrue\n11\n\n\n7\nRandomForestGini_BAG_L1\n0.837262\n0.093387\n0.595516\n0.093387\n0.595516\n1\nTrue\n5\n\n\n8\nRandomForestEntr_BAG_L1\n0.836139\n0.074692\n0.359487\n0.074692\n0.359487\n1\nTrue\n6\n\n\n9\nLightGBMXT_BAG_L1\n0.835017\n0.023820\n1.392500\n0.023820\n1.392500\n1\nTrue\n3\n\n\n10\nExtraTreesEntr_BAG_L1\n0.832772\n0.105990\n0.649323\n0.105990\n0.649323\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.829405\n0.071771\n0.686410\n0.071771\n0.686410\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.014100\n0.004636\n0.014100\n0.004636\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.630752\n0.031787\n0.003633\n0.031787\n0.003633\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-08.html#d.-예측-predict",
    "href": "posts/02wk-08.html#d.-예측-predict",
    "title": "02wk-08: 타이타닉 / Autogluon (best_quality)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(df_train.Survived == predictr.predict(df_train)).mean()\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n\n\n0.898989898989899\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\n# df_test.assign(Survived = predictr.predict(df_test)).loc[:,['PassengerId','Survived']]\\\n# .to_csv(\"autogluon(best_quality)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/13wk-56.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-56.html#a.-y의-분포-xy의-관계-시각화",
    "title": "13wk-56: 타이타닉 / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train, \n    label='Survived',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nSurvived\n891\n0.383838\n0.486592\n0.0\n0.0\n0.0\n1.0\n1.0\nint64\n2\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for Survived &gt;= 0.5\n\n\n\n\n\nFeature interaction between Sex/Survived in train_data"
  },
  {
    "objectID": "posts/13wk-56.html#target-variable-analysis",
    "href": "posts/13wk-56.html#target-variable-analysis",
    "title": "13wk-56: 타이타닉 / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-56.html#b.-중요한-설명변수",
    "href": "posts/13wk-56.html#b.-중요한-설명변수",
    "title": "13wk-56: 타이타닉 / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data = df_train, \n    label='Survived',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231130_200338/\"\n\n\nModel Prediction for Survived\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n0.809701\n0.856\n0.002972\n0.003853\n0.388687\n0.002972\n0.003853\n0.388687\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\nSex\n0.112687\n0.013033\n0.000021\n5\n0.139522\n0.085851\n\n\nName\n0.055970\n0.009140\n0.000082\n5\n0.074789\n0.037151\n\n\nSibSp\n0.026119\n0.010554\n0.002605\n5\n0.047850\n0.004389\n\n\nFare\n0.012687\n0.009730\n0.021720\n5\n0.032721\n-0.007348\n\n\nEmbarked\n0.011194\n0.006981\n0.011525\n5\n0.025567\n-0.003179\n\n\nAge\n0.010448\n0.003122\n0.000853\n5\n0.016876\n0.004020\n\n\nPassengerId\n0.008955\n0.005659\n0.012022\n5\n0.020607\n-0.002696\n\n\nCabin\n0.002985\n0.006675\n0.186950\n5\n0.016729\n-0.010758\n\n\nPclass\n0.002239\n0.005659\n0.213159\n5\n0.013890\n-0.009413\n\n\nParch\n0.001493\n0.002044\n0.088904\n5\n0.005701\n-0.002716\n\n\nTicket\n0.000000\n0.000000\n0.500000\n5\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nSurvived\n0\n1\nerror\n\n\n\n\n498\n499\n1\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25.0\n1\n2\n113781\n151.5500\nC22 C26\nS\n0\n0.046788\n0.953212\n0.906424\n\n\n267\n268\n3\nPersson, Mr. Ernst Ulrik\nmale\n25.0\n1\n0\n347083\n7.7750\nNaN\nS\n1\n0.932024\n0.067976\n0.864047\n\n\n569\n570\n3\nJonsson, Mr. Carl\nmale\n32.0\n0\n0\n350417\n7.8542\nNaN\nS\n1\n0.922265\n0.077735\n0.844530\n\n\n283\n284\n3\nDorking, Mr. Edward Arthur\nmale\n19.0\n0\n0\nA/5. 10482\n8.0500\nNaN\nS\n1\n0.921180\n0.078820\n0.842361\n\n\n821\n822\n3\nLulic, Mr. Nikola\nmale\n27.0\n0\n0\n315098\n8.6625\nNaN\nS\n1\n0.919709\n0.080291\n0.839419\n\n\n301\n302\n3\nMcCoy, Mr. Bernard\nmale\nNaN\n2\n0\n367226\n23.2500\nNaN\nQ\n1\n0.918546\n0.081454\n0.837093\n\n\n288\n289\n2\nHosono, Mr. Masabumi\nmale\n42.0\n0\n0\n237798\n13.0000\nNaN\nS\n1\n0.907043\n0.092957\n0.814085\n\n\n36\n37\n3\nMamee, Mr. Hanna\nmale\nNaN\n0\n0\n2677\n7.2292\nNaN\nC\n1\n0.906803\n0.093197\n0.813605\n\n\n127\n128\n3\nMadsen, Mr. Fridtjof Arne\nmale\n24.0\n0\n0\nC 17369\n7.1417\nNaN\nS\n1\n0.906605\n0.093395\n0.813210\n\n\n391\n392\n3\nJansson, Mr. Carl Olof\nmale\n21.0\n0\n0\n350034\n7.7958\nNaN\nS\n1\n0.905367\n0.094633\n0.810734\n\n\n\n\n\n\n\nRows with the least distance vs other class\n\n\nRows in this category are the closest to the decision boundary vs the other class and are good candidates for additional labeling\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nSurvived\n0\n1\nerror\n\n\n\n\n347\n348\n3\nDavison, Mrs. Thomas Henry (Mary E Finck)\nfemale\nNaN\n1\n0\n386525\n16.1000\nNaN\nS\n1\n0.510786\n0.489214\n0.021572\n\n\n192\n193\n3\nAndersen-Jensen, Miss. Carla Christine Nielsine\nfemale\n19.0\n1\n0\n350046\n7.8542\nNaN\nS\n1\n0.512167\n0.487833\n0.024334\n\n\n172\n173\n3\nJohnson, Miss. Eleanor Ileen\nfemale\n1.0\n1\n1\n347742\n11.1333\nNaN\nS\n1\n0.526793\n0.473207\n0.053585\n\n\n328\n329\n3\nGoldsmith, Mrs. Frank John (Emily Alice Brown)\nfemale\n31.0\n1\n1\n363291\n20.5250\nNaN\nS\n1\n0.531574\n0.468426\n0.063149\n\n\n593\n594\n3\nBourke, Miss. Mary\nfemale\nNaN\n0\n2\n364848\n7.7500\nNaN\nQ\n0\n0.463840\n0.536160\n0.072319\n\n\n376\n377\n3\nLandergren, Miss. Aurora Adelia\nfemale\n22.0\n0\n0\nC 7077\n7.2500\nNaN\nS\n1\n0.549471\n0.450529\n0.098942\n\n\n607\n608\n1\nDaniel, Mr. Robert Williams\nmale\n27.0\n0\n0\n113804\n30.5000\nNaN\nS\n1\n0.558802\n0.441198\n0.117605\n\n\n113\n114\n3\nJussila, Miss. Katriina\nfemale\n20.0\n1\n0\n4136\n9.8250\nNaN\nS\n0\n0.439738\n0.560262\n0.120524\n\n\n889\n890\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n1\n0.568476\n0.431524\n0.136952\n\n\n18\n19\n3\nVander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\nfemale\n31.0\n1\n0\n345763\n18.0000\nNaN\nS\n0\n0.418177\n0.581823\n0.163645"
  },
  {
    "objectID": "posts/13wk-56.html#c.-관측치별-해석",
    "href": "posts/13wk-56.html#c.-관측치별-해석",
    "title": "13wk-56: 타이타닉 / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n- 1번관측치\n\npredictr.predict(df_train.iloc[[1]])\n\n1    1\nName: Survived, dtype: int64\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[1]]\n)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n\n\n\n\n\n\n\n\n\n이름이 중요하다?\n\n- 이름을 바꿔보자..\n\nonerow = df_train.iloc[[1]].copy()\n\n\nonerow['Name']\n\n1    Cumings, Mrs. John Bradley (Florence Briggs Thayer)\nName: Name, dtype: object\n\n\n\nonerow['Name'] = 'Cumings, Mr. John Bradley'\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=onerow\n)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n2\n1\n1\nCumings, Mr. John Bradley\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n\n\n\n\n\n\n\n\n\n남자라는거징.."
  },
  {
    "objectID": "posts/13wk-49.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-49.html#a.-y의-분포-xy의-관계-시각화",
    "title": "13wk-49: 키와 몸무게 (결측치, 성별교호작용) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train, \n    label='height',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nheight\n280\n174.605431\n9.430102\n148.975298\n167.572671\n175.186487\n181.132612\n195.797169\nfloat64\n280\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for height &gt;= 0.5\n\n\n\n\n\nFeature interaction between weight/height in train_data\n\n\n\n\n\nFeature interaction between sex/height in train_data"
  },
  {
    "objectID": "posts/13wk-49.html#target-variable-analysis",
    "href": "posts/13wk-49.html#target-variable-analysis",
    "title": "13wk-49: 키와 몸무게 (결측치, 성별교호작용) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-49.html#b.-중요한-설명변수",
    "href": "posts/13wk-49.html#b.-중요한-설명변수",
    "title": "13wk-49: 키와 몸무게 (결측치, 성별교호작용) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수?",
    "text": "B. 중요한 설명변수?\n\nauto.quick_fit(\n    train_data = df_train, \n    label='height',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231130_185031/\"\n\n\nModel Prediction for height\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-3.441217\n-3.881789\n0.007022\n0.002552\n0.748225\n0.007022\n0.002552\n0.748225\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\nweight\n9.433162\n0.468715\n7.290537e-07\n5\n10.398253\n8.468071\n\n\nsex\n1.710680\n0.464422\n5.924364e-04\n5\n2.666932\n0.754427\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\nweight\nsex\nheight\nheight_pred\nerror\n\n\n\n\n208\nNaN\nfemale\n159.027430\n168.600342\n9.572911\n\n\n263\n54.145913\nfemale\n165.791300\n173.811340\n8.020041\n\n\n146\n76.642564\nmale\n175.011295\n182.954391\n7.943097\n\n\n228\n56.473758\nfemale\n165.962051\n173.811340\n7.849289\n\n\n92\n51.018586\nfemale\n160.851952\n167.398270\n6.546318\n\n\n198\nNaN\nmale\n173.915293\n180.355164\n6.439870\n\n\n157\n46.214566\nfemale\n154.289882\n160.576065\n6.286183\n\n\n106\n69.667856\nmale\n179.665916\n173.611328\n6.054588\n\n\n118\n48.711791\nfemale\n168.305739\n162.763138\n5.542602\n\n\n166\n77.068343\nmale\n177.439194\n182.954391\n5.515197"
  },
  {
    "objectID": "posts/13wk-49.html#c.-관측치별-해석",
    "href": "posts/13wk-49.html#c.-관측치별-해석",
    "title": "13wk-49: 키와 몸무게 (결측치, 성별교호작용) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n\n0번 obs\n- 0번 observation\n\ndf_train.iloc[[0]]\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n0\n71.169041\nmale\n180.906857\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[0]])\n\n0    178.368637\nName: height, dtype: float32\n\n\n\n왜 178.368637로 예측했을까?\n\n- 해석\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[0]]\n)\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n0\n71.169041\nmale\n180.906857\n\n\n\n\n\n\n\n\n\n\n\n일단 평균값인 173.029로 적합\nsex를 고려하여 +1.57\nweight를 고려하여 +3.77\n최종적으로는 178.369\n\n\n\n208번 obs\n- 0번 observation\n\ndf_train.iloc[[208]]\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n208\nNaN\nfemale\n159.02743\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[208]])\n\n208    169.037048\nName: height, dtype: float32\n\n\n\n왜 169.037048로 예측했을까?\n\n- 해석\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[208]]\n)\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n208\nNaN\nfemale\n159.02743\n\n\n\n\n\n\n\n\n\n\n\n일단 평균인 173.029로 적합\nsex를 고려하여 -4.42\nweight를 고려하여 +0.43\n\n\n결측값을 그냥 하나의 관측치로 해석함 (nan이라는 값을 가지고 있다고 해석해버림!!)\n\n\n이게 왜 가능하지? (이런걸 가능하게 하는 테크닉 많아요, 쉽게 생각해서 nan을 -9999로 처리하고 tree를 돌린다고 생각해보세요..)\n\n\n\n211번 obs\n- 0번 observation\n\ndf_train.iloc[[211]]\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n211\nNaN\nfemale\n165.076235\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[211]])\n\n211    169.037048\nName: height, dtype: float32\n\n\n- 해석\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[211]]\n)\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n211\nNaN\nfemale\n165.076235\n\n\n\n\n\n\n\n\n\n\n\n일단 평균인 173.029로 적합\nsex를 고려하여 -4.42\nweight를 고려하여 +0.43\n208번과 적합값이 같음!! (이거 합리적임..)\n\n- 우리가 생각한 현실적인 적합은 사실 이러함\n\ndf_train[df_train.sex == 'female'].weight.mean()\n\n49.567060917121516\n\n\n\nonerow = df_train.iloc[[211]].copy()\nonerow.weight = 49.567060917121516\nonerow\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n211\n49.567061\nfemale\n165.076235\n\n\n\n\n\n\n\n\npredictr.predict(onerow)\n\n211    164.756104\nName: height, dtype: float32\n\n\n\n우리가 생각한 방법과 같지 않지만 거의 대등한 효과를 준다!\n놀라운점은 데이터에 대한 어떠한 전처리도 필요하지 않다는것!\n\n\n\n198번 obs\n- 0번 observation\n\ndf_train.iloc[[198]]\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n198\nNaN\nmale\n173.915293\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[198]])\n\n198    178.495316\nName: height, dtype: float32\n\n\n- 해석\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[198]]\n)\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n198\nNaN\nmale\n173.915293\n\n\n\n\n\n\n\n\n\n\n\n일단 평균인 173.029로 적합\nsex를 고려하여 +2.2\nweight를 고려하여 +3.26"
  },
  {
    "objectID": "posts/13wk-53.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-53.html#a.-y의-분포-xy의-관계-시각화",
    "title": "13wk-53: 취업(다중공선성) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train, \n    label='employment_score',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nemployment_score\n500\n7.227104\n3.115979\n-0.644716\n4.695513\n7.281178\n9.548811\n15.120906\nfloat64\n500\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for employment_score &gt;= 0.5\n\n\n\n\n\nFeature interaction between toeic/employment_score in train_data\n\n\n\n\n\nFeature interaction between toeic2/employment_score in train_data\n\n\n\n\n\nFeature interaction between toeic4/employment_score in train_data\n\n\n\n\n\nFeature interaction between toeic3/employment_score in train_data\n\n\n\n\n\nFeature interaction between toeic1/employment_score in train_data\n\n\n\n\n\nFeature interaction between toeic0/employment_score in train_data"
  },
  {
    "objectID": "posts/13wk-53.html#target-variable-analysis",
    "href": "posts/13wk-53.html#target-variable-analysis",
    "title": "13wk-53: 취업(다중공선성) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-53.html#b.-중요한-설명변수",
    "href": "posts/13wk-53.html#b.-중요한-설명변수",
    "title": "13wk-53: 취업(다중공선성) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data = df_train, \n    label='employment_score',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231130_133142/\"\n\n\nModel Prediction for employment_score\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-1.012202\n-0.979194\n0.001492\n0.001117\n0.301716\n0.001492\n0.001117\n0.301716\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\ngpa\n1.020679\n0.095581\n0.000009\n5\n1.217483\n0.823876\n\n\ntoeic0\n0.266908\n0.043618\n0.000083\n5\n0.356718\n0.177098\n\n\ntoeic2\n0.252605\n0.038863\n0.000065\n5\n0.332626\n0.172585\n\n\ntoeic\n0.241941\n0.043093\n0.000116\n5\n0.330669\n0.153212\n\n\ntoeic3\n0.161979\n0.035012\n0.000246\n5\n0.234070\n0.089888\n\n\ntoeic1\n0.158796\n0.040579\n0.000470\n5\n0.242350\n0.075242\n\n\ntoeic4\n0.136791\n0.037064\n0.000588\n5\n0.213106\n0.060476\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\nemployment_score\nemployment_score_pred\nerror\n\n\n\n\n55\n0.200267\n450\n450.310311\n464.340472\n458.213429\n456.215452\n448.932120\n2.234912\n4.809618\n2.574706\n\n\n8\n4.191552\n25\n29.000939\n22.725391\n19.529454\n35.896321\n24.151228\n2.514707\n5.069639\n2.554931\n\n\n491\n1.754276\n425\n428.686989\n439.377437\n446.630603\n439.109681\n423.056878\n8.008441\n5.771637\n2.236804\n\n\n144\n2.013480\n520\n519.674312\n521.390587\n531.847782\n511.375625\n525.305439\n8.755093\n6.543661\n2.211432\n\n\n118\n3.585276\n675\n679.425199\n680.429579\n677.878530\n674.812300\n672.177564\n12.785551\n10.599931\n2.185620\n\n\n469\n1.969145\n5\n3.785864\n4.575646\n-8.358037\n1.071854\n7.253616\n4.552622\n2.385054\n2.167567\n\n\n403\n1.678080\n70\n53.993037\n65.691879\n65.135837\n58.510651\n51.307683\n4.136691\n2.040128\n2.096563\n\n\n75\n0.564461\n795\n799.270794\n791.212118\n803.426181\n805.178974\n803.979166\n10.702375\n8.615296\n2.087079\n\n\n293\n4.364023\n990\n973.878219\n966.687506\n991.332887\n987.137768\n989.321286\n15.120906\n13.106976\n2.013931\n\n\n137\n4.248511\n115\n106.422018\n114.653052\n106.830406\n124.361062\n118.754414\n7.183716\n5.199379\n1.984337"
  },
  {
    "objectID": "posts/13wk-53.html#c.-관측치별-해석",
    "href": "posts/13wk-53.html#c.-관측치별-해석",
    "title": "13wk-53: 취업(다중공선성) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n\npredictr.predict(df_train.iloc[[1]])\n\n1    10.530768\nName: employment_score, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[1]]*1\n)\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\n\n\n\n\n1\n10.789671\n0.355496\n935.0\n940.563187\n935.72357\n939.190519\n938.995672\n945.376482"
  },
  {
    "objectID": "posts/07wk-34.html",
    "href": "posts/07wk-34.html",
    "title": "07wk-34: 취업(오버피팅) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n3. Data\n\ndef generating_df(n_balance):\n    df = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\n    df_balance = pd.DataFrame((np.random.randn(500,n_balance)).reshape(500,n_balance)*1,columns = ['balance'+str(i) for i in range(n_balance)])\n    return pd.concat([df,df_balance],axis=1)\n\n\ndf = generating_df(10)\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\n\n\n\n\n0\n135\n0.051535\n0\n0.184517\n0.415330\n-0.686366\n-1.819431\n-1.173594\n-1.215262\n-1.610389\n-0.820352\n0.449367\n0.725023\n\n\n1\n935\n0.355496\n0\n-0.632594\n-0.664892\n0.211959\n-0.958648\n1.242596\n-2.303714\n-1.450162\n-0.295893\n-0.212096\n0.526111\n\n\n2\n485\n2.228435\n0\n1.165994\n-0.756352\n-0.777127\n-0.643733\n1.051517\n1.899984\n2.451968\n0.294510\n-1.462545\n-0.397320\n\n\n3\n65\n1.179701\n0\n-0.373464\n-0.175916\n-0.096167\n1.362542\n1.285671\n2.568078\n-0.706288\n0.033109\n-0.022258\n-0.127342\n\n\n4\n445\n3.962356\n1\n0.118567\n1.506343\n0.265080\n-0.488586\n-0.354807\n-0.715808\n0.868123\n1.744717\n0.101916\n0.025840\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n0.310288\n1.199902\n0.278003\n0.079322\n1.518781\n0.108520\n1.877523\n0.571970\n0.765288\n0.472346\n\n\n496\n310\n2.601212\n1\n-0.869109\n-1.798781\n-0.801852\n-0.304045\n0.413868\n1.303797\n2.261232\n-1.036498\n-0.540375\n1.263321\n\n\n497\n225\n0.042323\n0\n-0.065514\n-0.133791\n2.126346\n-1.352515\n0.296947\n-0.893480\n0.569711\n0.139151\n-0.643992\n0.126148\n\n\n498\n320\n1.041416\n0\n-0.927475\n1.515043\n1.626494\n0.322667\n1.016824\n1.055700\n-1.795255\n0.497891\n-1.101028\n-1.164185\n\n\n499\n375\n3.626883\n1\n-0.464310\n-2.043014\n1.043372\n-0.080865\n0.633239\n2.330138\n1.390587\n0.872401\n-0.502196\n-0.171452\n\n\n\n\n500 rows × 13 columns\n\n\n\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size=0.7, random_state=42)\n\n\nX,y = df_train.drop(['employment'],axis=1), df_train['employment']\nXX,yy = df_test.drop(['employment'],axis=1), df_test['employment']\n\n\n\n4. 분석\n- 분석1: 의사결정나무\n\n## step1 -- pass\n## step2 \npredictr = sklearn.tree.DecisionTreeClassifier(random_state=42)\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 1.0000\ntest_score = 0.7571\n\n\n- 분석2: 로지스틱 + Ridge\n\n## step1 -- pass\n## step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(penalty='l2')\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 0.8800\ntest_score = 0.8800\n\n\n- 분석3: 로지스틱 + Lasso\n\n## step1 -- pass\n## step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(penalty='l1', solver='liblinear')\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 0.8667\ntest_score = 0.8857\n\n\n\n\n5. 연구\n- Balance 변수들의 수가 커짐에 따라서 각 방법들(의사결정나무, 로지스틱+Ridge, 로지스틱+Lasso)의 train/test score는 어떻게 변화할까?\n- df, predictor -&gt; train_score, test_score 와 같은 함수를 만들자.\n\ndef anal(df,predictr):\n    df_train, df_test = sklearn.model_selection.train_test_split(df, test_size=0.7, random_state=42)\n    X,y = df_train.drop(['employment'],axis=1), df_train['employment']\n    XX,yy = df_test.drop(['employment'],axis=1), df_test['employment']\n    ## step1 -- pass\n    ## step2 -- pass \n    ## step3 \n    predictr.fit(X,y)\n    ## step4 -- pass \n    #--#\n    return predictr.score(X,y),predictr.score(XX,yy)\n\n\npredictr = sklearn.tree.DecisionTreeClassifier()\n\n\nanal(df,predictr)\n\n(1.0, 0.7657142857142857)\n\n\n- 실험해보자.\n\nn_balance_lst = range(0,5000,50)\n\n\npredictrs = [sklearn.tree.DecisionTreeClassifier(random_state=42),\n             sklearn.linear_model.LogisticRegressionCV(penalty='l2'),\n             sklearn.linear_model.LogisticRegressionCV(penalty='l1', solver='liblinear')]\n\n\nlst = [[anal(generating_df(n_balance),predictr) for predictr in predictrs] for n_balance in n_balance_lst]\n\n- 실험결과 정리\n\narr = np.array(lst)\ntr = arr[:,:,0]\ntst = arr[:,:,1]\n\n\ndf1= pd.DataFrame(tr,columns=['tree','ridge','lasso']).eval('dataset = \"train\"').eval('n_balance = @n_balance_lst')\ndf2= pd.DataFrame(tst,columns=['tree','ridge','lasso']).eval('dataset = \"test\"').eval('n_balance = @n_balance_lst')\nresult_df = pd.concat([df1,df2]).set_index(['dataset','n_balance']).stack().reset_index().set_axis(['dataset','n_balance','method','score'],axis=1)\n\n\nsns.lineplot(result_df.query('dataset==\"test\"'),x='n_balance',y='score',hue='method')\n\n&lt;Axes: xlabel='n_balance', ylabel='score'&gt;"
  },
  {
    "objectID": "posts/02wk-07.html#a.-데이터",
    "href": "posts/02wk-07.html#a.-데이터",
    "title": "02wk-07: 타이타닉 / Autogluon (Fsize,Drop)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\n!kaggle competitions download -c titanic\n!unzip titanic.zip -d ./titanic\ndf_train = TabularDataset('titanic/train.csv')\ndf_test = TabularDataset('titanic/test.csv')\n!rm titanic.zip\n!rm -rf titanic/\n\nDownloading titanic.zip to /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\n  0%|                                               | 0.00/34.1k [00:00&lt;?, ?B/s]\n100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 21.5MB/s]\nArchive:  titanic.zip\n  inflating: ./titanic/gender_submission.csv  \n  inflating: ./titanic/test.csv      \n  inflating: ./titanic/train.csv     \n\n\n- 피처엔지니어링\n\n_df_train = df_train.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)\n_df_test = df_test.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)"
  },
  {
    "objectID": "posts/02wk-07.html#b.-predictor-생성",
    "href": "posts/02wk-07.html#b.-predictor-생성",
    "title": "02wk-07: 타이타닉 / Autogluon (Fsize,Drop)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231024_084434\""
  },
  {
    "objectID": "posts/02wk-07.html#c.-적합fit",
    "href": "posts/02wk-07.html#c.-적합fit",
    "title": "02wk-07: 타이타닉 / Autogluon (Fsize,Drop)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(_df_train) # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231024_084434\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   265.23 GB / 490.57 GB (54.1%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    123682.78 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    10 features in original data used to generate 27 features in processed data.\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2417.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.18s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8101   = Validation score   (accuracy)\n    0.45s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8268   = Validation score   (accuracy)\n    0.23s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.47s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: CatBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8268   = Validation score   (accuracy)\n    0.69s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.78s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.7989   = Validation score   (accuracy)\n    0.78s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\nNo improvement since epoch 9: early stopping\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n    0.8268   = Validation score   (accuracy)\n    1.92s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n    0.8212   = Validation score   (accuracy)\n    0.21s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.838    = Validation score   (accuracy)\n    5.24s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBMLarge ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8268   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.46s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 12.83s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231024_084434\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7ff0efb17a00&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.069945  7.157853                0.000595           0.459994            2       True         14\n1        NeuralNetTorch   0.837989       0.021271  5.238818                0.021271           5.238818            1       True         12\n2         LightGBMLarge   0.826816       0.003424  0.397473                0.003424           0.397473            1       True         13\n3              CatBoost   0.826816       0.004246  0.686450                0.004246           0.686450            1       True          7\n4              LightGBM   0.826816       0.004521  0.231312                0.004521           0.231312            1       True          4\n5       NeuralNetFastAI   0.826816       0.010309  1.919406                0.010309           1.919406            1       True         10\n6               XGBoost   0.821229       0.004762  0.214719                0.004762           0.214719            1       True         11\n7      RandomForestEntr   0.821229       0.052209  0.334242                0.052209           0.334242            1       True          6\n8      RandomForestGini   0.815642       0.099201  0.473848                0.099201           0.473848            1       True          5\n9            LightGBMXT   0.810056       0.003292  0.445553                0.003292           0.445553            1       True          3\n10       ExtraTreesGini   0.804469       0.100738  0.783195                0.100738           0.783195            1       True          8\n11       ExtraTreesEntr   0.798883       0.040266  0.782176                0.040266           0.782176            1       True          9\n12       KNeighborsDist   0.653631       0.002721  0.008844                0.002721           0.008844            1       True          2\n13       KNeighborsUnif   0.653631       0.028206  0.010227                0.028206           0.010227            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.069945\n7.157853\n0.000595\n0.459994\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.021271\n5.238818\n0.021271\n5.238818\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.826816\n0.003424\n0.397473\n0.003424\n0.397473\n1\nTrue\n13\n\n\n3\nCatBoost\n0.826816\n0.004246\n0.686450\n0.004246\n0.686450\n1\nTrue\n7\n\n\n4\nLightGBM\n0.826816\n0.004521\n0.231312\n0.004521\n0.231312\n1\nTrue\n4\n\n\n5\nNeuralNetFastAI\n0.826816\n0.010309\n1.919406\n0.010309\n1.919406\n1\nTrue\n10\n\n\n6\nXGBoost\n0.821229\n0.004762\n0.214719\n0.004762\n0.214719\n1\nTrue\n11\n\n\n7\nRandomForestEntr\n0.821229\n0.052209\n0.334242\n0.052209\n0.334242\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.815642\n0.099201\n0.473848\n0.099201\n0.473848\n1\nTrue\n5\n\n\n9\nLightGBMXT\n0.810056\n0.003292\n0.445553\n0.003292\n0.445553\n1\nTrue\n3\n\n\n10\nExtraTreesGini\n0.804469\n0.100738\n0.783195\n0.100738\n0.783195\n1\nTrue\n8\n\n\n11\nExtraTreesEntr\n0.798883\n0.040266\n0.782176\n0.040266\n0.782176\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.002721\n0.008844\n0.002721\n0.008844\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.028206\n0.010227\n0.028206\n0.010227\n1\nTrue\n1\n\n\n\n\n\n\n\n- validation set의 의미:"
  },
  {
    "objectID": "posts/02wk-07.html#d.-예측-predict",
    "href": "posts/02wk-07.html#d.-예측-predict",
    "title": "02wk-07: 타이타닉 / Autogluon (Fsize,Drop)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(df_train.Survived == predictr.predict(_df_train)).mean()\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n\n\n0.9438832772166106\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\n# df_test.assign(Survived = predictr.predict(_df_test)).loc[:,['PassengerId','Survived']]\\\n# .to_csv(\"autogluon(Fsize,Drop)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/12wk-44.html#a.-baggin으로-적합",
    "href": "posts/12wk-44.html#a.-baggin으로-적합",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "A. Baggin으로 적합",
    "text": "A. Baggin으로 적합\n\n# step1\nX = pd.get_dummies(df_train.loc[:,'age':'region'],drop_first=True)\ny = df_train['charges']\n# step2 \npredictr = sklearn.ensemble.BaggingRegressor()\n# step3\npredictr.fit(X,y)\n# step4 -- pass \n\nBaggingRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BaggingRegressorBaggingRegressor()"
  },
  {
    "objectID": "posts/12wk-44.html#b.-결과-시각화",
    "href": "posts/12wk-44.html#b.-결과-시각화",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "B. 결과 시각화",
    "text": "B. 결과 시각화\n- 관찰: 트리들의 다양하지 않다.\n\nsklearn.tree.plot_tree(\n    predictr.estimators_[4],\n    feature_names=X.columns,\n    max_depth=1\n);\nfig = plt.gcf()\nfig.set_dpi(200)"
  },
  {
    "objectID": "posts/12wk-44.html#c.-우수성-vs-다양성",
    "href": "posts/12wk-44.html#c.-우수성-vs-다양성",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "C. 우수성 vs 다양성",
    "text": "C. 우수성 vs 다양성\n- 다양성이 없는게 왜 문제인가?\n\n기존의 의사결정나무: 데이터에 최적화된 똑똑한 하나의 트리를 발견\nBagging: “데이터에 최적화”를 희생하고 “다양성”을 확보. 즉 똑똑한 하나의 트리대신에 모자란 여러개의 트리를 생성하고 힘을 합침.\n\n- 우수성 vs 다양성 – 우수하지만 비슷한 생각을 가진 10명의 인재 vs 평범하지만 다양한 의견을 가진 10명의 인재\n\n뭐가 좋을까요?\n김용대 교수님의 통찰! https://biz.heraldcorp.com/view.php?ud=20170802000434\n\n\n\n\n\n\n\nNote\n\n\n\n기계학습 방법론 중 앙상블이라는 방법이 있다. 앙상블이란 음악에서 여러 명의 연주자에 의한 합주 또는 합창을 의미하는데, 다양한 의견들을 조화롭게 결합하는 방법을 의미하기도 한다. 기계학습에서 앙상블이란 같은 데이터를 여러 개의 기계학습 알고리즘들이 분석하여 각자 지식을 습득한 후 이를 결합하여 새롭고 유용한 지식을 창출하는 방법이다.\n앙상블 방법론에 숨어 있는 매우 흥미롭고 이해하기 어려운 과학적 현상으로는, 앙상블의 성능을 높이기 위해서는 개별 알고리즘들의 성능보다는 알고리즘들의 다양성이 훨씬 중요하다는 것이다. 즉, 주어진 문제에 대해서 모두 비슷한 답을 주는 성능이 우수한 10개의 알고리즘보다는 성능은 좀 떨어지지만 다양한 답을 제공하는 10개의 알고리즘이 앙상블에는 더 효율적이라는 것이다. 이를 인간 사회에 적용하면, 비슷한 생각을 가진 우수한 10명의 인재보다는 다양한 의견을 가진 평범한 10명의 의견이 훨씬 유용할 수 있다는 것이다.\n\n\n- 요약\n\n통찰: Bagging은 의사결정나무보다 다양성을 추구하는 알고리즘이다.\n문제점: 하지만 \\({\\bf X}\\)가 고차원인 상황에서 배깅만으로는 그렇게 다양한 트리가 나오지 않는다. (모든 트리가 천편일륜적으로 흡연여부가 보험료에 미치는 영향을 우선적으로 연구한다)\n소망: 혹시 어떤 괴짜는 흡연여부를 연구하지 않고 다른 변수들을 최우선으로 연구하는 연구자가 있다면 좋겠는데.. (그러면 트리가 다양해질텐데)"
  },
  {
    "objectID": "posts/12wk-44.html#a.-개념",
    "href": "posts/12wk-44.html#a.-개념",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "A. 개념",
    "text": "A. 개념\n- RandomForeset = Bagging + max_feature=??\n\n엄밀한 수식은 아닙니다.\n\n- 슈도-알고리즘의 비교\n## Bagging \ntrees = []\nfor i in range(100):\n    tree = sklearn.tree.DecisionTreeRegressor() \n    X_sample,y_sample = boostrap(X,y) # 매순간 샘플바뀜\n    tree.fit(X_sample,y_sample) # 일부 샘플만으로 적합\n    trees.append(tree) # 학습한 나무를 저장\nyhat = ensemble(trees) # 여러개의 나무를 종합하여 하나의 예측값을 만들어냄 (합주)\n\n## RandomForeset \nforest = [] \nfor i in range(100):\n    tree = sklearn.tree.DecisionTreeRegressor(max_feature=1) # 매순간 설명변수 바뀜\n    X_sample,y_sample = boostrap(X,y) # 매순간 샘플바뀜    \n    tree.fit(X_sample,y_sample) # 일부설명변수, 일부샘플만으로 적합 \n    forest.append(tree) # 학습한 나무를 숲에 저장 \nyhat = ensemble(forest) # 여러개의 나무를 종합하여 하나의 예측값을 만들어냄 (합주)"
  },
  {
    "objectID": "posts/12wk-44.html#b.-일단-적합",
    "href": "posts/12wk-44.html#b.-일단-적합",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "B. 일단 적합",
    "text": "B. 일단 적합\n\n# step1 -- pass \n# step2 \npredictr = sklearn.ensemble.RandomForestRegressor(\n    max_depth=1,\n    max_features=1/3\n)\n# step3\npredictr.fit(X,y)\n# step4 \n\nRandomForestRegressor(max_depth=1, max_features=0.3333333333333333)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=1, max_features=0.3333333333333333)\n\n\n\n주의1: max_feature=1은 1개의 feature를 고려한다는 의미이고, max_feature=1.0은 100%의 feature를 고려한다는 의미이다.\n\n\n주의2: max_feature=1.0 이 default값이며 이 값을 사용한다면 “RandomForest = Bagging” 이다. (아래 ref 참고)\n\n\nref: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
  },
  {
    "objectID": "posts/12wk-44.html#c.-시각화",
    "href": "posts/12wk-44.html#c.-시각화",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "C. 시각화",
    "text": "C. 시각화\n\nsklearn.tree.plot_tree(\n    predictr.estimators_[5],\n    feature_names=X.columns,\n    max_depth=1\n);"
  },
  {
    "objectID": "posts/12wk-44.html#a.-random_state-추출",
    "href": "posts/12wk-44.html#a.-random_state-추출",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "A. random_state 추출",
    "text": "A. random_state 추출\n- 첫번째 트리 - random_state 확인\n\npredictr.estimators_[0]\n\nDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=528585471)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=528585471)\n\n\n- 첫번째 트리의 random_state가 저장된 곳\n\npredictr.estimators_[0].random_state\n\n528585471\n\n\n- 각 나무들의 random_state 추출\n\nrs = [tree.random_state for tree in predictr.estimators_]"
  },
  {
    "objectID": "posts/12wk-44.html#b.-forest-생성",
    "href": "posts/12wk-44.html#b.-forest-생성",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "B. forest 생성",
    "text": "B. forest 생성\n\nmy_forest = [sklearn.tree.DecisionTreeRegressor(max_depth=1,max_features=1/3,random_state=r) for r in rs]\n\n\nmy_forest[-1]\n\nDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=838055637)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=838055637)\n\n\n\npredictr.estimators_[-1]\n\nDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=838055637)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=838055637)"
  },
  {
    "objectID": "posts/12wk-44.html#c.-부스트랩-샘플생성",
    "href": "posts/12wk-44.html#c.-부스트랩-샘플생성",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "C. 부스트랩 샘플생성",
    "text": "C. 부스트랩 샘플생성\n- 저장된 부스트랩 샘플을 확보하자. – 실패\n\npredictr.estimators_samples_\n\nAttributeError: 'RandomForestRegressor' object has no attribute 'estimators_samples_'\n\n\n\n샘플들을 재현하기 귀찮게 되어있다\n\n- 그냥 새로 만들어보자!\n\nsklearn.ensemble._forest._generate_sample_indices?\n\n\nSignature:\nsklearn.ensemble._forest._generate_sample_indices(\n    random_state,\n    n_samples,\n    n_samples_bootstrap,\n)\nDocstring: Private function used to _parallel_build_trees function.\nFile:      ~/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\nType:      function\n\n\n\n\nsample = sklearn.ensemble._forest._generate_sample_indices\n\n\nmy_index = [sample(random_state=r,n_samples=1338,n_samples_bootstrap=1338) for r in rs]"
  },
  {
    "objectID": "posts/12wk-44.html#d.-적합",
    "href": "posts/12wk-44.html#d.-적합",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "D. 적합",
    "text": "D. 적합\n\nfor idx,tree in zip(my_index,my_forest):\n    X_sampled, y_sampled = np.array(X)[idx], np.array(y)[idx]\n    tree.fit(X_sampled,y_sampled)"
  },
  {
    "objectID": "posts/12wk-44.html#e.-앙상블",
    "href": "posts/12wk-44.html#e.-앙상블",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "E. 앙상블",
    "text": "E. 앙상블\n\ndef ensemble(forest):\n    return np.stack([tree.predict(X) for tree in forest]).mean(axis=0)\n\n\nensemble(my_forest)\n\narray([14775.27695067, 12505.53186199, 12824.39472346, ...,\n       12472.61376294, 10949.14260716, 16264.83980108])\n\n\n\npredictr.predict(X)\n\narray([14775.27695067, 12505.53186199, 12824.39472346, ...,\n       12472.61376294, 10949.14260716, 16264.83980108])"
  },
  {
    "objectID": "posts/12wk-44.html#f.-주의",
    "href": "posts/12wk-44.html#f.-주의",
    "title": "12wk-44: Medical Cost / 랜덤포레스트",
    "section": "F. 주의",
    "text": "F. 주의\n- max_depth가 깊을 경우 ensemble(my_forest)와 predictr.predict(X)의 결과가 일치하지 않을 수 있다. 이유는 트리의 성장을 멈추는 조건에서 각 leaf의 최소 샘플숫자가 기여하는데, 샘플의 가중치를 고려하느냐 하지 않느냐에 따라서 샘플숫자의 차이가 있기 때문\n- 시각화 비교 (samples가 서로다름을 파악!!)\n\nsklearn.tree.plot_tree(my_forest[0])\n\n[Text(0.5, 0.75, 'x[1] &lt;= 34.102\\nsquared_error = 151919650.728\\nsamples = 1338\\nvalue = 13411.861'),\n Text(0.25, 0.25, 'squared_error = 103631519.07\\nsamples = 992\\nvalue = 11844.483'),\n Text(0.75, 0.25, 'squared_error = 263127010.326\\nsamples = 346\\nvalue = 17905.614')]\n\n\n\n\n\n\nsklearn.tree.plot_tree(predictr.estimators_[0])\n\n[Text(0.5, 0.75, 'x[1] &lt;= 34.102\\nsquared_error = 151919650.728\\nsamples = 834\\nvalue = 13411.861'),\n Text(0.25, 0.25, 'squared_error = 103631519.07\\nsamples = 621\\nvalue = 11844.483'),\n Text(0.75, 0.25, 'squared_error = 263127010.326\\nsamples = 213\\nvalue = 17905.614')]"
  },
  {
    "objectID": "posts/06wk-25.html#a.-정확한-설명",
    "href": "posts/06wk-25.html#a.-정확한-설명",
    "title": "06wk-25: 취업(다중공선성) / Lasso",
    "section": "A. 정확한 설명",
    "text": "A. 정확한 설명\n- 어려워요.."
  },
  {
    "objectID": "posts/06wk-25.html#b.-직관적-설명-엄밀하지-않은-설명",
    "href": "posts/06wk-25.html#b.-직관적-설명-엄밀하지-않은-설명",
    "title": "06wk-25: 취업(다중공선성) / Lasso",
    "section": "B. 직관적 설명 (엄밀하지 않은 설명)",
    "text": "B. 직관적 설명 (엄밀하지 않은 설명)\n- 느낌: 몇 개의 toeic coef들로 쉽게 0.01을 만들게 해서는 안된다.\n\n아이디어1: 0.01을 동일한 값으로 균등하게 배분한다. – Ridge, L2-penalty\n아이디어2: 아주 적은숫자의 coef만을 살려두고 나머지 coef값은 0으로 강제한다. – Lasso, L1-penalty\n\n- 계수값이 0이라는 의미: 그 변수를 제거한것과 같은 효과\n- 아이디어2의 기원: y ~ toeic + gpa 가 트루이지만, y ~ toeic0 + gpa 으로 적합해도 괜찮잖아?\n- 진짜 학습된 계수값이 대부분 0인지 확인해보자.\n\nplt.plot(predictr.coef_[1:])"
  },
  {
    "objectID": "posts/06wk-25.html#c.-alpha-에-따른-변화-관찰",
    "href": "posts/06wk-25.html#c.-alpha-에-따른-변화-관찰",
    "title": "06wk-25: 취업(다중공선성) / Lasso",
    "section": "C. \\(\\alpha\\) 에 따른 변화 관찰",
    "text": "C. \\(\\alpha\\) 에 따른 변화 관찰\n- 여러개의 predictor 학습\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train[['employment_score']]\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test[['employment_score']]\n## step2 \nalphas = np.linspace(0,2,100)\npredictrs = [sklearn.linear_model.Lasso(alpha=alpha) for alpha in alphas]\n## step3\nfor predictr in predictrs: \n    predictr.fit(X,y)\n## step4 : pass \n\n\nplt.plot(predictrs[0].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[0].alpha))\nplt.plot(predictrs[50].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[50].alpha))\nplt.plot(predictrs[-1].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[-1].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f38058e1390&gt;\n\n\n\n\n\n- predictor 들의 toeic 계수합은 여전히 0.01 근처\n\nprint(f'alpha={predictrs[0].alpha:.4f}\\tsum(toeic_coef)={predictrs[0].coef_[1:].sum()}')\nprint(f'alpha={predictrs[50].alpha:.4f}\\tsum(toeic_coef)={predictrs[50].coef_[1:].sum()}')\nprint(f'alpha={predictrs[-1].alpha:.4f}\\tsum(toeic_coef)={predictrs[-1].coef_[1:].sum()}')\n\nalpha=0.0000    sum(toeic_coef)=0.010291301406468518\nalpha=1.0101    sum(toeic_coef)=0.009986115762478664\nalpha=2.0000    sum(toeic_coef)=0.009864586871194559\n\n\n- number of non-zero coefs 를 시각화\n\nnon_zero_coefs = [(abs(predictr.coef_[1:])&gt;0).sum() for predictr in predictrs]\n\n\nplt.plot(alphas,non_zero_coefs)"
  },
  {
    "objectID": "posts/06wk-25.html#d.-coef를-0으로-만드는-수학적-장치",
    "href": "posts/06wk-25.html#d.-coef를-0으로-만드는-수학적-장치",
    "title": "06wk-25: 취업(다중공선성) / Lasso",
    "section": "D. coef를 0으로 만드는 수학적 장치",
    "text": "D. coef를 0으로 만드는 수학적 장치\n- Ridge(복습): coef의 값들을 엔빵하는 수학적 장치\n\n패널티: 유사토익들의 계수값을 제곱한뒤 합치고(=L2-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n- Lasso: coef의 값들을 대부분 0으로 만드는 수학적 장치\n\n패널티: 유사토익들의 계수값의 절대값을 구한뒤에 합치고(=L1-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n- 사실 L1, L2 패널티에 따라서 이러한 결과가 나오는 것은 이해하기 어렵다. (그래서 취업/대학원 진학시 단골질문중 하나)"
  },
  {
    "objectID": "posts/07wk-33.html",
    "href": "posts/07wk-33.html",
    "title": "07wk-33: 취업(다중공선성) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n3. Data\n\nnp.random.seed(43052)\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n\nX,y = df_train.loc[:,'gpa':],df_train['employment_score']\nXX,yy = df_test.loc[:,'gpa':],df_test['employment_score']\n\n\n실제 kaggle에서는 yy를 모르는 상황임\n\n\n\n4. 분석\n- 분석1: 의사결정나무\n\n## step1 -- pass \n## step2 \npredictr = sklearn.tree.DecisionTreeRegressor(random_state=42)\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment_score_hat'] = predictr.predict(X)\ndf_test['employment_score_hat'] = predictr.predict(XX)\n#---#\nprint(f'train score: {predictr.score(X,y):.4f}')\nprint(f'test score: {predictr.score(XX,yy):.4f}')\n\ntrain score: 1.0000\ntest score: 0.8300\n\n\n\n오버핏이긴한데 나쁘지 않음..\n\n- 분석2: Lasso\n\n## step1 -- pass \n## step2 \npredictr = sklearn.linear_model.LassoCV()\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment_score_hat'] = predictr.predict(X)\ndf_test['employment_score_hat'] = predictr.predict(XX)\n#---#\nprint(f'train score: {predictr.score(X,y):.4f}')\nprint(f'test score: {predictr.score(XX,yy):.4f}')\n\ntrain score: 0.8994\ntest score: 0.8587\n\n\n- 총평: Lasso가 좋긴해요. 그런데 의사결정나무도 나쁘지 않아요.\n\n참고로 Lasso는 엄청 발전된 모델\n의사결정나무는 아주 초기모델임"
  },
  {
    "objectID": "posts/10wk-37.html#a.-max_depth1",
    "href": "posts/10wk-37.html#a.-max_depth1",
    "title": "10wk-37: 아이스크림 – 의사결정나무, max_depth",
    "section": "A. max_depth=1",
    "text": "A. max_depth=1\n- step1~4\n\n## step1\nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1) \n## step3 \npredictr.fit(X,y) \n## step4 -- pass \n# predictr.predict(X) \n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n- 결과 시각화\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,predictr.predict(X),'--.',label='Predicted')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7faade634610&gt;\n\n\n\n\n\n- tree 시각화\n\nsklearn.tree.plot_tree(predictr)\n\n[Text(0.5, 0.75, 'x[0] &lt;= 5.05\\nsquared_error = 111.946\\nsamples = 100\\nvalue = 33.973'),\n Text(0.25, 0.25, 'squared_error = 34.94\\nsamples = 45\\nvalue = 24.788'),\n Text(0.75, 0.25, 'squared_error = 49.428\\nsamples = 55\\nvalue = 41.489')]"
  },
  {
    "objectID": "posts/10wk-37.html#b.-max_depth2",
    "href": "posts/10wk-37.html#b.-max_depth2",
    "title": "10wk-37: 아이스크림 – 의사결정나무, max_depth",
    "section": "B. max_depth=2",
    "text": "B. max_depth=2\n- step1~4\n\n## step1\nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=2) \n## step3 \npredictr.fit(X,y) \n## step4 -- pass \n# predictr.predict(X) \n\nDecisionTreeRegressor(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=2)\n\n\n- 결과 시각화\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,predictr.predict(X),'.--',label='Predicted')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7faade4b0bd0&gt;\n\n\n\n\n\n- tree 시각화\n\nsklearn.tree.plot_tree(predictr)\n\n[Text(0.5, 0.8333333333333334, 'x[0] &lt;= 5.05\\nsquared_error = 111.946\\nsamples = 100\\nvalue = 33.973'),\n Text(0.25, 0.5, 'x[0] &lt;= 1.75\\nsquared_error = 34.94\\nsamples = 45\\nvalue = 24.788'),\n Text(0.125, 0.16666666666666666, 'squared_error = 15.12\\nsamples = 19\\nvalue = 19.105'),\n Text(0.375, 0.16666666666666666, 'squared_error = 8.587\\nsamples = 26\\nvalue = 28.94'),\n Text(0.75, 0.5, 'x[0] &lt;= 10.7\\nsquared_error = 49.428\\nsamples = 55\\nvalue = 41.489'),\n Text(0.625, 0.16666666666666666, 'squared_error = 19.819\\nsamples = 47\\nvalue = 39.251'),\n Text(0.875, 0.16666666666666666, 'squared_error = 21.051\\nsamples = 8\\nvalue = 54.638')]"
  },
  {
    "objectID": "posts/10wk-37.html#c.-애니메이션",
    "href": "posts/10wk-37.html#c.-애니메이션",
    "title": "10wk-37: 아이스크림 – 의사결정나무, max_depth",
    "section": "C. 애니메이션",
    "text": "C. 애니메이션\n- step1~4\n\n## step1 \nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictrs = [sklearn.tree.DecisionTreeRegressor(max_depth=k) for k in range(1,11)]\n## step3 \nfor k in range(10):\n    predictrs[k].fit(X,y)\n## step4 -- pass\n\n- 애니메이션\n\nfig = plt.figure()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca()\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.5) \n    ax.plot(X,predictrs[frame].predict(X),'.--') \n    ax.set_title(f'max_depth={predictrs[frame].max_depth}')\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames=10\n)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/04wk-15.html#a.-숫자형자료의-impute",
    "href": "posts/04wk-15.html#a.-숫자형자료의-impute",
    "title": "04wk-15: sklearn.impute를 이용한 결측치 처리",
    "section": "A. 숫자형자료의 impute",
    "text": "A. 숫자형자료의 impute\n- 주어진자료\n\ndf = pd.DataFrame({'A':[2.1,1.9,2.2,np.nan,1.9], 'B':[0,0,np.nan,0,0]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n- 빈칸은 대충 아래와 같이 추정하면 되지 않을까?\n\ndf.loc[3,'A'] = df.A.mean()\ndf.loc[2,'B'] = df.B.mean()\n\n\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.100\n0.0\n\n\n1\n1.900\n0.0\n\n\n2\n2.200\n0.0\n\n\n3\n2.025\n0.0\n\n\n4\n1.900\n0.0\n\n\n\n\n\n\n\n- 자동으로 하려면?\n\ndf = pd.DataFrame({'A':[2.1,1.9,2.2,np.nan,1.9], 'B':[0,0,np.nan,0,0]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n(방법1)\n\nimputer = sklearn.impute.SimpleImputer()\nimputer.fit(df)\nimputer.transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법2)\n\nimputer = sklearn.impute.SimpleImputer()\nimputer.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n- 다른방식으로 결측값 대체\n(방법1) – 평균으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='mean')\nimputer.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법2) – 중앙값으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='median')\nimputer.fit_transform(df)\n\narray([[2.1, 0. ],\n       [1.9, 0. ],\n       [2.2, 0. ],\n       [2. , 0. ],\n       [1.9, 0. ]])\n\n\n(방법3) – 최빈값으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimputer.fit_transform(df)\n\narray([[2.1, 0. ],\n       [1.9, 0. ],\n       [2.2, 0. ],\n       [1.9, 0. ],\n       [1.9, 0. ]])\n\n\n(방법4) – 상수대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='constant',fill_value=-999)\nimputer.fit_transform(df)\n\narray([[   2.1,    0. ],\n       [   1.9,    0. ],\n       [   2.2, -999. ],\n       [-999. ,    0. ],\n       [   1.9,    0. ]])"
  },
  {
    "objectID": "posts/04wk-15.html#b.-범주형자료의-impute",
    "href": "posts/04wk-15.html#b.-범주형자료의-impute",
    "title": "04wk-15: sklearn.impute를 이용한 결측치 처리",
    "section": "B. 범주형자료의 impute",
    "text": "B. 범주형자료의 impute\n- 자료\n\ndf = pd.DataFrame({'A':['Y','N','Y','Y',np.nan], 'B':['stat','math',np.nan,'stat','bio']})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n- 최빈값 혹은 상수대체만 가능\n(방법1) – 최빈값을 이용\n\nimptr = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimptr.fit_transform(df)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)\n\n\n(방법2) – 상수로 대체함\n\nimptr1 = sklearn.impute.SimpleImputer(strategy='constant',fill_value='Y')\nimptr1.fit_transform(df[['A']])\nimptr2 = sklearn.impute.SimpleImputer(strategy='constant',fill_value='math')\nimptr2.fit_transform(df[['B']])\n\narray([['stat'],\n       ['math'],\n       ['math'],\n       ['stat'],\n       ['bio']], dtype=object)\n\n\n\nnp.concatenate([imptr1.fit_transform(df[['A']]),imptr2.fit_transform(df[['B']])],axis=1)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)"
  },
  {
    "objectID": "posts/04wk-15.html#c.-혼합형자료의-impute-1-모두-최빈값으로-impute",
    "href": "posts/04wk-15.html#c.-혼합형자료의-impute-1-모두-최빈값으로-impute",
    "title": "04wk-15: sklearn.impute를 이용한 결측치 처리",
    "section": "C. 혼합형자료의 impute – (1) 모두 최빈값으로 impute",
    "text": "C. 혼합형자료의 impute – (1) 모두 최빈값으로 impute\n# 예제: 아래의 df에서 결측치를 모두 최빈값으로 impute하라.\n\ndf = pd.DataFrame(\n    {'A':[2.1,1.9,2.2,np.nan,1.9],\n     'B':[0,0,np.nan,0,0],\n     'C':['Y','N','Y','Y',np.nan], \n     'D':['stat','math',np.nan,'stat','bio']}\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n(풀이)\n\nimptr = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimptr.fit_transform(df)\n\narray([[2.1, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'N', 'math'],\n       [2.2, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'Y', 'bio']], dtype=object)\n\n\n#"
  },
  {
    "objectID": "posts/04wk-15.html#d.-혼합형자료의-impute-2-숫자형은-평균값으로-범주는-최빈값으로-impute",
    "href": "posts/04wk-15.html#d.-혼합형자료의-impute-2-숫자형은-평균값으로-범주는-최빈값으로-impute",
    "title": "04wk-15: sklearn.impute를 이용한 결측치 처리",
    "section": "D. 혼합형자료의 impute – (2) 숫자형은 평균값으로, 범주는 최빈값으로 impute",
    "text": "D. 혼합형자료의 impute – (2) 숫자형은 평균값으로, 범주는 최빈값으로 impute\n# 예제: 아래의 df를 숫자형일 경우는 평균대치, 문자형일 경우는 최빈값으로 대치하라.\n\ndf = pd.DataFrame(\n    {'A':[2.1,1.9,2.2,np.nan,1.9],\n     'B':[0,0,np.nan,0,0],\n     'C':['Y','N','Y','Y',np.nan], \n     'D':['stat','math',np.nan,'stat','bio']}\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n(풀이)\n- step1: 복사본 생성\n\ndf_imputed = df.copy()\ndf_imputed\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n- step2: 데이터프레임 분리\n\ndf_num = df.select_dtypes(include=\"number\")\ndf_num\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n\ndf_cat = df.select_dtypes(exclude=\"number\")\ndf_cat \n\n\n\n\n\n\n\n\nC\nD\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n- step3: impute\n\ndf_imputed[df_num.columns] = sklearn.impute.SimpleImputer(strategy='mean').fit_transform(df_num)\ndf_imputed[df_cat.columns] = sklearn.impute.SimpleImputer(strategy='most_frequent').fit_transform(df_cat)\n\n\ndf_imputed\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.100\n0.0\nY\nstat\n\n\n1\n1.900\n0.0\nN\nmath\n\n\n2\n2.200\n0.0\nY\nstat\n\n\n3\n2.025\n0.0\nY\nstat\n\n\n4\n1.900\n0.0\nY\nbio"
  },
  {
    "objectID": "posts/04wk-14.html#a.-df.info",
    "href": "posts/04wk-14.html#a.-df.info",
    "title": "04wk-14: msno를 이용한 결측치 시각화",
    "section": "A. df.info()",
    "text": "A. df.info()\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   A       668 non-null    float64\n 1   B       656 non-null    float64\n 2   C       608 non-null    float64\n 3   D       668 non-null    float64\n 4   E       660 non-null    float64\ndtypes: float64(5)\nmemory usage: 39.2 KB"
  },
  {
    "objectID": "posts/04wk-14.html#b.-msno.bar",
    "href": "posts/04wk-14.html#b.-msno.bar",
    "title": "04wk-14: msno를 이용한 결측치 시각화",
    "section": "B. msno.bar()",
    "text": "B. msno.bar()\n\nmsno.bar(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-14.html#a.-msno.matrix",
    "href": "posts/04wk-14.html#a.-msno.matrix",
    "title": "04wk-14: msno를 이용한 결측치 시각화",
    "section": "A. msno.matrix()",
    "text": "A. msno.matrix()\n\nmsno.matrix(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-14.html#b.-msno.heatmap",
    "href": "posts/04wk-14.html#b.-msno.heatmap",
    "title": "04wk-14: msno를 이용한 결측치 시각화",
    "section": "B. msno.heatmap()",
    "text": "B. msno.heatmap()\n\nmsno.heatmap(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-14.html#c.-msno.dendrogram",
    "href": "posts/04wk-14.html#c.-msno.dendrogram",
    "title": "04wk-14: msno를 이용한 결측치 시각화",
    "section": "C. msno.dendrogram()",
    "text": "C. msno.dendrogram()\n\nmsno.dendrogram(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/01wk-01.html",
    "href": "posts/01wk-01.html",
    "title": "01wk-01: 캐글소개",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. 타이타닉 경진대회 찾기\n- 방법1: 검색을 통하여 접속하기\n\n\n\n그림1: 타이타닉 경진대회 검색\n\n\n- 방법2: 직접주소를 입력 https://www.kaggle.com/competitions/titanic\n\n\n3. 데이터구경\n- 데이터의 설명을 빠르게 파악하는 방법\n\n방법1: 변수위주로 Kaggle 홈페이지에서 파악\n방법2: 구글번역기 활용\n방법3: \bChatGPT이용\n\n- 제 생각\n\nChatGPT가 옳지 않은 소리를 할 떄도 있음. 하지만 처음에 데이터에 대한 개념을 빠르게 정리하고자 할 때 도움이 된다.\n변수이름이 약어로 된 경우가 많은데 이럴경우 GPT가 유용하다.\n\n\n\n4. 메뉴소개\n- 캐글 웹사이트의 메뉴탭 소개\n\nOverview (개요):\n\n\n경진대회 주최자가 경진대회의 배경, 목표, 데이터셋 설명 등을 설명\n\n\nData (데이터):\n\n\n경진대회에 사용되는 데이터셋에 관한 정보를 찾을 수 있음.\n데이터의 구성, 변수 설명, 예시 데이터 등이 제공되며, 데이터를 이해하고 분석할 수 있는데 필요한 정보들이 여기에 포함됨.\n\n\nCode (코드):\n\n\n경진대회 참가자들이 코드를 공유하고 토론하는 공간.\n주로 주어진 문제에 대한 데이터 분석 및 모델링 코드, 데이터 전처리 방법, 모델 학습 등에 관련된 내용이 이곳에 포함됨.\n\n\nDiscussion (토론):\n\n\n참가자들이 서로 의견을 교환하고 질문을 주고받을 수 있는 공간.\n데이터 분석 방법, 모델 구축 전략, 문제 해결 과정 등에 대한 토론이 이루어짐.\n\n\nLeaderboard (리더보드):\n\n\n경진대회 참가자들의 성능 평가 지표와 순위가 나열.\n참가자들의 모델 성능을 비교하고 경쟁 상황을 실시간으로 확인할 수 있음.\n\n\nRules (규칙):\n\n\n참가자들이 따라야 할 규칙, 데이터 사용 방법, 평가 지표 등이 명시되어 있음.\n\n- 체크하면 좋은 것들\n\nOveview: 평가지표(metric), 상(prize), 시간(timeline)\nRules: 평가지표(metric), 외부데이터 사용여부, 하루 최대 제출 수, 최종선택가능한 솔루션 수 (limit)\n\n- 대회의 유형 (ref: https://github.com/PacktPublishing/The-Kaggle-Book)\n\nGetting Started: 상을 제공하지 않음. 튜토리얼용. 매우 협력적인 환경.\nFeatured: 가장 일반적인 유형. 스폰서 회사의 비지니스 관련문제. 상금이 후함. 솔루션을 소개하는 자세한 리포트를 준비해야하고 발표할 것을 요구받을 수 있음.\nAnalytics: 질적 평가. 참가자의 PPT를 제출로 받음."
  },
  {
    "objectID": "posts/07wk-32.html",
    "href": "posts/07wk-32.html",
    "title": "07wk-32: 아이스크림(교호작용) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',label='choco')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',label='vanilla')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f81d29533d0&gt;\n\n\n\n\n\n\n\n5. 분석\n- 분석1: 선형회귀\n\n# step1\nX = pd.get_dummies(df_train[['temp','type']],drop_first=True)\ny = df_train['sales']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n#---#\nf'train score = {predictr.score(X,y):.4f}'\n\n'train score = 0.9250'\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',alpha=0.2,label='choco')\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales_hat[df_train.type=='choco'],'--',color='C0')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',alpha=0.2,label='vanilla')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales_hat[df_train.type=='vanilla'],'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f81d2cee8c0&gt;\n\n\n\n\n\n- 분석2\n\n# step1\nX = pd.get_dummies(df_train[['temp','type']],drop_first=True)\ny = df_train['sales']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n#---#\nf'train score = {predictr.score(X,y):.4f}'\n\n'train score = 0.9964'\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',alpha=0.2,label='choco')\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales_hat[df_train.type=='choco'],'--',color='C0')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',alpha=0.2,label='vanilla')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales_hat[df_train.type=='vanilla'],'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f81d2560c40&gt;\n\n\n\n\n\n* 오버피팅에 대한 제 개념: 통계에서 “관측치 = 언더라잉 + 랜덤” 으로 볼 수 있다. 모형이 설명해야할 영역은 “언더라잉” 이다. 만약에 모형이 언더라잉을 잘 설명하지 못한다면 언더피팅이고, 주어진 모형이 언더라잉을 넘어 오차항까지 설명하고 있다면 오버피팅이다.\n\n마음속의 underlying 을 간직한다 – 애매하죠?\n그 underlying 보다 잘 맞추면 오버피팅이다.\n내 마음속의 underlying 제대로 학습못하고 있다고 판단되면 모형미스 혹은 언더피팅이다.\n\n이러한 논리로 인하면 위의 의사결정나무로 적합된 결과는 오버피팅이다. (그렇지만 언더피팅보단 나을지도?)"
  },
  {
    "objectID": "posts/10wk-38.html#a.-분할이-정해졌을때-haty을-결정하는-방법",
    "href": "posts/10wk-38.html#a.-분할이-정해졌을때-haty을-결정하는-방법",
    "title": "10wk-38: 아이스크림 – 의사결정나무 원리",
    "section": "A. 분할이 정해졌을때 \\(\\hat{y}\\)을 결정하는 방법?",
    "text": "A. 분할이 정해졌을때 \\(\\hat{y}\\)을 결정하는 방법?\n- step1~4\n\n## step1\nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1) \n## step3 \npredictr.fit(X,y) \n## step4 -- pass \n# predictr.predict(X) \n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n- tree 시각화 \\(\\to\\) 분할파악\n\nsklearn.tree.plot_tree(predictr)\n\n[Text(0.5, 0.75, 'x[0] &lt;= 5.05\\nsquared_error = 111.946\\nsamples = 100\\nvalue = 33.973'),\n Text(0.25, 0.25, 'squared_error = 34.94\\nsamples = 45\\nvalue = 24.788'),\n Text(0.75, 0.25, 'squared_error = 49.428\\nsamples = 55\\nvalue = 41.489')]\n\n\n\n\n\n- 분할에 따른 \\(\\hat{y}\\) 계산\n\ndf_train[df_train.temp&lt;= 5.05].sales.mean(),df_train[df_train.temp&gt; 5.05].sales.mean()\n\n(24.787609205775055, 41.489079055828356)"
  },
  {
    "objectID": "posts/10wk-38.html#b.-분할을-결정하는-방법",
    "href": "posts/10wk-38.html#b.-분할을-결정하는-방법",
    "title": "10wk-38: 아이스크림 – 의사결정나무 원리",
    "section": "B. 분할을 결정하는 방법?",
    "text": "B. 분할을 결정하는 방법?\n- 예비학습\n\npredictr.score(X,y)\n\n0.6167038863844929\n\n\n\n이 값이 내부적으로 어떻게 계산된거지?\n\n\npredictr.score??\n\n\ny_hat = y_pred = predictr.predict(X)\nsklearn.metrics.r2_score(y,y_pred)\n\n0.6167038863844929\n\n\n- 좋은 분할을 판단하는 기준? – 여기에서 r2_score가 이용됨\n- 우선 논의를 편하게하기 위해서 \\(({\\bf X},{\\bf y})\\)와 경계값 \\(c\\)를 줄때 \\(\\hat{\\bf y}\\)을 계산해주는 함수를 구현하자.\n\ndef fit_predict(X,y,c):\n    X = np.array(X).reshape(-1)\n    y = np.array(y) \n    yhat = y*0   \n    yhat[X&lt;=c] = y[X&lt;=c].mean()\n    yhat[X&gt;c] = y[X&gt;c].mean()\n    return yhat\n\n- 서로 다른 분할에 대하여 시각화를 진행\n\nyhat_bad = fit_predict(X,y,c=-1)\nyhat_good = fit_predict(X,y,c=5)\nfig, ax = plt.subplots(1,2) \nax[0].plot(X,y,'o',alpha=0.5)\nax[0].plot(X,yhat_bad,'--.')\nax[0].set_title('bad')\nax[1].plot(X,y,'o',alpha=0.5)\nax[1].plot(X,yhat_good,'--.')\nax[1].set_title('good')\n\nText(0.5, 1.0, 'good')\n\n\n\n\n\n\n딱봐도 오른쪽이 좋은 분할같은데, 컴퓨터한테 이걸 어떻게 설명하지?\n\n- 좋은분할을 구하는 이유는 좋은 yhat을 얻기 위함이다. 그렇다면 좋은 yhat을 얻게 해주는 분할이 좋은 분할이라 해석할 수 있다. \\(\\to\\) 아이디어: 그런데 좋은 yhat은 sklearn.metrics.r2_score(y,yhat)의 값이 높지 않을까?\n- 그렇다면 위의 그림에서 왼쪽보다 오른쪽이더 좋은 분할이라면 r2_score(y,yhat_good)의 값이 r2_score(y,yhat_bad) 값보다 높을 것!\n\nsklearn.metrics.r2_score(y,yhat_bad), sklearn.metrics.r2_score(y,yhat_good)\n\n(0.13932141536746745, 0.6167038863844928)\n\n\n- 트리의 max_depth=1 일 경우 분할을 결정하는 방법 – 노가다..\n\n적당한 \\(c\\)를 고른다.\n분할 \\((-\\infty,c), [c,\\infty)\\) 를 생성하고 yhat를 계산한다.\nr2_score(y,yhat)를 계산하고 기록한다.\n1-3의 과정을 무한반복 한다. 그리고 r2_score(y,yhat)의 값을 가장 작게 만드는 \\(c\\)가 무엇인지 찾는다.\n\n\ncuts = np.arange(-5,15)\nfig = plt.figure()\ndef func(frame):\n    ax = fig.gca()\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.5)\n    c = cuts[frame] \n    yhat = fit_predict(X,y,c)\n    ax.plot(X,yhat,'.')\n    r2 = sklearn.metrics.r2_score(y,yhat)\n    ax.set_title(f'c={c}, r2_score={r2:.2f}')\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames=20\n)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- tree가 찾은 값 5.05를 우리가 직접 찾아보자.\n\ncuts = np.arange(-5,15,0.05).round(2)\nscore = np.array([sklearn.metrics.r2_score(y,fit_predict(X,y,c)) for c in cuts])\nplt.plot(cuts,score)\n\n\n\n\n- 방법1: 시각화로 찾는방법\n\npd.DataFrame({'cut':cuts,'score':score})\\\n.plot.line(x='cut',y='score',backend='plotly')\n\n                                                \n\n\n- 방법2: 정석\n\ncuts[score.argmax()]\n\n5.0\n\n\n- max_depth=2일 경우? max_depth=1의 결과로 발생한 2개의 조각을 각각 전체자료로 생각하고, max_depth=1일 때의 분할방법을 반복적용한다.\n- X=[temp,type] 와 같은 경우라면? 설명변수를 하나씩 고정하여 각각 최적분할을 생성하고 r2_score관점에서 가장 우수한 설명변수를 선택"
  },
  {
    "objectID": "posts/13wk-51.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-51.html#a.-y의-분포-xy의-관계-시각화",
    "title": "13wk-51: 아이스크림(type무의미) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train, \n    label='sales',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nsales\n280\n51.101886\n21.167573\n10.335207\n33.053077\n47.844021\n70.451589\n88.994376\nfloat64\n280\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for sales &gt;= 0.5\n\n\n\n\n\nFeature interaction between temp/sales in train_data\n\n\n\n\n\n\nType는 중요하지 않음"
  },
  {
    "objectID": "posts/13wk-51.html#target-variable-analysis",
    "href": "posts/13wk-51.html#target-variable-analysis",
    "title": "13wk-51: 아이스크림(type무의미) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-51.html#b.-중요한-설명변수",
    "href": "posts/13wk-51.html#b.-중요한-설명변수",
    "title": "13wk-51: 아이스크림(type무의미) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data = df_train, \n    label='sales',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231130_121551/\"\n\n\nModel Prediction for sales\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-3.549018\n-4.206044\n0.002171\n0.001249\n0.27914\n0.002171\n0.001249\n0.27914\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\ntemp\n25.164809\n1.617020\n0.000002\n5\n28.494276\n21.835342\n\n\ntype\n-0.048470\n0.059119\n0.929654\n5\n0.073258\n-0.170197\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\nsales_pred\nerror\n\n\n\n\n73\n-3.7\nvanilla\n12.432354\n24.724379\n12.292025\n\n\n191\n-0.3\nvanilla\n16.436525\n24.724379\n8.287854\n\n\n218\n14.7\nchoco\n60.178468\n52.961044\n7.217424\n\n\n166\n16.1\nchoco\n66.821367\n59.932861\n6.888506\n\n\n5\n23.2\nvanilla\n75.697957\n82.155197\n6.457240\n\n\n118\n8.3\nchoco\n45.364110\n38.923119\n6.440991\n\n\n198\n4.4\nvanilla\n24.924572\n31.039103\n6.114530\n\n\n7\n11.2\nchoco\n45.593168\n51.416027\n5.822859\n\n\n89\n25.7\nvanilla\n87.788320\n82.155197\n5.633123\n\n\n109\n2.0\nvanilla\n19.398204\n24.724379\n5.326174\n\n\n\n\n\n\n\n\ntype는 중오하지않음"
  },
  {
    "objectID": "posts/13wk-51.html#c.-관측치별-해석",
    "href": "posts/13wk-51.html#c.-관측치별-해석",
    "title": "13wk-51: 아이스크림(type무의미) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[0]]\n)\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\n\n\n\n\n0\n19.4\nchoco\n64.807407\n\n\n\n\n\n\n\n\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[1]]\n)\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\n\n\n\n\n1\n0.9\nvanilla\n25.656697"
  },
  {
    "objectID": "posts/07wk-27.html",
    "href": "posts/07wk-27.html",
    "title": "07wk-27: 아이스크림(이상치) / 회귀분석 – 추가해설",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 200\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n200.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n...\n...\n...\n\n\n95\n12.4\n17.508688\n\n\n96\n13.4\n17.105376\n\n\n97\n14.7\n17.164930\n\n\n98\n15.0\n18.555388\n\n\n99\n15.2\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df_train.temp,df_train.ice_sales,'o')\n\n\n\n\n\n상상: 온도가 -4.1인 지점에서 “썰매축제”가 열렸다고 가정하자. 그래서 사람이 갑자기 많이 왔음. 그래서 아이스크림이 많이 팔렸음.\n\n\n\n4. 분석\n- 선형회귀로 적합\n\n# step1 \nX,y = df_train[['temp']], df_train['ice_sales']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_train['ice_sales_hat'] = predictr.predict(X)\n\n- 시각화\n\n# plt.plot(df_train.temp,df_train.ice_sales,'o')\n# plt.plot(df_train.temp,df_train.ice_sales_hat,'--')\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\n\n\n\n\n- 새로운 unseen data를 가정, 데이터는 온도가 12.5~18 에 걸쳐있다고 가정한다.\n\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\nice_sales_hat\n\n\n\n\n0\n-4.1\n200.000000\n20.989373\n\n\n1\n-3.7\n9.234175\n20.731457\n\n\n2\n-3.0\n9.642778\n20.280103\n\n\n3\n-1.3\n9.657894\n19.183959\n\n\n4\n-0.5\n9.987787\n18.668126\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n17.508688\n10.350324\n\n\n96\n13.4\n17.105376\n9.705533\n\n\n97\n14.7\n17.164930\n8.867305\n\n\n98\n15.0\n18.555388\n8.673867\n\n\n99\n15.2\n18.787014\n8.544909\n\n\n\n\n100 rows × 3 columns\n\n\n\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(12.5,18,50)})\ndf_test['ice_sales_hat'] = predictr.predict(XX)\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--')\n\n\n\n\n\n온도가 올라갈수록 아이스크림 판매량은 줄어든다는 해석 (더 온도가 올라간다면 판매량이 음수가 나올 수도 있겠음 )\n저 정도의 아웃라이어는 모형에서 제외하는게 타당하지 않나? (하지만 저러한 아웃라이어가 데이터의 가치가 있을 수도 있음. 그런데 또 데이터의 가치가 있는지 얺는지는 어떻게 판단하지?)\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\ntrain,test의 score로 오버피팅을 정의하는 교재를 제가 싫어하는 이유\n\n위 그림의 예시는 train score가 test score 보다 좋을 수 있음. 그런데 이건 딱히 오버핏이라 보기 어려움.\n\n\n\n\n\n5. Discussion\n- 딱히 정답이 없음.."
  },
  {
    "objectID": "posts/07wk-35.html",
    "href": "posts/07wk-35.html",
    "title": "07wk-35: 아이스크림(이상치) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 200\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n200.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n...\n...\n...\n\n\n95\n12.4\n17.508688\n\n\n96\n13.4\n17.105376\n\n\n97\n14.7\n17.164930\n\n\n98\n15.0\n18.555388\n\n\n99\n15.2\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df_train.temp,df_train.ice_sales,'o')\n\n\n\n\n\n\n4. 분석\n- 분석: 의사결정나무\n\n# step1\nX = df_train[['temp']]\ny = df_train['ice_sales']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_train['ice_sales_hat'] = predictr.predict(X)\n\n\n# plt.plot(df_train.temp,df_train.ice_sales,'o')\n# plt.plot(df_train.temp,df_train.ice_sales_hat,'--')\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\n\n\n\n\n- 12.5~18 구간사이의 unseen data를 가상으로 만들고 예측값을 살펴보자.\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(12.5,18,100)})\n\n\ndf_test['ice_sales_hat'] = predictr.predict(XX)\n\n\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o',color='C0',alpha=0.5)\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--',color='C1',alpha=0.5)\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--',color='C2',linewidth=2)\n\n\n\n\n- -15~0 구간사이의 unseen data를 가상으로 만들고 예측값을 살펴보자.\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(-15,0,100)})\n\n\ndf_test['ice_sales_hat'] = predictr.predict(XX)\n\n\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o',color='C0',alpha=0.5)\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--',color='C1',alpha=0.5)\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--',color='C2',linewidth=2)\n\n\n\n\n\n뭐 이 데이터에서는 최선이지 않을까?\n\n\n\n6. HW\n- 없어요. 다른과목 중간고사 준비 잘하세요!"
  },
  {
    "objectID": "posts/03wk-11.html#a.-data-정리",
    "href": "posts/03wk-11.html#a.-data-정리",
    "title": "03wk-11: Medical Cost / 회귀분석",
    "section": "A. Data 정리",
    "text": "A. Data 정리\n\ndf_train.columns\n\nIndex(['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges'], dtype='object')\n\n\n\nX = pd.get_dummies(df_train.drop(['charges'],axis=1))\ny = df_train[['charges']]\n\n\nX\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\nsex_female\nsex_male\nsmoker_no\nsmoker_yes\nregion_northeast\nregion_northwest\nregion_southeast\nregion_southwest\n\n\n\n\n0\n19\n27.900\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n1\n18\n33.770\n1\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n28\n33.000\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n33\n22.705\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n32\n28.880\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\n30.970\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1334\n18\n31.920\n0\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1335\n18\n36.850\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1336\n21\n25.800\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1337\n61\n29.070\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n1338 rows × 11 columns\n\n\n\n\ny\n\n\n\n\n\n\n\n\ncharges\n\n\n\n\n0\n16884.92400\n\n\n1\n1725.55230\n\n\n2\n4449.46200\n\n\n3\n21984.47061\n\n\n4\n3866.85520\n\n\n...\n...\n\n\n1333\n10600.54830\n\n\n1334\n2205.98080\n\n\n1335\n1629.83350\n\n\n1336\n2007.94500\n\n\n1337\n29141.36030\n\n\n\n\n1338 rows × 1 columns"
  },
  {
    "objectID": "posts/03wk-11.html#b.-predictor-생성",
    "href": "posts/03wk-11.html#b.-predictor-생성",
    "title": "03wk-11: Medical Cost / 회귀분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression()"
  },
  {
    "objectID": "posts/03wk-11.html#c.-학습",
    "href": "posts/03wk-11.html#c.-학습",
    "title": "03wk-11: Medical Cost / 회귀분석",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-11.html#d.-예측",
    "href": "posts/03wk-11.html#d.-예측",
    "title": "03wk-11: Medical Cost / 회귀분석",
    "section": "D. 예측",
    "text": "D. 예측\n\ndf_train.assign(yhat = predictr.predict(X))\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\nyhat\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n25293.713028\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n3448.602834\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n6706.988491\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n3754.830163\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n5592.493386\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n12351.323686\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n3511.930809\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n4149.132486\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n1246.584939\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n37085.623268\n\n\n\n\n1338 rows × 8 columns"
  },
  {
    "objectID": "posts/03wk-11.html#e.-평가",
    "href": "posts/03wk-11.html#e.-평가",
    "title": "03wk-11: Medical Cost / 회귀분석",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y) # R^2\n\n0.7509130345985207\n\n\n\n0.7 이상이면 망한모형까지는 아님 (대회용으로는 부적절할 수 있으나 대충 쓸 수는 있는 정도)"
  },
  {
    "objectID": "posts/02wk-03.html#a.-test-답을-모름-제출해야-알-수-있음",
    "href": "posts/02wk-03.html#a.-test-답을-모름-제출해야-알-수-있음",
    "title": "02wk-03: 타이타닉 / 첫 제출",
    "section": "A. test – 답을 모름, 제출해야 알 수 있음",
    "text": "A. test – 답을 모름, 제출해야 알 수 있음\n- 제출결과는 리더보드에서 확인할 수 있음."
  },
  {
    "objectID": "posts/02wk-03.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "href": "posts/02wk-03.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "title": "02wk-03: 타이타닉 / 첫 제출",
    "section": "B. train – 스스로 풀어보고 채점할 수 있음",
    "text": "B. train – 스스로 풀어보고 채점할 수 있음\n- 캐글에서 code \\(\\to\\) New Notebook 클릭\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n\ndf_train=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_test=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n\n# !kaggle competitions download -c titanic\n# !unzip titanic.zip -d ./titanic\n# df_train = pd.read_csv('titanic/train.csv')\n# df_test = pd.read_csv('titanic/test.csv')\n# !rm titanic.zip\n# !rm -rf titanic/\n\n# 예비학습 – accuracy의 계산\n\ndf = pd.DataFrame({'Surv':[1,0,1,1,0],'Sex':['f','m','f','m','m']})\n\n- Surv 열의 선택\n\ndf.Surv\n\n0    1\n1    0\n2    1\n3    1\n4    0\nName: Surv, dtype: int64\n\n\n- Sex 열의 선택\n\ndf.Sex\n\n0    f\n1    m\n2    f\n3    m\n4    m\nName: Sex, dtype: object\n\n\n- Sex == f이면 생존(1), 그렇지 않으면 사망(0)이라고 예측\n\n(df.Sex == 'f')*1\n\n0    1\n1    0\n2    1\n3    0\n4    0\nName: Sex, dtype: int64\n\n\n- 결과를 정리하면 아래와 같음\n\npd.DataFrame({'real': df.Surv, 'estimate': (df.Sex == 'f')*1})\n\n\n\n\n\n\n\n\nreal\nestimate\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n1\n0\n\n\n4\n0\n0\n\n\n\n\n\n\n\n- accuracy를 손으로 계산하면 \\(\\frac{4}{5}=0.8\\).\n- 컴퓨터로 accuracy를 계산한다면\n\n(df.Surv == (df.Sex == 'f')*1).sum()/5 # 방법1\n\n0.8\n\n\n\n(df.Surv == (df.Sex == 'f')*1).mean() # 방법2\n\n0.8\n\n\n#\n- 실제자료의 accuracy를 구해보자.\n\n(df_train.Survived == (df_train.Sex == 'female')).mean()\n\n0.7867564534231201"
  },
  {
    "objectID": "posts/11wk-42.html",
    "href": "posts/11wk-42.html",
    "title": "11wk-42: Weighted_Data / 의사결정나무 weights",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.tree\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n3. weights\n# 예제1 – 아래의 예제를 생각하자.\n\nX = np.array([1,2,3,4,5,6,7]).reshape(-1,1)\ny = np.array([10,11,12,20,21,22,23])\n\n\nplt.plot(X,y,'o')\n\n\n\n\n- 어디에서 분기점을 나누는게 좋을까? 당연히 3.5 정도..\n\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\npredictr.fit(X,y)\nsklearn.tree.plot_tree(predictr);\n\n\n\n\n#\n# 예제2 – 아래의 예제를 생각하자.\n\nX = np.array([1]*5000+[2]*5000+[3,4,5,6,7]).reshape(-1,1)\ny = np.array([10]*5000+[11]*5000+[12,20,21,22,23])\n\n\nplt.plot(X,y,'o',alpha=0.1)\n\n\n\n\n- 분기점은 어디에 나누는게 좋을까? 이 경우는 1.5근처에서 나누는게 합리적으로 보임\n\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\n\n\npredictr.fit(X,y)\n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n\nsklearn.tree.plot_tree(\n    predictr\n)\n\n[Text(0.5, 0.75, 'x[0] &lt;= 1.5\\nsquared_error = 0.299\\nsamples = 10005\\nvalue = 10.505'),\n Text(0.25, 0.25, 'squared_error = 0.0\\nsamples = 5000\\nvalue = 10.0'),\n Text(0.75, 0.25, 'squared_error = 0.089\\nsamples = 5005\\nvalue = 11.009')]\n\n\n\n\n\n#\n# 예제3 – 다시 아래의 예제를 생각하자.\n\nX = np.array([1,2,3,4,5,6,7]).reshape(-1,1)\ny = np.array([10,11,12,20,21,22,23])\n\n\nplt.plot(X,y,'o')\nplt.plot(X[:2],y[:2],'o')\n\n\n\n\n\n주황색점들을 잘 맞추는 것이 파란색점들을 잘 맞추는 것보다 5000배정도 중요하다고 상상하자.\n\n\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\npredictr.fit(X,y,sample_weight=[5000,5000,1,1,1,1,1])\nsklearn.tree.plot_tree(predictr);\n\n\n\n\n#"
  },
  {
    "objectID": "posts/13wk-54.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-54.html#a.-y의-분포-xy의-관계-시각화",
    "title": "13wk-54: 체중감량(교호작용) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train, \n    label='Weight_Loss',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nWeight_Loss\n10000\n5.11908\n6.092669\n-3.484888\n0.277797\n2.683447\n12.075458\n18.725299\nfloat64\n10000\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for Weight_Loss &gt;= 0.5\n\n\n\n\n\nFeature interaction between Exercise/Weight_Loss in train_data"
  },
  {
    "objectID": "posts/13wk-54.html#target-variable-analysis",
    "href": "posts/13wk-54.html#target-variable-analysis",
    "title": "13wk-54: 체중감량(교호작용) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-54.html#b.-중요한-설명변수",
    "href": "posts/13wk-54.html#b.-중요한-설명변수",
    "title": "13wk-54: 체중감량(교호작용) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data = df_train, \n    label='Weight_Loss',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231130_133535/\"\n\n\nModel Prediction for Weight_Loss\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-1.013205\n-0.973093\n0.005269\n0.001751\n0.840761\n0.005269\n0.001751\n0.840761\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\nExercise\n6.735479\n0.105037\n7.094759e-09\n5\n6.951752\n6.519206\n\n\nSupplement\n4.018616\n0.073537\n1.344948e-08\n5\n4.170030\n3.867202\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\nWeight_Loss_pred\nerror\n\n\n\n\n4639\nTrue\nFalse\n4.358143\n0.484748\n3.873395\n\n\n1683\nTrue\nTrue\n18.432093\n14.966088\n3.466005\n\n\n1275\nFalse\nTrue\n1.693150\n5.012056\n3.318906\n\n\n2631\nFalse\nTrue\n8.328789\n5.012056\n3.316733\n\n\n5334\nTrue\nFalse\n3.769029\n0.484748\n3.284281\n\n\n3437\nFalse\nFalse\n-3.205225\n0.039978\n3.245204\n\n\n2761\nFalse\nTrue\n1.853100\n5.012056\n3.158956\n\n\n9675\nTrue\nTrue\n18.070419\n14.966088\n3.104331\n\n\n3161\nFalse\nFalse\n-3.020254\n0.039978\n3.060232\n\n\n4637\nFalse\nTrue\n2.077349\n5.012056\n2.934706"
  },
  {
    "objectID": "posts/13wk-54.html#c.-관측치별-해석",
    "href": "posts/13wk-54.html#c.-관측치별-해석",
    "title": "13wk-54: 체중감량(교호작용) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n\npredictr.predict(df_train.iloc[[1]])\n\n1    0.55076\nName: Weight_Loss, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[1]]*1\n)\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n1\n1.0\n0.0\n1.604542"
  },
  {
    "objectID": "posts/11wk-40.html#a.-기본시각화",
    "href": "posts/11wk-40.html#a.-기본시각화",
    "title": "11wk-40: Medical Cost / 의사결정나무의 시각화",
    "section": "A. 기본시각화",
    "text": "A. 기본시각화\n\nsklearn.tree.plot_tree(predictr);\n\n\n\n\n\n잘 안보임"
  },
  {
    "objectID": "posts/11wk-40.html#b.-max_depth-조정",
    "href": "posts/11wk-40.html#b.-max_depth-조정",
    "title": "11wk-40: Medical Cost / 의사결정나무의 시각화",
    "section": "B. max_depth 조정",
    "text": "B. max_depth 조정\n\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth=0\n);"
  },
  {
    "objectID": "posts/11wk-40.html#c.-변수이름-추가",
    "href": "posts/11wk-40.html#c.-변수이름-추가",
    "title": "11wk-40: Medical Cost / 의사결정나무의 시각화",
    "section": "C. 변수이름 추가",
    "text": "C. 변수이름 추가\n\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth=0,\n    feature_names=X.columns\n);"
  },
  {
    "objectID": "posts/11wk-40.html#d.-fig-오브젝트",
    "href": "posts/11wk-40.html#d.-fig-오브젝트",
    "title": "11wk-40: Medical Cost / 의사결정나무의 시각화",
    "section": "D. fig 오브젝트",
    "text": "D. fig 오브젝트\n- plt.gcf()를 이용하여 fig 오브젝트 추출\n\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth=1,\n    feature_names=X.columns\n);\nfig = plt.gcf()\n\n\n\n\n- fig.suptitle 을 이용하여 제목을 붙일 수도 있지 않을까?\n\nfig.suptitle(\"title??\")\n\nText(0.5, 0.98, 'title??')\n\n\n\nfig\n\n\n\n\n- dpi 조정\n\nfig.set_dpi(250)\nfig"
  },
  {
    "objectID": "posts/11wk-40.html#e.-matplotlib의-ax에-그리기",
    "href": "posts/11wk-40.html#e.-matplotlib의-ax에-그리기",
    "title": "11wk-40: Medical Cost / 의사결정나무의 시각화",
    "section": "E. matplotlib의 ax에 그리기",
    "text": "E. matplotlib의 ax에 그리기\n\nfig = plt.figure()\nax = fig.subplots(2,1)\nax[0].plot(y,y,'--')\nax[0].plot(y,predictr.predict(X),'o',alpha=0.1)\nsklearn.tree.plot_tree(predictr,feature_names=X.columns,ax=ax[1],max_depth=0);"
  },
  {
    "objectID": "posts/13wk-48.html#a.-맨날-쓰는-코드",
    "href": "posts/13wk-48.html#a.-맨날-쓰는-코드",
    "title": "13wk-48: 아이스크림 / 자료분석(Autogluon)",
    "section": "A. 맨날 쓰는 코드",
    "text": "A. 맨날 쓰는 코드\n\n# step1 -- pass\n# step2 \npredictr = TabularPredictor(label = 'sales')\n# step3 \npredictr.fit(df_train,num_gpus=1)\n# step4 \nyhat = predictr.predict(df_train)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231130_184003/\"\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231130_184003/\"\nAutoGluon Version:  0.8.1\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #140-Ubuntu SMP Thu Aug 4 02:23:37 UTC 2022\nDisk Space Avail:   1585.66 GB / 1887.51 GB (84.0%)\nTrain Data Rows:    100\nTrain Data Columns: 1\nLabel Column: sales\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n    Label info (max, min, mean, stddev): (61.561043278721556, 10.90026146402572, 33.97342, 10.63375)\n    If 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    447198.4 MB\n    Train Data (Original)  Memory Usage: 0.0 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['temp']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['temp']\n    0.0s = Fit runtime\n    1 features in original data used to generate 1 features in processed data.\n    Train Data (Processed) Memory Usage: 0.0 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.02s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 80, Val Rows: 20\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models ...\nFitting model: KNeighborsUnif ...\n    -4.2111  = Validation score   (-root_mean_squared_error)\n    0.0s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\n    -4.6714  = Validation score   (-root_mean_squared_error)\n    0.0s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMXT ...\n    Training LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -35.2477     = Validation score   (-root_mean_squared_error)\n    0.87s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    Training LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -5.3708  = Validation score   (-root_mean_squared_error)\n    0.36s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestMSE ...\n    -4.4041  = Validation score   (-root_mean_squared_error)\n    0.46s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: CatBoost ...\n    Training CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n    Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n    -4.321   = Validation score   (-root_mean_squared_error)\n    2.07s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -4.2375  = Validation score   (-root_mean_squared_error)\n    0.43s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    -3.6121  = Validation score   (-root_mean_squared_error)\n    1.61s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    -4.0555  = Validation score   (-root_mean_squared_error)\n    0.75s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    -3.4775  = Validation score   (-root_mean_squared_error)\n    0.81s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMLarge ...\n    Training LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\n    -3.979   = Validation score   (-root_mean_squared_error)\n    0.43s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    -3.4775  = Validation score   (-root_mean_squared_error)\n    0.26s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 8.33s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231130_184003/\")\n\n\n[1000]  valid_set's rmse: 5.45375\n[2000]  valid_set's rmse: 5.40245"
  },
  {
    "objectID": "posts/13wk-48.html#b.-적합결과-시각화",
    "href": "posts/13wk-48.html#b.-적합결과-시각화",
    "title": "13wk-48: 아이스크림 / 자료분석(Autogluon)",
    "section": "B. 적합결과 시각화",
    "text": "B. 적합결과 시각화\n\nsns.scatterplot(df_train, x='temp',y='sales',label='y')\nsns.lineplot(df_train, x='temp',y=yhat,color='C1',linestyle='--',label='yhat')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f652e2e53c0&gt;\n\n\n\n\n\n\n잘맞춘다?"
  },
  {
    "objectID": "posts/13wk-48.html#c.-모형들-확인",
    "href": "posts/13wk-48.html#c.-모형들-확인",
    "title": "13wk-48: 아이스크림 / 자료분석(Autogluon)",
    "section": "C. 모형들 확인",
    "text": "C. 모형들 확인\n\npredictr.leaderboard(silent=True)\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nNeuralNetTorch\n-3.477510\n0.003048\n0.812747\n0.003048\n0.812747\n1\nTrue\n10\n\n\n1\nWeightedEnsemble_L2\n-3.477510\n0.003352\n1.075003\n0.000304\n0.262256\n2\nTrue\n12\n\n\n2\nNeuralNetFastAI\n-3.612069\n0.005018\n1.607540\n0.005018\n1.607540\n1\nTrue\n8\n\n\n3\nLightGBMLarge\n-3.978956\n0.001014\n0.428261\n0.001014\n0.428261\n1\nTrue\n11\n\n\n4\nXGBoost\n-4.055491\n0.003652\n0.751393\n0.003652\n0.751393\n1\nTrue\n9\n\n\n5\nKNeighborsUnif\n-4.211090\n0.026856\n0.002591\n0.026856\n0.002591\n1\nTrue\n1\n\n\n6\nExtraTreesMSE\n-4.237516\n0.054158\n0.425446\n0.054158\n0.425446\n1\nTrue\n7\n\n\n7\nCatBoost\n-4.321046\n0.002000\n2.073435\n0.002000\n2.073435\n1\nTrue\n6\n\n\n8\nRandomForestMSE\n-4.404096\n0.042513\n0.458560\n0.042513\n0.458560\n1\nTrue\n5\n\n\n9\nKNeighborsDist\n-4.671405\n0.027403\n0.002722\n0.027403\n0.002722\n1\nTrue\n2\n\n\n10\nLightGBM\n-5.370826\n0.000874\n0.357949\n0.000874\n0.357949\n1\nTrue\n4\n\n\n11\nLightGBMXT\n-35.247682\n0.001486\n0.866613\n0.001486\n0.866613\n1\nTrue\n3"
  },
  {
    "objectID": "posts/13wk-48.html#d.-최강모형의-r2_score-계산",
    "href": "posts/13wk-48.html#d.-최강모형의-r2_score-계산",
    "title": "13wk-48: 아이스크림 / 자료분석(Autogluon)",
    "section": "D. 최강모형의 r2_score 계산",
    "text": "D. 최강모형의 r2_score 계산\n- r2_score 계산 – 방법1\n\n_y = df_train.sales\n_yhat = predictr.predict(df_train) \nsklearn.metrics.r2_score(_y,yhat)\n\n0.930515451144895\n\n\n- r2_score 계산 – 방법2\n\n_y = df_train.sales \n_yhat = predictr.predict(df_train,model='NeuralNetTorch')\nsklearn.metrics.r2_score(_y,_yhat)\n\n0.930515451144895"
  },
  {
    "objectID": "posts/13wk-48.html#e.-특정모형의-적합값-구경",
    "href": "posts/13wk-48.html#e.-특정모형의-적합값-구경",
    "title": "13wk-48: 아이스크림 / 자료분석(Autogluon)",
    "section": "E. 특정모형의 적합값 구경",
    "text": "E. 특정모형의 적합값 구경\n- XGBoost 궁금해..\n\n_y = df_train.sales \n_yhat = predictr.predict(df_train, model='XGBoost')\nsklearn.metrics.r2_score(_y,_yhat)\n\n0.9516437954914487\n\n\n\nsns.scatterplot(df_train, x='temp',y='sales',color='C0',label='y')\nplt.plot(df_train.temp,_yhat,'--',color='C1',label='yhat')\nax = plt.gca()\nax.set_title(\"XGBoost\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f5f401ddc30&gt;"
  },
  {
    "objectID": "posts/08wk-supp1.html",
    "href": "posts/08wk-supp1.html",
    "title": "08wk-supp1: 중간점검",
    "section": "",
    "text": "1. 강의영상"
  },
  {
    "objectID": "posts/03wk-10.html#a.-데이터",
    "href": "posts/03wk-10.html#a.-데이터",
    "title": "03wk-10: 아이스크림(범주) / 회귀분석 – 추가해설",
    "section": "A. 데이터",
    "text": "A. 데이터\n\nX = df_train[['temp','type']] # 독립변수, 설명변수, 피쳐\ny = df_train[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nX = X.assign(type = [type == 'choco' for type in X.type])"
  },
  {
    "objectID": "posts/03wk-10.html#b.-predictor-생성",
    "href": "posts/03wk-10.html#b.-predictor-생성",
    "title": "03wk-10: 아이스크림(범주) / 회귀분석 – 추가해설",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression() \npredictr \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-10.html#c.-학습-fit-learn",
    "href": "posts/03wk-10.html#c.-학습-fit-learn",
    "title": "03wk-10: 아이스크림(범주) / 회귀분석 – 추가해설",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-10.html#d.-예측-predict",
    "href": "posts/03wk-10.html#d.-예측-predict",
    "title": "03wk-10: 아이스크림(범주) / 회귀분석 – 추가해설",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n\nyhat = predictr.predict(X)\n\n\nplt.plot(df_train.temp,df_train.sales,'o',alpha=0.5)\nplt.plot(df_train.temp,yhat, 'x',alpha=0.5)\n\n\n\n\n- 어떻게 맞춘거지?\n\\[\\textup{아이스크림 판매량} = 40 + \\textup{아이스크림종류} \\times (-20) + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\npredictr.coef_, predictr.intercept_\n\n(array([[  2.52239574, -20.54021854]]), array([40.16877158]))\n\n\n- 온도가 -2이고, type이 초코라면? 예측값은?\n\nXX = pd.DataFrame({'temp':[-2.0],'type':[1]})\nXX\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n-2.0\n1\n\n\n\n\n\n\n\n\npredictr.predict(XX)\n\narray([[14.58376156]])\n\n\n- 온도가 -2이고, type이 바닐라라면? 예측값은?\n\nXX = pd.DataFrame({'temp':[-2.0],'type':[0]})\nXX\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n-2.0\n0\n\n\n\n\n\n\n\n\npredictr.predict(XX)\n\narray([[35.1239801]])"
  },
  {
    "objectID": "posts/12wk-46.html#a.-accuracy",
    "href": "posts/12wk-46.html#a.-accuracy",
    "title": "12wk-46: 평가지표의 이해",
    "section": "A. Accuracy",
    "text": "A. Accuracy\n- 느낌: 정답율\n- 정의: \\(\\text{accuracy}=\\frac{TP+TN}{total}=\\frac{\\#O/O+ \\#X/X}{total}\\)\n\n한국말로는 정확도, 정분류율이라고 한다.\n한국말이 헷갈리므로 그냥 영어를 외우는게 좋다.\n\n- (상확극 시점1) 왜 애큐러시는 불충분한가?\n\n회사: 퇴사자예측프로그램 개발해. 잘 맞출수록 좋고, 계산에 많은 시간이 걸리지 않는 가벼운 predictor일수록 좋아.\n최규빈: 귀찮은데 다 안나간다고 하자! -&gt; 99퍼의 accuracy\n\n\n모델에 사용한 파라메터 = 0. 그런데 애큐러시 = 99! 이거 엄청 좋은 모형이다?"
  },
  {
    "objectID": "posts/12wk-46.html#b.-tpr-recallsensitivity",
    "href": "posts/12wk-46.html#b.-tpr-recallsensitivity",
    "title": "12wk-46: 평가지표의 이해",
    "section": "B. TPR (=Recall=Sensitivity)",
    "text": "B. TPR (=Recall=Sensitivity)\n- 느낌: 이상상황 감지율!\n- 정의: \\(\\text{recall}=\\text{sensitivity}=\\text{TPR}=\\frac{TP}{TP+FN}=\\frac{\\# O/O}{\\# O/O+\\# O/X}\\)\n\n분모: 실제 O인 관측치 수\n분자: 실제 O를 O라고 예측한 관측치 수\n뜻: 실제 O를 O라고 예측한 비율\n\n- (상황극 시점2) 우리가 recall을 봐야하는 이유\n\n인사팀: 실제 퇴사자를 퇴사자로 예측해야 의미가 있음! 우리는 퇴사할것 같은 10명을 찍어달란 의미였어요! (그래야 면담을 하든 할거아냐!)\n최규빈: 계산빠르고 잘 맞추는 predictor 만들어 달라면서요? 제 predictor는 계산시간은 0이고, accuracy는 0.99에요.\n인사팀: (고민중..) 사실 생각해보니까 이 경우는 accuracy는 의미가 없네. 실제 나간 사람 중 최규빈이 나간다고 한 사람이 몇인지 카운트 하는게 더 의미가 있겠다. 우리는 앞으로 recall을 보겠다!\n\n\n예시1: 실제로 퇴사한 10명중 최규빈이 나간다고 찍은 사람이 5명이면 recall=0.5.\n\n\n예시2: 최규빈이 아무도 나가지 않는다고 예측해버린다? 실제 10명중에서 최규빈이 나간다고 적중시킨사람은 0명이므로 이 경우는 recall=0.\n\n\n인사팀 결론: 우리가 필요한건 recall 이니까 앞으로 recall을 가져와! accuracy는 큰 의미없어. (그래도 명색이 모델인데 accuracy가 90은 되면 좋겠다.)"
  },
  {
    "objectID": "posts/12wk-46.html#c.-precision",
    "href": "posts/12wk-46.html#c.-precision",
    "title": "12wk-46: 평가지표의 이해",
    "section": "C. Precision",
    "text": "C. Precision\n- 느낌: 적중률..\n- 정의: \\(\\text{precision}=\\frac{TP}{TP+FP}=\\frac{\\# O/O}{\\# O/O+\\# X/O}\\)\n\n분모: O라고 예측한 관측치\n분자: O라고 예측한 관측치중 진짜 O인 관측치\n뜻: O라고 예측한 관측치중 진짜 O인 비율\n\n- (상황극 시점3) Recall 만으로 불충분한 이유\n\n최규빈: 에휴.. 귀찮은데 그냥 좀 만 수틀리면 다 나갈것 같다고 해야겠다. -&gt; 한 100명 나간다고 했음 -&gt; 실제로 최규빈이 찍은 100명중에 10명이 다 나감!\n\n\n이 경우 accuracy는 91%, recall은 100% (퇴사자 10명을 일단은 다 맞췄으므로).\n\n\n인사팀: (화가 많이 남) 멀쩡한 사람까지 다 퇴사할 것 같다고 하면 어떡해요? 최규빈 연구원이 나간다고 한 100명중에 실제로 10명만 나갔어요.\n인사팀: 마치 총으로 과녁중앙에 맞춰 달라고 했더니 기관총을 가져와서 한번 긁은것이랑 뭐가 달라요? 맞추는게 문제가 아니고 precision이 너무 낮아요.\n최규빈: 저번 요청사항이 accuracy 90% 이상, recall은 높을수록 좋다는게 주문 아니었나요? 제 predictor는 accuracy가 91% 그리고 recall은 100% 입니다.\n인사팀: (고민중..) 앞으로는 recall과 함께 precision도 같이 제출하세요. 여기에서 precision은 당신이 나간다고 한 사람중에 실제 나간사람의 비율을 의미해요. 이 경우는 \\(\\frac{10}{100}\\)이니까 precision이 10%입니다.\n인사팀 속마음: Recall 올리겠다고 무작정 너무 많이 예측하지 말란 말이야!"
  },
  {
    "objectID": "posts/12wk-46.html#d.-f1-score",
    "href": "posts/12wk-46.html#d.-f1-score",
    "title": "12wk-46: 평가지표의 이해",
    "section": "D. F1-score",
    "text": "D. F1-score\n- 느낌: 적중률과 민감도의 균형\n- 정의: Recall과 precision의 조화평균\n- (상황극 시점4) recall, precision을 모두 고려\n\n최규빈: 그런데 recall/precision을 같이 내는건 좋은데요, 둘은 trade off의 관계에 있습니다. 물론 둘다 올리는 모형이 있다면 좋지만 그게 쉽지는 않아요. 보통은 precision을 올리려면 recall이 희생되는 면이 있고요, recall을 올리려고 하면 precision이 다소 떨어집니다.\n최규빈: 평가기준이 애매하다는 의미입니다. 모형1,2가 있는데 모형1은 모형2보다 precision이 약간 좋고 대신 recall이 떨어진다면 모형1이 좋은것입니까? 아니면 모형2가 좋은것입니까?\n인사팀: 그렇다면 둘을 평균내서 F1-score를 계산해서 제출해주세요.\n\n\n속마음: AUC 쓰면 더 좋은뎅.."
  },
  {
    "objectID": "posts/12wk-46.html#e.-specificity",
    "href": "posts/12wk-46.html#e.-specificity",
    "title": "12wk-46: 평가지표의 이해",
    "section": "E. Specificity",
    "text": "E. Specificity\n- 느낌: 안나갈 사람을 안나갔다고 찾아낸 비율\n- 정의: \\(\\text{specificity}=\\frac{TN}{FP+TN}=\\frac{\\# X/X}{\\# X/O+\\# X/X}\\)\n- 안중요함"
  },
  {
    "objectID": "posts/12wk-46.html#f.-fpr",
    "href": "posts/12wk-46.html#f.-fpr",
    "title": "12wk-46: 평가지표의 이해",
    "section": "F. FPR",
    "text": "F. FPR\n- 느낌: 생사람 잡은 사람의 비율 (오해해서 미안하다, TPR올릴려다 보니까 어쩔수 없었네)\n\nTPR(=recall)을 올리는 가장 편한방법은, 조금만 이상한 징조가 있어도 “너 나갈것 같은데?”라고 몰아가는 것이다. 하지만 이러한 방식은 적중률의 문제가 있다. 또한 멀쩡히 잘 다니고 있는 사람을 생트집 잡아 몰아간다는 문제도 있다.\n\n- 정의: \\(\\text{FPR}=1-\\text{specificity}=\\frac{FP}{FP+TN}\\)=\\(\\frac{\\# X/O}{\\# X/O+\\# X/X}\\)\n- 예시: 아래의 상황을 고려하자.\n\n총 회사원 1000명.\n최규빈 연구원이 “퇴사할것 같은 사람”으로 찍은 직원은 100명.\n실제 퇴사자 10명, 실제퇴사자 10중 최규빈 연구원이 나간다고 한 사람은 8명.\n실제 퇴사자중 최규빈 연구원이 눈치채지 못한 사람은 2명.\n최규빈은 92명의 생사람을 잡음..\n\n정리하면\n\nO/O = 8\nO/X = 2\nX/O = 92\nX/X = 898\n\n\nrecall = TPR = 8/10 # OO/(OO+OX)\n\n\nprecision = 8/100 # OO/(OO+XO)\n\n\nFPR = 92/990 # XO/(XO+XX) # 분모의미: 억울한사람이 되려면 일단 열시미 다니고 있어야함\n\n\nprecision, FPR # 비슷해보이는데 좀 다르죠?\n\n(0.08, 0.09292929292929293)"
  },
  {
    "objectID": "posts/12wk-46.html#f.-roc-curve",
    "href": "posts/12wk-46.html#f.-roc-curve",
    "title": "12wk-46: 평가지표의 이해",
    "section": "F. ROC curve",
    "text": "F. ROC curve\n- 휴식: 무빙.. (이영미 교수님1이 재밋다해서..)\n1 봄날의 햇살\n\npredictor: 박병은\n박과장!! 약속과 다르지 않소!! –&gt; 사살 –&gt; 진짜 간첩이었으므로 사살하는 순간 precision/recall 동시에 상승\n동해물과 백두산이… –&gt; 사살 –&gt; 이 사람은 진짜 간첩인지 아닌지 확실하지 않음. 그렇지만 사살함. 따라서 recall은 올라갈 수는 있겠으나 precision 하락 및 FPR 상승의 위험이 있음.\n이런 미친새끼.. 여기서 억울한 휴민트들이 얼마나.. –&gt; 너처럼 계속 classification하면 FPR이 너무 높아진다는 의미\n괜찮아.. 내가 알아서 할게 –&gt; 난 FPR 높아도 괜찮아, 위험감수하고 TPR 올리고 싶어. 이런 내 결정은 내가 알아서 책임질게\n\n- 소감: 총을 얼마나 민감하게 당길지를 결정함에 따라서 FPR과 TRP의 값이 수시로 달라진다.\n- 정의: \\(x\\)축=FPR, \\(y\\)축=TPR 을 그린 커브\n- 의미:\n\n“x=생사람잡은비율” vs “y=이상상황감지율=recall”을 그린 곡선이 ROC커브이다.\n생각해보면 생사람을 잡을 수록 (=\\(x\\)값이 커질수록) 감지율이 높아진다 (=\\(y\\)값도 커진다).\n생사람 잡은 비율이 매우 적은데 (=\\(x\\)값이 작은데) 감지율이 높으면 매우 우수한 predictor 이다.\n따라서 초반부터 ROC curve가 급격하게 상승하면 좋은 predictor 이다."
  },
  {
    "objectID": "posts/12wk-46.html#g.-auc",
    "href": "posts/12wk-46.html#g.-auc",
    "title": "12wk-46: 평가지표의 이해",
    "section": "G. AUC",
    "text": "G. AUC\n- 정의: ROC curve의 밑면적"
  },
  {
    "objectID": "posts/06wk-24.html",
    "href": "posts/06wk-24.html",
    "title": "06wk-24: 취업(다중공선성) / RidgeCV",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport sklearn.linear_model\n\n\n\n3. Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n\n4. RidgeCV\n- RidgeCV 클래스에서 모형을 선택해보자.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.RidgeCV()\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV()\n\n\n\npredictr.score(X,y)\n\n0.9999996840224911\n\n\n\npredictr.score(XX,yy)\n\n0.1191494594976158\n\n\n- alpha들의 후보를 우리가 직접 선정하자.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.RidgeCV(alphas=[5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8])\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])\n\n\n\npredictr.score(X,y)\n\n0.7521268560159359\n\n\n\npredictr.score(XX,yy)\n\n0.7450309251010895\n\n\n\npredictr.alpha_\n\n50000000.0\n\n\n참고로 이 적합결과는 아래의 코드를 실행한것과 같다\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.Ridge(alpha=50000000.0)\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidge(alpha=50000000.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=50000000.0)\n\n\n\npredictr.score(X,y)\n\n0.752126856015936\n\n\n\npredictr.score(XX,yy)\n\n0.7450309251010895"
  },
  {
    "objectID": "posts/01wk-02.html#a.-gender_submission.csv",
    "href": "posts/01wk-02.html#a.-gender_submission.csv",
    "title": "01wk-02: 타이타닉 데이터의 이해 – 추가해설",
    "section": "A. gender_submission.csv",
    "text": "A. gender_submission.csv\n- 왜 여자만 살까?\n\n\n\n\n\n\nGTP 답변\n\n\n\ngender_submission.csv파일은 단순히 모든 여성 승객이 생존한다고 가정한 예측을 담은 파일로, 실제 데이터에 기반한 예측이 아닙니다. 이 파일은 제출 파일의 형식과 구성을 보여주기 위한 예시입니다. 실제로는 성별이나 다른 특성에 따라 생존 여부가 결정되는 것이 아닌, 여러 다양한 변수들이 생존에 영향을 미치게 됩니다.\n실제로 타이타닉호에서는 여성들이 생존율이 높았습니다. 타이타닉호 침몰 사건 당시 여성과 아이들을 먼저 구조하고 구명보트에 태우는 등의 우선순위가 있었기 때문입니다. 따라서 “gender_submission.csv” 파일은 예시로서 제공되었고, 실제 예측 모델을 개발할 때에는 여러 다른 변수들도 고려하여 예측해야 합니다."
  },
  {
    "objectID": "posts/01wk-02.html#b.-embarked",
    "href": "posts/01wk-02.html#b.-embarked",
    "title": "01wk-02: 타이타닉 데이터의 이해 – 추가해설",
    "section": "B. embarked",
    "text": "B. embarked\n- 항구는 승객의 배경과 사회적 상황을 반영할 수 있다.\n\n\n\n\n\n\nGPT 답변\n\n\n\n네, 항구별로 승객들의 배경과 사회적 상황이 다를 수 있습니다. 다양한 항구에서 탑승한 승객들은 지역적, 경제적, 문화적인 차이를 반영할 수 있습니다. 몇 가지 예시를 들어보겠습니다:\n\n경제적 차이: 서로 다른 항구에서 탑승한 승객들은 다른 경제적 배경을 가지고 있을 수 있습니다. 상류층 승객이나 부유한 승객들이 탑승한 항구일수록 1등급 티켓을 선택한 경우가 많을 것입니다.\n문화적 차이: 다른 항구에서 탑승한 승객들은 다른 문화적 배경을 가지고 있을 수 있습니다. 이로 인해 승객들의 습관, 언어, 관습 등이 다를 수 있습니다.\n지역적 차이: 항구의 위치와 지역적 특성에 따라 승객들의 배경이 달라질 수 있습니다. 해당 지역의 주요 산업이나 경제 활동에 따라 승객들의 직업이나 사회적 지위가 다를 수 있습니다.\n가족 관계: 특정 항구에서 가족들이 함께 탑승한 경우, 해당 지역의 가족 구조와 관련된 차이가 반영될 수 있습니다.\n\n이러한 다양한 차이를 분석하여 항구별로 생존율이나 다른 변수들과의 상관 관계를 파악할 수 있습니다. 이는 모델링 과정에서 추가적인 특성 엔지니어링이나 분석에 활용될 수 있습니다.\n\n\n- 항구는 국적에 대한 정보를 내포할 수도 있다.\n\n\n\n\n\n\nGPT 답변\n\n\n\n타이타닉호의 데이터에서는 승객들의 국적에 대한 직접적인 정보는 제공되지 않습니다. 따라서 승객들의 국적에 대한 정보를 확인하는 것은 어렵습니다. 그러나 항구별로 승객들의 출발지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수는 있습니다.\n예를 들어, Cherbourg 항구가 프랑스에 위치하며, Queenstown 항구가 아일랜드에 위치해 있습니다. 따라서 해당 항구에서 탑승한 승객들 중 일부는 해당 국가의 국적을 가질 가능성이 높을 것입니다. 그러나 국적은 항상 탑승 항구와 일치하지 않을 수 있으며, 탑승한 승객들의 다양한 출신지와 배경을 고려해야 합니다.\n요약하면, 타이타닉호 데이터에서는 직접적인 국적 정보는 제공되지 않지만, 항구별로 승객들의 출신지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수 있습니다."
  },
  {
    "objectID": "posts/01wk-02.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "href": "posts/01wk-02.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "title": "01wk-02: 타이타닉 데이터의 이해 – 추가해설",
    "section": "C. sibsp와 parch에 대한 피처엔지니어링 아이디어",
    "text": "C. sibsp와 parch에 대한 피처엔지니어링 아이디어\n- 두 변수 sibsp와 parch를 합쳐 family_size라는 하나의 변수를 만들고, 이를 활용하여 y를 예측할 수 있다.\n\n데이터를 봐야 하겠지만 가족단위로 보트를 줬을 확률도 큼\n가족단위로 보트를 실제로 주었을 경우 family_size라는 변수는 매우 의미있는 해석이 됨\n\n\n\n\n\n\n\nGPT 답변\n\n\n\n가족 규모 특성 생성: ‘sibsp’와 ’parch’ 변수를 활용하여 승객의 가족 규모를 나타내는 새로운 변수를 만들 수 있습니다. 예를 들어, ‘sibsp’와 ’parch’ 값을 더한 후 1을 더해 ‘family_size’ 변수로 만들 수 있습니다. 이를 통해 가족 규모가 큰 승객이 생존 여부에 영향을 미칠 수 있는지 확인할 수 있습니다.\n\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n다중공선성: ‘sibsp’ 와 ‘parch’ 를 더하여 새로운 변수를 만든다면, 이후에는 ‘sibsp’와 ’parch’ 둘중 하나를 제거하나 둘다 제거하는 방법을 고려해야 한다."
  },
  {
    "objectID": "posts/01wk-02.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "href": "posts/01wk-02.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "title": "01wk-02: 타이타닉 데이터의 이해 – 추가해설",
    "section": "D. embarked와 class에 대한 피처엔지니어링의 아이디어",
    "text": "D. embarked와 class에 대한 피처엔지니어링의 아이디어\n- 경우에 따라서 두가지 범주를 하나도 합쳐서 만든 새로운 변수가 매우 의미있을 수 있음\n\n경우에 따라서 특정 국적 (프랑스)의 1등급 손님이 구명보트를 많이 얻었을 수도 있음.\n이럴경우 embarked와 class를 독립적으로 생각하는것보다 둘을 합친 변수가 훨씬 유의미한 변수임\n\n\n\n\n\n\n\nGPT 답변\n\n\n\n물론, 자세한 설명을 드리겠습니다. “항구와 클래스의 조합 특성 생성”이라는 개념은 ‘embarked’ (탑승 항구)와 ‘pclass’ (티켓 클래스) 두 변수를 활용하여 새로운 특성을 만들어 분석하는 것을 의미합니다.\n예를 들어, “embarked” 변수에는 ‘C’, ‘Q’, ‘S’와 같은 항구 정보가 있으며, “pclass” 변수에는 ’1’, ‘2’, ’3’과 같은 티켓 클래스 정보가 있습니다. 이 두 변수를 조합하여 새로운 변수를 만들어보겠습니다.\n생성할 새로운 변수의 예시:\n“embarked_pclass”: ‘C_1’, ‘C_2’, ‘C_3’, ‘Q_1’, ‘Q_2’, ‘Q_3’, ‘S_1’, ‘S_2’, ‘S_3’ 위와 같이 “embarked”와 “pclass”를 조합하여 새로운 변수 값을 만듭니다. 예를 들어 ’embarked’가 ’C’이고 ’pclass’가 ’1’인 경우, “embarked_pclass” 변수 값은 ’C_1’이 됩니다. 이렇게 조합된 변수는 해당 승객이 어떤 항구에서 탑승했고 어떤 티켓 클래스를 사용했는지를 동시에 나타내는 특성입니다.\n이렇게 생성된 “embarked_pclass” 변수를 사용하여 특정 항구와 클래스의 조합이 생존 여부에 어떤 영향을 미치는지를 분석할 수 있습니다. 예를 들어, ‘C_1’ 조합이 생존율과 관련이 있는지, ‘Q_3’ 조합이 생존 여부에 영향을 미치는지 등을 확인하여 해당 특성이 모델링에 유용한 정보를 제공하는지 알아볼 수 있습니다.\n\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n교호작용: 그냥 교호작용 고려한다는 소리죠? –&gt; 트리계열은 결과가 크게 좋아질 것이라 기대하기 어렵겠고, 선형모형계열로 적합한다면 결과가 좋아질 수도 있겠음.\n\n\n\n\n\n\n\n\nGPT 답변 2 – 그냥 “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것과 무슨차이냐는 질문에 대한 답변\n\n\n\n네, “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것도 가능합니다. 그러나 새로운 변수를 만드는 것은 데이터를 조금 더 유용하게 활용하는 방법 중 하나입니다. 여기에 몇 가지 이유가 있습니다:\n상호작용 효과 강조: 새로운 변수를 만들면 “embarked”와 “pclass”의 조합이 생존 여부에 미치는 영향을 강조할 수 있습니다. 단순히 각각의 변수를 개별적으로 사용하는 것보다 조합된 변수를 통해 서로 간의 상호작용 효과를 쉽게 확인할 수 있습니다.\n더 풍부한 패턴 탐색: 새로운 변수를 만들면 서로 다른 항구와 클래스 조합들 간의 생존률 차이를 더 세부적으로 분석할 수 있습니다. 이를 통해 특정 항구와 클래스의 승객들이 생존 여부에 미치는 영향을 더 자세히 이해할 수 있습니다.\n다양한 조합 고려: 조합된 변수를 만들면 “embarked”와 “pclass”의 가능한 모든 조합을 커버할 수 있습니다. 이는 분석의 전체적인 시각을 확장하고, 어떤 조합이 중요한지를 더 효과적으로 파악할 수 있도록 도와줍니다.\n모델 학습에 더 적합한 데이터 생성: 몇몇 머신 러닝 알고리즘은 상호작용 변수를 활용하여 모델의 성능을 향상시킬 수 있습니다. 이러한 알고리즘들은 새로운 변수를 활용하여 더 복잡한 패턴을 학습할 수 있습니다.\n새로운 변수를 만들어 분석하는 것은 분석의 창의성과 도메인 지식을 활용하여 데이터로부터 더 많은 의미를 도출하려는 노력의 일부입니다. 분석의 목적에 따라 각 변수를 개별적으로 사용하거나 조합하여 새로운 변수를 만들 수 있습니다.\n\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n교호작용: 선형모델을 적합하는 상황을 가정하자. 교호작용의 효과가 있는 자료에서, 이를 무시하고 적합한다면 (“embarked” 변수와 “pclass” 변수만 넣고 적합한다면) 언더핏이 생김."
  },
  {
    "objectID": "posts/03wk-13.html#a.-데이터-정리",
    "href": "posts/03wk-13.html#a.-데이터-정리",
    "title": "03wk-13: 타이타닉 / 로지스틱 – 추가해설",
    "section": "A. 데이터 정리",
    "text": "A. 데이터 정리\n\nX = pd.get_dummies(df_train.drop(['PassengerId','Survived'],axis=1))\ny = df_train[['Survived']]"
  },
  {
    "objectID": "posts/03wk-13.html#b.-predictor-생성",
    "href": "posts/03wk-13.html#b.-predictor-생성",
    "title": "03wk-13: 타이타닉 / 로지스틱 – 추가해설",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()\npredictr \n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-13.html#c.-학습-fit-learn",
    "href": "posts/03wk-13.html#c.-학습-fit-learn",
    "title": "03wk-13: 타이타닉 / 로지스틱 – 추가해설",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
  },
  {
    "objectID": "posts/03wk-13.html#a.-데이터정리",
    "href": "posts/03wk-13.html#a.-데이터정리",
    "title": "03wk-13: 타이타닉 / 로지스틱 – 추가해설",
    "section": "A. 데이터정리",
    "text": "A. 데이터정리\n\nX = pd.get_dummies(df_train[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]])\ny = df_train[[\"Survived\"]]"
  },
  {
    "objectID": "posts/03wk-13.html#b.-predictor-생성-1",
    "href": "posts/03wk-13.html#b.-predictor-생성-1",
    "title": "03wk-13: 타이타닉 / 로지스틱 – 추가해설",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()"
  },
  {
    "objectID": "posts/03wk-13.html#c.-학습",
    "href": "posts/03wk-13.html#c.-학습",
    "title": "03wk-13: 타이타닉 / 로지스틱 – 추가해설",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X, y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-13.html#d.-예측",
    "href": "posts/03wk-13.html#d.-예측",
    "title": "03wk-13: 타이타닉 / 로지스틱 – 추가해설",
    "section": "D. 예측",
    "text": "D. 예측\n\n#predictr.predict(X)\ndf_train.assign(Survived_hat=predictr.predict(X)).loc[:,['Survived','Survived_hat']]\n\n\n\n\n\n\n\n\nSurvived\nSurvived_hat\n\n\n\n\n0\n0\n0\n\n\n1\n1\n1\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n0\n0\n\n\n...\n...\n...\n\n\n886\n0\n0\n\n\n887\n1\n1\n\n\n888\n0\n1\n\n\n889\n1\n0\n\n\n890\n0\n0\n\n\n\n\n891 rows × 2 columns"
  },
  {
    "objectID": "posts/03wk-13.html#e.-평가",
    "href": "posts/03wk-13.html#e.-평가",
    "title": "03wk-13: 타이타닉 / 로지스틱 – 추가해설",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.8002244668911336"
  },
  {
    "objectID": "posts/07wk-30.html",
    "href": "posts/07wk-30.html",
    "title": "07wk-30: 아이스크림(교호작용) / 회귀분석",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\nimport sklearn.model_selection\n\n\n\n3. 예비학습\n\ndf = pd.DataFrame({'X1':[2,3,4,1],'X2':['A','B','A','C']})\ndf \n\n\n\n\n\n\n\n\nX1\nX2\n\n\n\n\n0\n2\nA\n\n\n1\n3\nB\n\n\n2\n4\nA\n\n\n3\n1\nC\n\n\n\n\n\n\n\n\npd.get_dummies(df)\n\n\n\n\n\n\n\n\nX1\nX2_A\nX2_B\nX2_C\n\n\n\n\n0\n2\nTrue\nFalse\nFalse\n\n\n1\n3\nFalse\nTrue\nFalse\n\n\n2\n4\nTrue\nFalse\nFalse\n\n\n3\n1\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\nX2_A, X2_B, X2_C는 셋다 있을 필요는 없지 않나? –&gt; 공선성문제가 생길수도 있음.\n\n\npd.get_dummies(df,drop_first=True)\n\n\n\n\n\n\n\n\nX1\nX2_B\nX2_C\n\n\n\n\n0\n2\nFalse\nFalse\n\n\n1\n3\nTrue\nFalse\n\n\n2\n4\nFalse\nFalse\n\n\n3\n1\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n4. Data\n- load\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n- 시각화 및 해석\n\nplt.plot(df_train[df.type=='choco'].temp,df_train[df.type=='choco'].sales,'o',label='choco')\nplt.plot(df_train[df.type=='vanilla'].temp,df_train[df.type=='vanilla'].sales,'o',label='vanilla')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7fa2047e20&gt;\n\n\n\n\n\n\n온도에 따른 아이스크림 판매량이 아이스크림의 tpye에 따라 동일하다면 기울기가 동일하고 절편이 다른 두 직선이 나올것임.\n하지만 현재는 초코보다 바닐라맛이 기온의 영향을 많이 받아보임 \\(\\to\\) (바닐라아이스크림,온도)는 (초코아이스크림,온도)보다 궁합이 좋다. \\(\\to\\) 아이스크림 type과 온도사이에는 교호작용이 존재한다.\n\n\n\n5. 분석1\n- 분석1: 모형을 아래와 같이 본다.\n\n\\({\\bf X}\\): temp, type\n\\({\\bf y}\\): sales\n\n\n# step1 \nX,y = pd.get_dummies(df_train[['temp','type']],drop_first=True), df['sales']\n# step2\npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n\n\npredictr.score(X,y)\n\n0.9249530603100549\n\n\n\n점수가 잘나왔다고 너무 좋아하지 마세요.\n시각화를 반드시 해보고 더 맞출수 있는 여지가 있는지 항상 확인할 것\n\n\nplt.plot(df_train[df.type=='choco'].temp,df_train[df.type=='choco'].sales,'o',label='choco',color='C0',alpha=0.5)\nplt.plot(df_train[df.type=='choco'].temp,df_train[df.type=='choco'].sales_hat,'--',color='C0')\nplt.plot(df_train[df.type=='vanilla'].temp,df_train[df.type=='vanilla'].sales,'o',label='vanilla',color='C1',alpha=0.5)\nplt.plot(df_train[df.type=='vanilla'].temp,df_train[df.type=='vanilla'].sales_hat,'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7fa2199ab0&gt;\n\n\n\n\n\n\n이 모형은 초코/바닐라에 대한 기울기차이를 “표현”할 수 없다. 이러한 상황은 “모형의 표현력이 약하다” 혹은 “언더피팅”인 상황이라고 한다.\n\n\n\n6. 분석2\n- 모형을 아래와 같이 본다.\n\n\\({\\bf X}\\): temp, type, temp \\(\\times\\) type\n\\({\\bf y}\\): sales\n\n\npd.get_dummies(df_train[['temp','type']],drop_first=True).eval('interaction = temp*type_vanilla')\n\n\n\n\n\n\n\n\ntemp\ntype_vanilla\ninteraction\n\n\n\n\n0\n-4.1\nFalse\n-0.0\n\n\n1\n-3.7\nFalse\n-0.0\n\n\n2\n-3.0\nFalse\n-0.0\n\n\n3\n-1.3\nFalse\n-0.0\n\n\n4\n-0.5\nFalse\n-0.0\n\n\n...\n...\n...\n...\n\n\n95\n12.4\nTrue\n12.4\n\n\n96\n13.4\nTrue\n13.4\n\n\n97\n14.7\nTrue\n14.7\n\n\n98\n15.0\nTrue\n15.0\n\n\n99\n15.2\nTrue\n15.2\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n# step1 \nX = pd.get_dummies(df_train[['temp','type']],drop_first=True).eval('interaction = temp*type_vanilla')\ny = df['sales']\n# step2\npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n\n\npredictr.score(X,y)\n\n0.9865793819066231\n\n\n\nplt.plot(df_train[df.type=='choco'].temp,df_train[df.type=='choco'].sales,'o',label='choco',color='C0',alpha=0.5)\nplt.plot(df_train[df.type=='choco'].temp,df_train[df.type=='choco'].sales_hat,'--',color='C0')\nplt.plot(df_train[df.type=='vanilla'].temp,df_train[df.type=='vanilla'].sales,'o',label='vanilla',color='C1',alpha=0.5)\nplt.plot(df_train[df.type=='vanilla'].temp,df_train[df.type=='vanilla'].sales_hat,'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7fa1925690&gt;\n\n\n\n\n\n\nNote: 초코/바닐라에 대한 절편차이는 type로, 초코/바닐라에 대한 기울기 차이는 temp\\(\\times\\)type로 표현한다."
  },
  {
    "objectID": "posts/12wk-45.html#a.-재현의-확인",
    "href": "posts/12wk-45.html#a.-재현의-확인",
    "title": "12wk-45: 아이스크림 / 부스팅",
    "section": "A. 재현의 확인",
    "text": "A. 재현의 확인\n- 아이디어:\n\n처음부터 yhat을 강하게 학습하지 않고 약하게 조금씩 학습하자.\n부족한 공부는 (=학습이 덜 되어있는 부분 =y-yhat)은 조금씩 강화하면서 보완하자.\n\n- 구현: my_trees, my_residuals를 직접구현\n\nmy_trees = [] \nmy_residuals = [] \n\n\nres = y - y.mean()\n# 첫공부 \nfor i in range(100):\n    tree = sklearn.tree.DecisionTreeRegressor(\n        criterion = 'friedman_mse',\n        max_depth=3\n    )\n    tree.fit(X,res)\n    yhat = tree.predict(X) \n    res = res - yhat * 0.1 # 학습한걸 다 반영하지 말고 0.1정도만 반영. 여기서 0.1은 학습율\n    my_trees.append(tree)\n    my_residuals.append(res)\n\n- 비교: my_trees와 trees의 비교 (고정된 \\(i\\))\n\ni=10\nfig = plt.figure()\nax = fig.subplots(2,2)\nax[0,0].plot(X,y,'o',alpha=0.5)\nax[0,0].plot(X,ensemble(my_trees,i))\nax[0,1].plot(X,y,'o',alpha=0.5)\nax[0,1].plot(X,ensemble(trees,i))\nsklearn.tree.plot_tree(my_trees[i],max_depth=0,ax=ax[1,0]);\nsklearn.tree.plot_tree(trees[i],max_depth=0,ax=ax[1,1]);\n\n\n\n\n- 비교: my_trees와 trees의 비교 (애니메이션)\n\n#i=10\nfig = plt.figure()\nax = fig.subplots(2,2)\nplt.close()\n#---#\ndef func(i):\n    ax[0,0].clear()\n    ax[0,0].plot(X,y,'o',alpha=0.5)\n    ax[0,0].plot(X,ensemble(my_trees,i))\n    #--#\n    ax[0,1].clear()\n    ax[0,1].plot(X,y,'o',alpha=0.5)\n    ax[0,1].plot(X,ensemble(trees,i))\n    #--#\n    ax[1,0].clear()\n    sklearn.tree.plot_tree(my_trees[i],max_depth=0,ax=ax[1,0]);\n    #--#\n    ax[1,1].clear()\n    sklearn.tree.plot_tree(trees[i],max_depth=0,ax=ax[1,1]);\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=100)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/12wk-45.html#b.-step별-분석",
    "href": "posts/12wk-45.html#b.-step별-분석",
    "title": "12wk-45: 아이스크림 / 부스팅",
    "section": "B. Step별 분석",
    "text": "B. Step별 분석\n\nfig,ax = plt.subplots(1,4,figsize=(8,2))\nplt.close()\n\n\ndef func(i):\n    ax[0].clear();\n    ax[0].plot(X,y,'o',alpha=0.5)\n    ax[0].plot(X,ensemble(my_trees,i),'--')\n    ax[0].set_title(\"Step0\")\n    ax[1].clear();\n    ax[1].set_ylim(-20,20)\n    ax[1].plot(X,my_residuals[i],'o',alpha=0.5)\n    ax[1].set_title(\"Step1:Residual\")\n    ax[2].clear();\n    ax[2].set_ylim(-20,20)\n    ax[2].plot(X,my_residuals[i],'o',alpha=0.5)\n    ax[2].plot(X,my_trees[i].predict(X),'--')\n    ax[2].set_title(\"Step2:Fit\")\n    ax[3].clear();\n    ax[3].plot(X,y,'o',alpha=0.5)\n    ax[3].plot(X,ensemble(my_trees,i),'--',color='C1')\n    ax[3].plot(X,ensemble(my_trees,i+1),'--',color='C3')\n    ax[3].set_title(\"Step3:Update\")        \n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames = 50\n)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n관찰1: “Step1: Residual”은 점점 단순오차차럼 변화한다.\n관찰2: “Step2: Fit”의 분기점들은 고정된 값이 아니다. (계속 변한다)\n관찰3: “Step3: Update” 업데이터되는 양은 반복이 진행될수록 점점 작아진다.\n\n- 위의 그림에서\n\nStep0: 공부할 자료, 현재까지 공부량\nStep1: 남은 공부량\nStep2: 공부! (이해O / 암기X)\nStep3: 공부의 10%의 기억.. 기억나는 것만 두뇌에 update되어있음.\n\n- 느낌: 조금씩 데이터를 학습한다. 학습할 자료가 오차항처럼 보인다면? 그때는 적합을 멈춘다. (오차항을 적합할 필요는 없잖아?)"
  },
  {
    "objectID": "posts/07wk-28.html#a.-결측치의-존재",
    "href": "posts/07wk-28.html#a.-결측치의-존재",
    "title": "07wk-28: 선형모형의 적",
    "section": "A. 결측치의 존재",
    "text": "A. 결측치의 존재\n- 문제: 데이터에서 누락된 값이 있는 경우, 선형모델이 돌아가지 않음. (“NaN이 있어서 모형을 적합할 수 없습니다”라는 에러 발생)\n- 해결방법\n\n방법1: 결측치를 제거\n\n결측치가 포함된 열을 제거\n결측치가 포함된 행을 제거\n위의 두 방법을 혼합\n\n방법2: 결측치를 impute\n\ntrain 에서는 fit_transform, test 에서는 transform\ntrain, test에서 모두 fit_transform\n임의의 값 (예를들면 -999)로 일괄 impute\ninterploation (이미지나 시계열 자료)"
  },
  {
    "objectID": "posts/07wk-28.html#b.-다중공선성의-존재",
    "href": "posts/07wk-28.html#b.-다중공선성의-존재",
    "title": "07wk-28: 선형모형의 적",
    "section": "B. 다중공선성의 존재",
    "text": "B. 다중공선성의 존재\n- 문제: 데이터의 설명변수가 역할이 겹칠경우 선형모형의 일반화 성능이 좋지 않음.\n- 해결방법\n\n방법1: 변수제거\n\nX의 corr을 파악하고 (혹은 히트맵을 그리고) 느낌적으로 제거\nPCA등 차원축소기법을 이용한 제거\n\n방법2: 공선성을 가지는 변수를 모아 새로운 변수로 변환\n\n느낌적으로 변환 (예시 Fsize = Sibsp + Parch + 1, 이후 Sibsp, Parch 는 drop)\nPCA를 이용한 변환\n\n방법3: Ridge, Lasso 등 패널티계열을 사용\n\nRigde\nLasso\nElastic net\n\n\n- 방법1-1 (X의 corr을 파악하고 느낌적으로 제거) 의 예시\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nX = df.loc[:,'gpa':'toeic2']\nX\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\n0\n0.051535\n135\n129.566309\n133.078481\n121.678398\n\n\n1\n0.355496\n935\n940.563187\n935.723570\n939.190519\n\n\n2\n2.228435\n485\n493.671390\n493.909118\n475.500970\n\n\n3\n1.179701\n65\n62.272565\n55.957257\n68.521468\n\n\n4\n3.962356\n445\n449.280637\n438.895582\n433.598274\n\n\n...\n...\n...\n...\n...\n...\n\n\n495\n4.288465\n280\n276.680902\n274.502675\n277.868536\n\n\n496\n2.601212\n310\n296.940263\n301.545000\n306.725610\n\n\n497\n0.042323\n225\n206.793217\n228.335345\n222.115146\n\n\n498\n1.041416\n320\n327.461442\n323.019899\n329.589337\n\n\n499\n3.626883\n375\n370.966595\n364.668477\n371.853566\n\n\n\n\n500 rows × 5 columns\n\n\n\ncorr 조사\n\nX.corr()\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\ngpa\n1.000000\n-0.033983\n-0.035722\n-0.037734\n-0.034828\n\n\ntoeic\n-0.033983\n1.000000\n0.999435\n0.999322\n0.999341\n\n\ntoeic0\n-0.035722\n0.999435\n1.000000\n0.998746\n0.998828\n\n\ntoeic1\n-0.037734\n0.999322\n0.998746\n1.000000\n0.998721\n\n\ntoeic2\n-0.034828\n0.999341\n0.998828\n0.998721\n1.000000\n\n\n\n\n\n\n\nheatmap 플랏\n\nsns.heatmap(X.corr(),annot=True)\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/07wk-28.html#c.-관련이-없는-변수의-존재",
    "href": "posts/07wk-28.html#c.-관련이-없는-변수의-존재",
    "title": "07wk-28: 선형모형의 적",
    "section": "C. 관련이 없는 변수의 존재",
    "text": "C. 관련이 없는 변수의 존재\n- 문제: 데이터에서 불필요한 설명변수가 너무 많을 경우 선형모형의 일반화 성능이 좋지 않음.\n\n불필요한 설명변수임의 쉬운 예시: 고객이름, ID, Index 관련 변수\n\n- 해결방법\n\n방법1: 변수제거\n\n(y,X)의 corr을 파악하고 (혹은 히트맵을 그리고) 느낌적으로 제거\nPCA를 이용한 제거\nLasso를 이용한 제거\n\n방법2: 더 많은 데이터를 확보 (궁극기술, 그런데 차원의 저주때문에 힘듬)\n\n- 방법1-1의 예시\n\nnp.random.seed(1)\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_balance = pd.DataFrame((np.random.randn(500,3)).reshape(500,3)*1,columns = ['balance'+str(i) for i in range(3)])\ndf_train = pd.concat([df,df_balance],axis=1)\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\n0\n135\n0.051535\n0\n1.624345\n-0.611756\n-0.528172\n\n\n1\n935\n0.355496\n0\n-1.072969\n0.865408\n-2.301539\n\n\n2\n485\n2.228435\n0\n1.744812\n-0.761207\n0.319039\n\n\n3\n65\n1.179701\n0\n-0.249370\n1.462108\n-2.060141\n\n\n4\n445\n3.962356\n1\n-0.322417\n-0.384054\n1.133769\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n-1.326490\n0.308204\n1.115489\n\n\n496\n310\n2.601212\n1\n1.008196\n-3.016032\n-1.619646\n\n\n497\n225\n0.042323\n0\n2.005141\n-0.187626\n-0.148941\n\n\n498\n320\n1.041416\n0\n1.165335\n0.196645\n-0.632590\n\n\n499\n375\n3.626883\n1\n-0.209847\n1.897161\n-1.381391\n\n\n\n\n500 rows × 6 columns\n\n\n\n\ndf_train.corr()\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\ntoeic\n1.000000\n-0.033983\n0.260183\n0.002682\n0.110530\n0.024664\n\n\ngpa\n-0.033983\n1.000000\n0.711022\n-0.025197\n0.005272\n0.020794\n\n\nemployment\n0.260183\n0.711022\n1.000000\n-0.007348\n0.036706\n0.032284\n\n\nbalance0\n0.002682\n-0.025197\n-0.007348\n1.000000\n-0.059167\n0.040035\n\n\nbalance1\n0.110530\n0.005272\n0.036706\n-0.059167\n1.000000\n-0.030215\n\n\nbalance2\n0.024664\n0.020794\n0.032284\n0.040035\n-0.030215\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df_train.corr(),annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n설명변수 balance0,1,2(=X3,X4,X5)는 반응변수 employment(=y)와 관련이 없어 -&gt; X3,X4,X5는 제외하자.\n반응변수 employment(=y)와 관련이 있는 설명변수인 toiec,gpa (=X1,X2)는 남기자.\n공선성체크: 설명변수 toeic, gpa (=X1,X2)의 corr은 -0.034 로 높지 않으니 다중공선성문제를 걱정할 필요가 없음.\n\n- 방법1-3의 예시\n\nnp.random.seed(1)\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_balance = pd.DataFrame((np.random.randn(500,3)).reshape(500,3)*1,columns = ['balance'+str(i) for i in range(3)])\ndf_train = pd.concat([df,df_balance],axis=1)\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\n0\n135\n0.051535\n0\n1.624345\n-0.611756\n-0.528172\n\n\n1\n935\n0.355496\n0\n-1.072969\n0.865408\n-2.301539\n\n\n2\n485\n2.228435\n0\n1.744812\n-0.761207\n0.319039\n\n\n3\n65\n1.179701\n0\n-0.249370\n1.462108\n-2.060141\n\n\n4\n445\n3.962356\n1\n-0.322417\n-0.384054\n1.133769\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n-1.326490\n0.308204\n1.115489\n\n\n496\n310\n2.601212\n1\n1.008196\n-3.016032\n-1.619646\n\n\n497\n225\n0.042323\n0\n2.005141\n-0.187626\n-0.148941\n\n\n498\n320\n1.041416\n0\n1.165335\n0.196645\n-0.632590\n\n\n499\n375\n3.626883\n1\n-0.209847\n1.897161\n-1.381391\n\n\n\n\n500 rows × 6 columns\n\n\n\n\n# step1\nX,y = df_train[['toeic','gpa','balance0','balance1','balance2']], df_train['employment']\n# step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(\n    Cs = [0.1, 1, 10, 100],\n    penalty='l1',\n    solver='liblinear',\n    random_state=42\n)\n# step3 \npredictr.fit(X,y)\n# step4 -- pass\n\nLogisticRegressionCV(Cs=[0.1, 1, 10, 100], penalty='l1', random_state=42,\n                     solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionCVLogisticRegressionCV(Cs=[0.1, 1, 10, 100], penalty='l1', random_state=42,\n                     solver='liblinear')\n\n\n\npredictr.coef_\n\narray([[0.00260249, 1.41401358, 0.        , 0.        , 0.        ]])\n\n\n\ns = pd.Series(predictr.coef_.reshape(-1))\ns.index= X.columns\ns\n\ntoeic       0.002602\ngpa         1.414014\nbalance0    0.000000\nbalance1    0.000000\nbalance2    0.000000\ndtype: float64"
  },
  {
    "objectID": "posts/07wk-28.html#d.-이상치의-존재",
    "href": "posts/07wk-28.html#d.-이상치의-존재",
    "title": "07wk-28: 선형모형의 적",
    "section": "D. 이상치의 존재",
    "text": "D. 이상치의 존재\n- 문제: 이상치가 존재할 경우 전체 모형이 무너질 수 있음\n- 이상치가 있을 경우 해결할 수 있는 방법\n\n방법1: 이상치를 제거하고 분석한다.\n\n느낌적으로 제거함.\n이상치를 감지하는 지표을 사용하여 제거한 이후 분석\n이상치를 자동으로 감지하는 모형을 사용하여 이상치를 제거한 이후 분석\n\n방법2: 로버스트 선형회귀 계열을 이용\n\nsklearn.linear_model.HuberRegressor 등\n\n\n방법3: 이상치를 완화시키는 변환을 사용\n\nsklearn.preprocessing.PowerTransformer 이용\n\n\n- 방법3-1의 예시\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 50\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})[:10]\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n5\n-0.3\n10.205951\n\n\n6\n0.3\n8.486925\n\n\n7\n0.4\n8.817227\n\n\n8\n0.4\n8.273155\n\n\n9\n0.7\n8.863784\n\n\n\n\n\n\n\n\ntransformr = sklearn.preprocessing.PowerTransformer()\nx,y = transformr.fit_transform(df_train).T\n\n\nsns.scatterplot(df_train,x='temp',y='ice_sales',label='before')\nsns.scatterplot(x=x,y=y,label='after')\n\n&lt;Axes: xlabel='temp', ylabel='ice_sales'&gt;\n\n\n\n\n\n—참고—\nPowerTransformer()는 자료가 정규분포가 아닌 경우 강제로 정규화하는 변환이다.\n\nx = np.random.exponential(scale=10, size=1000)\ntransformr = sklearn.preprocessing.PowerTransformer(method='box-cox')  # 'box-cox' 또는 'yeo-johnson' 중 선택 가능, 디폴트는 이오존슨\ny = transformr.fit_transform(x.reshape(-1, 1))\n\n\nfig, ax = plt.subplots(1,2)\nax[0].hist(x,bins=25)\nax[1].hist(y,bins=25)\nfig.set_figwidth(12)"
  },
  {
    "objectID": "posts/07wk-28.html#e.-교호작용의-존재",
    "href": "posts/07wk-28.html#e.-교호작용의-존재",
    "title": "07wk-28: 선형모형의 적",
    "section": "E. 교호작용의 존재",
    "text": "E. 교호작용의 존재\n- 문제: 설명 변수 간의 상호 작용이 있는 경우 이를 고려하지 않으면 모델이 데이터를 잘 설명하지 못할 수 있음.\n- 해결: 고려하면 됩니당.."
  },
  {
    "objectID": "posts/04wk-16.html#a.-결측치-체크",
    "href": "posts/04wk-16.html#a.-결측치-체크",
    "title": "04wk-16: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "A. 결측치 체크",
    "text": "A. 결측치 체크\n- 결측치확인\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB"
  },
  {
    "objectID": "posts/04wk-16.html#b.-시각화",
    "href": "posts/04wk-16.html#b.-시각화",
    "title": "04wk-16: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "B. 시각화",
    "text": "B. 시각화\n\nmsno.matrix(df_train) \n# msno.bar(df_train) # 큰 의미 X \n# msno.dendrogram(df_train) # 큰 의미 X \n# msno.heatmap(df_train) # 큰 의미 X \n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-16.html#c.-결측치-처리",
    "href": "posts/04wk-16.html#c.-결측치-처리",
    "title": "04wk-16: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "C. 결측치 처리",
    "text": "C. 결측치 처리\n\ndf_train.select_dtypes(include=\"number\")\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\n27.0\n0\n0\n13.0000\n\n\n887\n888\n1\n1\n19.0\n0\n0\n30.0000\n\n\n888\n889\n0\n3\nNaN\n1\n2\n23.4500\n\n\n889\n890\n1\n1\n26.0\n0\n0\n30.0000\n\n\n890\n891\n0\n3\n32.0\n0\n0\n7.7500\n\n\n\n\n891 rows × 7 columns\n\n\n\n\ndef impute_missing(df):\n    df_imputed = df.copy()\n    df_num = df.select_dtypes(include=\"number\")\n    df_cat = df.select_dtypes(exclude=\"number\")\n    df_imputed[df_num.columns] = sklearn.impute.SimpleImputer().fit_transform(df_num) \n    df_imputed[df_cat.columns] = sklearn.impute.SimpleImputer(strategy='most_frequent').fit_transform(df_cat) \n    return df_imputed\n\n\nimpute_missing(df_test)\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892.0\n3.0\nKelly, Mr. James\nmale\n34.50000\n0.0\n0.0\n330911\n7.8292\nB57 B59 B63 B66\nQ\n\n\n1\n893.0\n3.0\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.00000\n1.0\n0.0\n363272\n7.0000\nB57 B59 B63 B66\nS\n\n\n2\n894.0\n2.0\nMyles, Mr. Thomas Francis\nmale\n62.00000\n0.0\n0.0\n240276\n9.6875\nB57 B59 B63 B66\nQ\n\n\n3\n895.0\n3.0\nWirz, Mr. Albert\nmale\n27.00000\n0.0\n0.0\n315154\n8.6625\nB57 B59 B63 B66\nS\n\n\n4\n896.0\n3.0\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.00000\n1.0\n1.0\n3101298\n12.2875\nB57 B59 B63 B66\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305.0\n3.0\nSpector, Mr. Woolf\nmale\n30.27259\n0.0\n0.0\nA.5. 3236\n8.0500\nB57 B59 B63 B66\nS\n\n\n414\n1306.0\n1.0\nOliva y Ocana, Dona. Fermina\nfemale\n39.00000\n0.0\n0.0\nPC 17758\n108.9000\nC105\nC\n\n\n415\n1307.0\n3.0\nSaether, Mr. Simon Sivertsen\nmale\n38.50000\n0.0\n0.0\nSOTON/O.Q. 3101262\n7.2500\nB57 B59 B63 B66\nS\n\n\n416\n1308.0\n3.0\nWare, Mr. Frederick\nmale\n30.27259\n0.0\n0.0\n359309\n8.0500\nB57 B59 B63 B66\nS\n\n\n417\n1309.0\n3.0\nPeter, Master. Michael J\nmale\n30.27259\n1.0\n1.0\n2668\n22.3583\nB57 B59 B63 B66\nC\n\n\n\n\n418 rows × 11 columns\n\n\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n결측치 처리가 별로임: 사실 타이타닉 자료의 경우 위와 같이 train/test를 독립된 데이터세트로 보고 결측치를 처리하는 것 보다 모듈19-MinMaxScaler에서 소개한 것 처럼 train에서 fit한 imputer를 test에서 .transform()하는 것이 훨씬 자연스럽다. 다만 이 예제를 설명하는 단계에서는 train/test를 독립된 데이터세트로 보고 전처리를 할 경우 생기는 비상식을 이해하기 어려워 위와 같은 방식으로 진행했음."
  },
  {
    "objectID": "posts/04wk-16.html#a.-자료의-정리",
    "href": "posts/04wk-16.html#a.-자료의-정리",
    "title": "04wk-16: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "A. 자료의 정리",
    "text": "A. 자료의 정리\n\n{c:len(set(df_train[c])) for c in df_train.select_dtypes(include=\"object\").columns}\n\n{'Name': 891, 'Sex': 2, 'Ticket': 681, 'Cabin': 148, 'Embarked': 4}\n\n\n\nX = pd.get_dummies(impute_missing(df_train).drop(['Survived','Name','Ticket','Cabin'],axis=1))\ny = impute_missing(df_train)[['Survived']]\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n너무 많은 범주: Name, Ticket, Cabin은 너무 많은 범주를 포함하고 있으므로 제외한다고 설명했다. 그런데 사실\n\n이름에는 혼인여부에 대한 정보가 포함될 수도 있어서 버리기 아깝다.\n범주가 너무 많다면 통째로 변수를 제거하는 것 보다, 공통범주로 묶어서 처리하는 방법도 고려할 수 있다.\n\n는 점에서 다른 분석방법을 고려할 수도 있다.\n\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\nPassengerId: 사실 PassengerId불필요한 정보이고 (PassengerId 와 y는 corr이 없음) 있어봤자 오버핏만 유발하므로 제외하는게 좋다. 단지 이 시점에서는 “불필요한 변수를 제거하지 않으면 왜 오버피팅 현상이 발생하는가?” 를 설명할 수 없어서 남겨두었던 것."
  },
  {
    "objectID": "posts/04wk-16.html#b.-predictor-생성",
    "href": "posts/04wk-16.html#b.-predictor-생성",
    "title": "04wk-16: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()"
  },
  {
    "objectID": "posts/04wk-16.html#c.-학습",
    "href": "posts/04wk-16.html#c.-학습",
    "title": "04wk-16: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/04wk-16.html#d.-예측",
    "href": "posts/04wk-16.html#d.-예측",
    "title": "04wk-16: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "D. 예측",
    "text": "D. 예측\n\npredictr.predict(X)\n\narray([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n       0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n       0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n       0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n       0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n       1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n       0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n       1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n       1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n       1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n       1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n       0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n       1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n       1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n       0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n       0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n       0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n       0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n       0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n       1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n       1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n       1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n       1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n       0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n       1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n       1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n       0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n       0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n       0., 1., 0., 1., 1., 0., 0.])"
  },
  {
    "objectID": "posts/04wk-16.html#e.-평가",
    "href": "posts/04wk-16.html#e.-평가",
    "title": "04wk-16: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.8024691358024691"
  },
  {
    "objectID": "posts/05wk-20.html",
    "href": "posts/05wk-20.html",
    "title": "05wk-20: StandardScaler를 이용한 전처리",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.preprocessing \n\n\n\n3. StandardScaler()\n- 예제자료 로드\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 스케일러를 생성\n\nsclr = sklearn.preprocessing.StandardScaler()\nsclr.fit_transform(df)\n\narray([[-0.8680409 , -0.98104887],\n       [ 1.81575704, -0.73905505],\n       [ 0.3061207 ,  0.75205327],\n       [-1.10287322, -0.08287854],\n       [ 0.17193081,  2.13248542],\n       [-1.10287322,  0.44828929],\n       [-0.34805505, -0.77533368],\n       [ 1.12803382, -0.75451182]])\n\n\n- 계산식\n\n(df.toeic - df.toeic.mean())/df.toeic.std(ddof=0) # 계산식\n\n0   -0.868041\n1    1.815757\n2    0.306121\n3   -1.102873\n4    0.171931\n5   -1.102873\n6   -0.348055\n7    1.128034\nName: toeic, dtype: float64\n\n\n\n\n4. 비교\n- MinMaxScaler와 StandardScaler는 데이터의 스케일을 조정하는 두 가지 일반적인 방법이다.\n\nMinMaxScaler:\n\n작동 원리: 데이터를 0과 1 사이의 값으로 조정\n장점: 원하는 범위 내로 데이터를 조정할 때 유용. 특히 신경망에서는 활성화 함수의 범위와 일치하도록 입력 값을 조정하는 데 유용.1\n단점: 이상치에 매우 민감하다. 이상치 때문에 전체 데이터의 스케일이 크게 영향받을 수 있음.\n\nStandardScaler:\n\n작동 원리: 데이터의 평균을 0, 표준편차를 1로 만드는 방식으로 조정.\n장점: 이상치에 MinMaxScaler보다 덜 민감함. 많은 통계적 기법들, 특히 PCA 같은 선형 알고리즘에서 잘 작동함.2\n단점: MinMaxScaler와 달리, 표준화된 데이터의 값이 특정 범위 내에 있음을 보장하지 않음.3\n\n\n1 sigmoid, tanh와 같은 활성화 함수의 출력값과 맞추는 용도2 그야 PCA는 정규분포를 가정하고 만든 알고리즘이라~3 MinMaxScaler도 딱히 엄격하게 보장하는건 아니야- 무식한 설명 (1)\n\nMinMaxScaler: 컴퓨터공학과, 전자공학과 느낌\nStandardScaler: 통계학과 느낌\n\n- 무식한 설명 (2)\n\nMinMaxScaler: 데이터가 기본적으로 0$\\(1 혹은 -1\\)$1 사이의 범위에 있다고 가정한다.\nStandardScaler: 데이터가 기본적으로 정규분포를 따른다고 가정하는 모형들과 잘 맞는다.\n\n- 둘 중 어느 것을 선택할지 결정하기 위한 고려사항:\n\n이상치가 많으면 StandardScaler가 더 적합할 수 있다.\n모델의 알고리즘과 특성에 따라 선택해야 한다. 예를 들어, 신경망은 일반적으로 0과 1 사이의 값이나 -1과 1 사이의 값으로 입력을 받는 활성화 함수를 사용하므로 MinMaxScaler가 적합할 수 있다.\n\n결론적으로, 두 스케일링 방법 중 어느 것이 더 좋은지는 사용 사례와 데이터의 특성에 따라 다르기 때문에, 가능한 경우 둘 다 시도해보고 모델의 성능을 비교하는 것이 좋다.\n- 둘 중 어느 것을 선택할지 결정하기 위한 고려사항 – 무식한 설명\n\n보통은 아무거나 해도 큰일 안남.\n아주 특수한 경우4를 제외하고는 어차피 이론적인 선택기준은 없음.\n\n\n\n4 Classical PCA"
  },
  {
    "objectID": "posts/05wk-21.html#a.-학부12학년-수준의-설명",
    "href": "posts/05wk-21.html#a.-학부12학년-수준의-설명",
    "title": "05wk-21: 취업(오버피팅) / 오버피팅의 개념 – 추가해설",
    "section": "A. 학부1~2학년 수준의 설명",
    "text": "A. 학부1~2학년 수준의 설명\n- 과적합(Overfitting): 머신러닝과 통계에서 자주 나타나는 문제로, 모델이 학습데이터에 과도하게 최적화가 되어서 실제로 새로운 데이터나 테스트데이터에서 성능이 저하되는 현상을 말함.\n- 오버피팅의 원인:\n\n불필요한 특징: 불필요한 특징이 데이터에 포함되어 있다면 오버피팅이 발생할 수 있음."
  },
  {
    "objectID": "posts/05wk-21.html#b.-일반인-수준의-설명",
    "href": "posts/05wk-21.html#b.-일반인-수준의-설명",
    "title": "05wk-21: 취업(오버피팅) / 오버피팅의 개념 – 추가해설",
    "section": "B. 일반인 수준의 설명",
    "text": "B. 일반인 수준의 설명\n- 시험 공부(1): 공부를 하랬더니 외우고 있음..\n- 시험 공부(2): (시험 하루 전날에) 공부 그만하고 술이나 먹으러 가자.. 더 공부하면 train error만 줄일 뿐이야..\n- 운전: 특정도로에서만 운전연습을 했음. 그래서 그 도로의 구멍, 곡률, 신호등의 위치까지 완벽하게 숙지하였음. 그 결과 그 도로에서는 잘하게 되었지만, 그 도로 이외의 다른도로에서 운전을 한다면 문제가 발생함.\n- 언어: 특정 주제나 특정 상황에 대한 대화만을 반복적으로 연습하여, 그 상황에서는 완벽한 대화가 가능하지만 그 외의 상황에서는 대화를 제대로 이어나갈 수 없음.\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n오버피팅에 대한 내 개념: Predictor가 언더라잉이 아니라 오차항을 적합하고 있는 상황.\n\n이 예제에서 언더라잉은 gpa,toiec만으로 충분히 적합할 수 있다.\n따라서 gpa,toiec 이의외 변수가 적합하고 있는 것은 오차항이다.\n그래서 오버핏이 발생"
  },
  {
    "objectID": "posts/13wk-50.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-50.html#a.-y의-분포-xy의-관계-시각화",
    "title": "13wk-50: 아이스크림(이상치) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train, \n    label='ice_sales',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nice_sales\n100\n13.178805\n4.337878\n8.273155\n11.296645\n12.856589\n14.294614\n50.0\nfloat64\n100\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for ice_sales &gt;= 0.5\n\n\n\n\n\nFeature interaction between temp/ice_sales in train_data"
  },
  {
    "objectID": "posts/13wk-50.html#target-variable-analysis",
    "href": "posts/13wk-50.html#target-variable-analysis",
    "title": "13wk-50: 아이스크림(이상치) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-50.html#b.-중요한-설명변수",
    "href": "posts/13wk-50.html#b.-중요한-설명변수",
    "title": "13wk-50: 아이스크림(이상치) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수?",
    "text": "B. 중요한 설명변수?\n\npass # 설명변수가 하나라서.."
  },
  {
    "objectID": "posts/13wk-50.html#c.-관측치별-해석",
    "href": "posts/13wk-50.html#c.-관측치별-해석",
    "title": "13wk-50: 아이스크림(이상치) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n\ndf_train.iloc[[0]]\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.0\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[0]])\n\n0    14.337484\nName: ice_sales, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[0]]\n)\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.0"
  },
  {
    "objectID": "posts/10wk-39.html",
    "href": "posts/10wk-39.html",
    "title": "10wk-39: 의사결정나무 Discussion",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. 의사결정나무 Discussions\n- 의사결정나무 vs 선형모형\n\n아이스크림+축제: 이상치에 강했음.\n운동+보조제: 교호작용을 고려하지 않아도 괜찮았음.\n토익유사점수: 다중공선성문제가 발생하는 경우에도 모형이 덜 망함.\n밸런스게임: 필요없는 변수가 있을 경우에도 모형이 덜 망함.\n\n- 의사결정나무의 장점들\n\n시각화가 유리하다. 설명력이 좋다.\n특성(feature)의 중요도를 파악하기 용이하다.\n\\({\\bf y} \\sim {\\bf X}\\) 사이에 존재하는 비선형성을 쉽게 모델링 할 수 있다. \\(\\to\\) 쉽게 말해서 잘 맞춘다는 소리에요\n모형에 대한 가정들이 필요 없다. (넌파라메트릭 모형 특징)\n\n- 의사결정나무의 단점: 오버피팅이 일어나기 너무 쉽다. (모형이 너무 흔들려..)\n- 의사결정나무에 대한 자잘한 개념들 (자격증에서 잘 물어봄)\n최소 샘플 분할(Min Samples Split):\n\n노드를 분할하기 위한 최소 샘플 수.\n적절한 설정으로 과소적합 및 과적합 조절 가능.\n\n가지치기(Pruning):\n\n트리의 불필요한 부분을 제거.\n과적합 방지 및 모델 성능 향상에 도움.\n\n정보 이득(Information Gain):\n\n분할 전후의 엔트로피 차이를 측정.\n높은 정보 이득은 더 좋은 분할을 의미.\n\n지니 불순도(Gini Impurity):\n\n노드의 순도 측정 지표.\n낮은 지니 불순도는 높은 클래스 순도를 의미.\n\n\n결국 “트리를 어디까지 성장시킬래?”라는 물음에 대답하기 위해 고안된 개념들이다. 근본적으로 “트리를 어디까지 성장시킬래?”에 대한 이론적인 명확한 기술은 없다. 이는 넌파라메트릭 모형이 가지는 공통적인 특징임.\n\n- 의사결정나무는 오버피팅을 잡기위해서 지루한 싸움을 시작함.\n\n발전과정: 의사결정나무 \\(\\to\\) 배깅, 랜덤포레스트, 부스팅\n의사결정나무를 응용한 다양한 방법들이 개발되었다. (너무 많아요 진짜) \\(\\to\\) 모든 방법들의 원리를 세세하게 파헤치는건 비효율적이다.\n그러한 다양한 방법들을 적덩히 분류해보면 대체로 배깅, 랜덤포레스트, 부스팅 계열로 나뉜다.1 \\(\\to\\) 배깅, 랜덤포레스트, 부스팅에 대한 공통적 아이디어를 파악하는건 효율적이다.\n현재 최고로 (state of the art, SOTA) 로 평가받는 알고리즘은 부스팅계열의 XGBoost, LightGBM, CatBoost 이다.\n\n\n\n1 모든 방법들이 세개의 카테고리중 하나에만 들어가는건 아니다"
  },
  {
    "objectID": "posts/05wk-19.html#a.-모티브",
    "href": "posts/05wk-19.html#a.-모티브",
    "title": "05wk-19: MinMaxScaler를 이용한 전처리",
    "section": "A. 모티브",
    "text": "A. 모티브\n- 예제자료 로드\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 모형을 돌려보고 해석한 결과\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n\n토익이 중요해? 아니면 학점이 중요해?\n얼만큼 중요해?\n\n- 모티브: 토익점수를 0-1사이로 맞추고 gpa도 0-1사이로 맞추면 해석이 쉽지 않을까?"
  },
  {
    "objectID": "posts/05wk-19.html#b.-사용방법",
    "href": "posts/05wk-19.html#b.-사용방법",
    "title": "05wk-19: MinMaxScaler를 이용한 전처리",
    "section": "B. 사용방법",
    "text": "B. 사용방법\n- 스케일러 생성\n\nsclr = sklearn.preprocessing.MinMaxScaler()\n\n- fit, transform\n\nsclr.fit(df)\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nsclr.transform(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n- fit_transform\n\nsclr.fit_transform(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])"
  },
  {
    "objectID": "posts/05wk-19.html#c.-잘못된-사용",
    "href": "posts/05wk-19.html#c.-잘못된-사용",
    "title": "05wk-19: MinMaxScaler를 이용한 전처리",
    "section": "C. 잘못된 사용",
    "text": "C. 잘못된 사용\n- sclr.fit()와 sclr.fit_transform()은 입력으로 2차원 자료구조를 기대한다. (그중에서도 은근히 numpy array를 기대함)\n\nsclr.fit_transform(df['toeic']) # df['toeic']는 1차원 자료구조\n\nValueError: Expected 2D array, got 1D array instead:\narray=[135. 935. 485.  65. 445.  65. 290. 730.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\n\n\nsclr.fit_transform(df[['toeic']]) # df[['toeic']]는 2차원 자료구조\n\narray([[0.08045977],\n       [1.        ],\n       [0.48275862],\n       [0.        ],\n       [0.43678161],\n       [0.        ],\n       [0.25862069],\n       [0.76436782]])"
  },
  {
    "objectID": "posts/05wk-19.html#a.-사용방법",
    "href": "posts/05wk-19.html#a.-사용방법",
    "title": "05wk-19: MinMaxScaler를 이용한 전처리",
    "section": "A. 사용방법",
    "text": "A. 사용방법\n- scaler를 오브젝트로 따로 만들지 않고 함수형으로 구현\n\nsklearn.preprocessing.minmax_scale(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n- 이것은 심지어 1차원 자료구조에도 적용가능하다.\n\nsklearn.preprocessing.minmax_scale(df['toeic'])\n\narray([0.08045977, 1.        , 0.48275862, 0.        , 0.43678161,\n       0.        , 0.25862069, 0.76436782])\n\n\n- 열별로 스케일링을 하는게 아니라 행별로 스케일링을 하는 것도 가능하다. (여기서는 필요없지만..)\n\nsklearn.preprocessing.minmax_scale(df,axis=1)\n\narray([[1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.]])"
  },
  {
    "objectID": "posts/05wk-19.html#b.-discussions",
    "href": "posts/05wk-19.html#b.-discussions",
    "title": "05wk-19: MinMaxScaler를 이용한 전처리",
    "section": "B. discussions",
    "text": "B. discussions\n- 언뜻 보기에는 MinMaxScaler 보다 minmax_scale이 좋아보이는데, 생각보다 일반적으로 minmax_scale을 사용하지는 않음. 이유는 아래와 같음.\n\n파이썬을 쓰는 사람들이 함수형 접근방식보다 객체지향 접근방식을 선호한다. (이건 제생각)\n학습데이터와 테스트데이터의 스케일링시 동일한 변환을 유지하는 상황에서는 MinMaxScaler 가 유리함.\ninverse_transform 메서드를 같은 부가기능을 제공함."
  },
  {
    "objectID": "posts/05wk-19.html#a.-잘못된-스케일링-방법-비효율의-문제",
    "href": "posts/05wk-19.html#a.-잘못된-스케일링-방법-비효율의-문제",
    "title": "05wk-19: MinMaxScaler를 이용한 전처리",
    "section": "A. 잘못된 스케일링 방법 – 비효율의 문제",
    "text": "A. 잘못된 스케일링 방법 – 비효율의 문제\n\nsklearn.preprocessing.minmax_scale(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsklearn.preprocessing.minmax_scale(XX)\n\narray([[0. ],\n       [0.5],\n       [1. ]])\n\n\n- 이 방법은 전략적으로 비효율적인 문제이지, 치팅과 관련된 치명적인 잘못은 아니다.\n\n만약에 어떠한 경우에 이러한 전처리 방식이 오히려 전략적이라고 판단될 경우 사용할수도 있음."
  },
  {
    "objectID": "posts/05wk-19.html#b.-올바른-스케일링-방법",
    "href": "posts/05wk-19.html#b.-올바른-스케일링-방법",
    "title": "05wk-19: MinMaxScaler를 이용한 전처리",
    "section": "B. 올바른 스케일링 방법",
    "text": "B. 올바른 스케일링 방법\n- 방법1\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit(X) # \n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nsclr.transform(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsclr.transform(XX)\n\narray([[0.125],\n       [0.375],\n       [0.625]])\n\n\n- 방법2\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit_transform(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsclr.transform(XX)\n\narray([[0.125],\n       [0.375],\n       [0.625]])"
  },
  {
    "objectID": "posts/05wk-19.html#c.-scaled_value-in-01",
    "href": "posts/05wk-19.html#c.-scaled_value-in-01",
    "title": "05wk-19: MinMaxScaler를 이용한 전처리",
    "section": "C. scaled_value \\(\\in\\) [0,1]?",
    "text": "C. scaled_value \\(\\in\\) [0,1]?\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit_transform(X)\nsclr.transform(XX)\n\narray([[0.16666667],\n       [0.5       ],\n       [1.33333333]])\n\n\n\n스케일링된 값이 1보다 클 수도 있다."
  },
  {
    "objectID": "posts/05wk-19.html#d.-아주-잘못된-스케일링-방법-정보누수",
    "href": "posts/05wk-19.html#d.-아주-잘못된-스케일링-방법-정보누수",
    "title": "05wk-19: MinMaxScaler를 이용한 전처리",
    "section": "D. 아주 잘못된 스케일링 방법 – 정보누수",
    "text": "D. 아주 잘못된 스케일링 방법 – 정보누수\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n- 데이터를 합친다.. (미쳤어??)\n\nnp.concatenate([X,XX])\n\narray([[1. ],\n       [2. ],\n       [3. ],\n       [4. ],\n       [3.5],\n       [1.5],\n       [2.5],\n       [5. ]])\n\n\n- 합친데이터에서 스케일링\n\nsklearn.preprocessing.minmax_scale(np.concatenate([X,XX]))[:5]\n\narray([[0.   ],\n       [0.25 ],\n       [0.5  ],\n       [0.75 ],\n       [0.625]])\n\n\n- 이러한 전처리 방식을 정보누수라고 한다. (대회 규정에 따라서 탈락사유에 해당함)"
  },
  {
    "objectID": "posts/04wk-17.html",
    "href": "posts/04wk-17.html",
    "title": "04wk-17: 취업 / 로지스틱의 깊은 이해",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\n\n\n\n3. 데이터 불러오기 \\(\\to\\) 학습\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\n\n\n\n\n0\n135\n0.051535\n0\n\n\n1\n935\n0.355496\n0\n\n\n2\n485\n2.228435\n0\n\n\n3\n65\n1.179701\n0\n\n\n4\n445\n3.962356\n1\n\n\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n\n\n496\n310\n2.601212\n1\n\n\n497\n225\n0.042323\n0\n\n\n498\n320\n1.041416\n0\n\n\n499\n375\n3.626883\n1\n\n\n\n\n500 rows × 3 columns\n\n\n\n\nX = df_train[['toeic','gpa']]\ny = df_train[['employment']]\npredictr = sklearn.linear_model.LogisticRegression()\npredictr.fit(X,y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n\n4. yhat이 나오는 방식?\n- 확인: 무슨 수식에 의하여 나오긴함\n\npredictr.coef_, predictr.intercept_\n\n(array([[0.00571598, 2.46520018]]), array([-8.45433334]))\n\n\n\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n\n0      0.000523\n1      0.096780\n2      0.453003\n3      0.005627\n4      0.979312\n         ...   \n495    0.976295\n496    0.432939\n497    0.000855\n498    0.016991\n499    0.932777\nLength: 500, dtype: float64\n\n\n\n((v &gt; 0.5) == predictr.predict(X)).mean()\n\n1.0\n\n\n- 하여튼 아래와 같은 구조임\n(구조1)\n\n\n\n\nflowchart LR\n  A[\"predictr (not learned)\"]\n  B[\"predictr (learned)\"]\n  C(\"v (=prob)\")\n  D(\"yhat\")\n  A --&gt; |\"`.fit(X,y)`\"| B --&gt; |\"`????`\" | C --&gt; |\"`v&gt;0.5`\"| D\n\n\n\n\n\n(구조2) – 단순화\n\n\n\n\nflowchart LR\n  A[\"predictr (not learned)\"]\n  B[\"predictr (learned)\"]\n  C(\"yhat\")\n  A --&gt; |\"`.fit(X,y)`\"| B --&gt; |\"`.predict(X)`\" | C\n\n\n\n\n\n- v 값을 알고 싶다면 어쩌지?\n\nv[:5].round(3)\n\n0    0.001\n1    0.097\n2    0.453\n3    0.006\n4    0.979\ndtype: float64\n\n\n\npredictr.predict_proba(X)[:5].round(3) \n\narray([[0.999, 0.001],\n       [0.903, 0.097],\n       [0.547, 0.453],\n       [0.994, 0.006],\n       [0.021, 0.979]])"
  },
  {
    "objectID": "posts/11wk-41.html",
    "href": "posts/11wk-41.html",
    "title": "11wk-41: Medical Cost / 의사결정나무 max_feature,random_state",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.tree\nimport graphviz\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n3. 데이터준비\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/insurance.csv')\ndf_train\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n\n\n\n\n1338 rows × 7 columns\n\n\n\n\n\n4. max_features\n- max_features에 대한 제한을 주지 않음 \\(\\to\\) 항상 같은 결과가 나옴\n\n# step1\nX = pd.get_dummies(df_train.loc[:,'age':'region'],drop_first=True)\ny = df_train['charges']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nsklearn.tree.plot_tree(predictr,max_depth=0,feature_names=X.columns);\n\n\n\n\n- max_features=4로 제한\n\nlen(X.columns) \n\n8\n\n\n\nmax_features=4로 제한한다는 의미는 8개의 설명변수중에서 4개만 임의로 뽑아서 그중에서 “최적의 변수”와 “최적의 \\(c\\)”를 찾겠다는 의미\n\n\n# step1\nX = pd.get_dummies(df_train.loc[:,'age':'region'],drop_first=True)\ny = df_train['charges']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor(max_features=4)\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \n\nDecisionTreeRegressor(max_features=4)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_features=4)\n\n\n\nsklearn.tree.plot_tree(predictr,max_depth=0,feature_names=X.columns);\n\n\n\n\n\n절반정도는 smoking 유무가 가장 위에 위치한다.\n\n\n\n5. random_state\n- max_features=4로 제한\n\n# step1\nX = pd.get_dummies(df_train.loc[:,'age':'region'],drop_first=True)\ny = df_train['charges']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor(max_features=4,random_state=43)\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \n\nDecisionTreeRegressor(max_features=4, random_state=43)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_features=4, random_state=43)\n\n\n\nsklearn.tree.plot_tree(predictr,max_depth=0,feature_names=X.columns);"
  },
  {
    "objectID": "posts/11wk-43.html#a.-원리",
    "href": "posts/11wk-43.html#a.-원리",
    "title": "11wk-43: 아이스크림 판매량 / 배깅",
    "section": "A. 원리",
    "text": "A. 원리\n- 알고리즘\n\n80개의 샘플에서 80개를 중복을 허용하여 뽑는다.\n1에서 뽑힌 샘플들을 이용하여 tree를 적합시킨다.\n1-2를 10번 반복하고 10개의 tree의 평균값을 yhat으로 선택한다."
  },
  {
    "objectID": "posts/11wk-43.html#b.-plot_tree-체크",
    "href": "posts/11wk-43.html#b.-plot_tree-체크",
    "title": "11wk-43: 아이스크림 판매량 / 배깅",
    "section": "B. plot_tree 체크",
    "text": "B. plot_tree 체크\n- 10개의 트리들의 리스트\n\ntrees = predictr.estimators_\ntrees\n\n[DecisionTreeRegressor(random_state=1644635363),\n DecisionTreeRegressor(random_state=1304269235),\n DecisionTreeRegressor(random_state=1794000214),\n DecisionTreeRegressor(random_state=1273087880),\n DecisionTreeRegressor(random_state=995922005),\n DecisionTreeRegressor(random_state=1372517728),\n DecisionTreeRegressor(random_state=1087222928),\n DecisionTreeRegressor(random_state=3687756),\n DecisionTreeRegressor(random_state=1772778467),\n DecisionTreeRegressor(random_state=92158766)]\n\n\n- 재표본데이터셋\n\npredictr.estimators_samples_[0] # (X,y)의 쌍을 80개 중복을 허용하여 뽑기 위한 인덱스\n\narray([19, 10, 25, 29, 50,  7, 46, 31, 10, 39, 78, 14, 54, 79, 28, 35, 73,\n        0, 74, 72, 66, 36, 55, 24, 41, 11, 68, 65, 71, 36, 54, 41, 76, 34,\n        0, 59,  5,  7, 67, 61, 64, 21, 27, 26, 43, 55, 49, 23, 29, 27, 41,\n       14, 58,  5, 12, 40, 12, 38,  8, 19, 63,  4, 35, 75, 64,  9, 69, 17,\n       32, 15, 60, 55, 18, 55, 22, 73, 28, 48, 57, 63])\n\n\n\nsamples = predictr.estimators_samples_\n\n- 첫번째 트리 재현\n\nsklearn.tree.plot_tree(\n    predictr.estimators_[0],\n    feature_names=X.columns,\n    max_depth=1\n);\n\n\n\n\n\nX_array = np.array(X)\ny_array = np.array(y)\n\n\ntree = sklearn.tree.DecisionTreeRegressor()\ntree.fit(X_array[samples[0]],y_array[samples[0]])\n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nsklearn.tree.plot_tree(\n    tree,\n    feature_names=X.columns,\n    max_depth=1\n);\n\n\n\n\n- tree 비교 (고정된 \\(i\\))\n\ni=4\nfig, ax = plt.subplots(2,1)\n#---#\nsklearn.tree.plot_tree(\n    predictr.estimators_[i],\n    feature_names=X.columns,\n    max_depth=1,\n    ax=ax[0]\n)\nax[0].set_title('predictr.estimator')\n#---#\nmy_tree = sklearn.tree.DecisionTreeRegressor()\nmy_tree.fit(X_array[samples[i]],y_array[samples[i]])\nsklearn.tree.plot_tree(\n    my_tree,\n    feature_names=X.columns,\n    max_depth=1,\n    ax=ax[1]\n);\nax[1].set_title('my_tree')\n\nText(0.5, 1.0, 'my_tree')\n\n\n\n\n\n- tree 비교 (애니메이션)\n\nfig, ax = plt.subplots(2,1)\nplt.close()\n#---#\ndef func(i):\n    ax[0].clear()\n    sklearn.tree.plot_tree(\n        predictr.estimators_[i],\n        feature_names=X.columns,\n        max_depth=1,\n        ax=ax[0]\n    )\n    ax[0].set_title('predictr.estimator')\n    #---#\n    ax[1].clear()\n    my_tree = sklearn.tree.DecisionTreeRegressor()\n    my_tree.fit(X_array[samples[i]],y_array[samples[i]])\n    sklearn.tree.plot_tree(\n        my_tree,\n        feature_names=X.columns,\n        max_depth=1,\n        ax=ax[1]\n    );\n    ax[1].set_title('my_tree')\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=10)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/11wk-43.html#c.-resampling-fit",
    "href": "posts/11wk-43.html#c.-resampling-fit",
    "title": "11wk-43: 아이스크림 판매량 / 배깅",
    "section": "C. ReSampling + Fit",
    "text": "C. ReSampling + Fit\n- 고정된 \\(i\\)\n\ni=4\nplt.plot(X,y,'o',alpha=0.2,color='gray')\nplt.plot(X_array[samples[i]],y_array[samples[i]],'o',alpha=1/3)\nplt.plot(X,trees[i].predict(X),'--')\n\n\n\n\n- 애니매이션\n\n#---#\nfig = plt.figure()\nax = fig.gca() \nplt.close()\n#---#\ndef func(i):\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.2,color='gray')\n    ax.plot(X_array[samples[i]],y_array[samples[i]],'o',alpha=1/3)\n    ax.plot(X,trees[i].predict(X),'--')\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=10)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/11wk-43.html#d.-앙상블결과-재현",
    "href": "posts/11wk-43.html#d.-앙상블결과-재현",
    "title": "11wk-43: 아이스크림 판매량 / 배깅",
    "section": "D. 앙상블결과 재현",
    "text": "D. 앙상블결과 재현\n- 최종결과물 (손으로..)\n\npredictr.predict(X)\n\narray([11.88782962, 14.05941305, 15.02231867, 18.03161729, 19.62619066,\n       19.86214551, 15.84293717, 15.95940294, 15.95940294, 20.30137042,\n       20.30137042, 22.51278676, 22.51278676, 23.68899036, 20.7954938 ,\n       26.45727462, 26.45727462, 20.48421278, 20.48421278, 25.08188452,\n       25.08188452, 25.08188452, 31.42611771, 25.99393577, 25.99393577,\n       25.99393577, 27.05912187, 27.05912187, 29.60439358, 29.94005816,\n       29.18760881, 29.18760881, 30.75340115, 30.82608162, 32.48384789,\n       31.03678302, 29.02978839, 31.17487146, 31.17487146, 31.05349512,\n       29.147739  , 29.147739  , 29.147739  , 30.40843883, 30.40843883,\n       33.53154643, 34.26668831, 33.20982041, 33.20982041, 36.82818648,\n       36.82818648, 34.66545508, 34.66545508, 34.24047203, 33.0829342 ,\n       33.0829342 , 35.29894866, 35.50366771, 35.47938512, 35.47938512,\n       38.8116606 , 38.8116606 , 37.74794717, 34.84063828, 39.73515434,\n       40.01130524, 40.05274675, 41.9980937 , 42.26869452, 40.81707653,\n       40.16985211, 41.5373848 , 39.69311797, 42.97563198, 45.99122302,\n       49.35681519, 43.64765096, 45.32629064, 47.10042494, 46.28105912])\n\n\n\nnp.stack([tree.predict(X) for tree in predictr.estimators_]).mean(axis=0)\n\narray([11.88782962, 14.05941305, 15.02231867, 18.03161729, 19.62619066,\n       19.86214551, 15.84293717, 15.95940294, 15.95940294, 20.30137042,\n       20.30137042, 22.51278676, 22.51278676, 23.68899036, 20.7954938 ,\n       26.45727462, 26.45727462, 20.48421278, 20.48421278, 25.08188452,\n       25.08188452, 25.08188452, 31.42611771, 25.99393577, 25.99393577,\n       25.99393577, 27.05912187, 27.05912187, 29.60439358, 29.94005816,\n       29.18760881, 29.18760881, 30.75340115, 30.82608162, 32.48384789,\n       31.03678302, 29.02978839, 31.17487146, 31.17487146, 31.05349512,\n       29.147739  , 29.147739  , 29.147739  , 30.40843883, 30.40843883,\n       33.53154643, 34.26668831, 33.20982041, 33.20982041, 36.82818648,\n       36.82818648, 34.66545508, 34.66545508, 34.24047203, 33.0829342 ,\n       33.0829342 , 35.29894866, 35.50366771, 35.47938512, 35.47938512,\n       38.8116606 , 38.8116606 , 37.74794717, 34.84063828, 39.73515434,\n       40.01130524, 40.05274675, 41.9980937 , 42.26869452, 40.81707653,\n       40.16985211, 41.5373848 , 39.69311797, 42.97563198, 45.99122302,\n       49.35681519, 43.64765096, 45.32629064, 47.10042494, 46.28105912])\n\n\n- 최종결과물 (코드로 정리)\n\ndef ensemble(trees,i=None):\n    if i is None:\n        i = len(trees)\n    else: \n        i = i+1\n    yhat = np.stack([tree.predict(X) for tree in trees[:i]]).mean(axis=0)\n    return yhat\n\n\nensemble(trees,0) # 0번트리만 적용\n\narray([10.90026146, 10.90026146, 10.90026146, 19.46336233, 19.46336233,\n       20.31785349, 16.3076088 , 16.3076088 , 16.3076088 , 20.27763408,\n       20.27763408, 21.52796629, 21.52796629, 21.52796629, 18.34698175,\n       27.5369675 , 27.5369675 , 20.30881248, 20.30881248, 25.04963215,\n       25.04963215, 25.04963215, 32.42440294, 26.49340711, 26.49340711,\n       26.49340711, 26.40925726, 26.40925726, 29.55903213, 30.75418385,\n       29.70592592, 29.70592592, 31.45007539, 32.89828946, 32.89828946,\n       31.12503261, 25.9552363 , 33.12203011, 33.12203011, 30.60313283,\n       29.45886461, 29.45886461, 29.45886461, 30.60789344, 30.60789344,\n       30.60789344, 36.5245913 , 34.24458444, 34.24458444, 37.4829917 ,\n       37.4829917 , 37.4829917 , 37.4829917 , 31.13974993, 31.13974993,\n       31.13974993, 31.13974993, 36.58400962, 35.1723381 , 35.1723381 ,\n       39.75311187, 39.75311187, 39.75311187, 34.68877582, 44.47780794,\n       39.1744058 , 40.19626989, 42.86734269, 42.60143843, 40.80476673,\n       40.80476673, 42.1996627 , 38.72741866, 41.43992372, 45.95732063,\n       50.81374143, 42.30473921, 42.30473921, 48.7391566 , 46.00793717])\n\n\n\nensemble(trees,1) # 0번트리,1번트리의 예측값 평균\n\narray([10.90026146, 12.45139248, 12.45139248, 18.56852168, 19.46336233,\n       20.31785349, 16.03419127, 16.57964463, 16.57964463, 21.02420483,\n       21.02420483, 21.3736233 , 21.3736233 , 23.07741787, 22.94197463,\n       27.5369675 , 27.5369675 , 19.83347885, 19.83347885, 26.16305209,\n       26.16305209, 26.16305209, 32.42440294, 28.7554569 , 28.7554569 ,\n       28.7554569 , 27.61337612, 27.61337612, 29.55903213, 30.75418385,\n       28.54972991, 28.54972991, 31.45007539, 30.82608162, 30.82608162,\n       31.66094517, 29.07604701, 32.65944392, 32.65944392, 30.60313283,\n       29.40787056, 29.40787056, 29.40787056, 30.5566788 , 30.5566788 ,\n       33.57934676, 36.5245913 , 35.63234869, 35.63234869, 37.25155232,\n       37.25155232, 35.85263528, 35.85263528, 32.46466946, 33.6755663 ,\n       33.6755663 , 35.78403852, 35.87817386, 35.1723381 , 35.1723381 ,\n       40.62427057, 40.62427057, 40.62427057, 34.68877582, 44.47780794,\n       41.82610687, 40.50051831, 41.83605471, 41.70310258, 40.80476673,\n       39.92694722, 40.6243952 , 40.08367119, 41.43992372, 43.69862217,\n       50.81374143, 42.30473921, 43.9833789 , 48.7391566 , 47.37354689])"
  },
  {
    "objectID": "posts/11wk-43.html#e.-학습과정-시각화",
    "href": "posts/11wk-43.html#e.-학습과정-시각화",
    "title": "11wk-43: 아이스크림 판매량 / 배깅",
    "section": "E. 학습과정 시각화",
    "text": "E. 학습과정 시각화\n- 고정된 \\(i\\)\n\ni=9\nfig,ax = plt.subplots(1,4,figsize=(8,2))\n#--#\nax[0].set_title(\"Step0\")\nax[0].plot(X,y,'o',color='gray',alpha=0.2)\n#--#\nax[1].set_title(\"Step1:ReSampling\")\nax[1].plot(X,y,'o',color='gray',alpha=0.2)\nax[1].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\n#--#\nax[2].set_title(\"Step2:Fit\")\nax[2].plot(X,y,'o',color='gray',alpha=0.2)\nax[2].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\nax[2].plot(X,trees[i].predict(X),'--')\n#--#\nax[3].set_title(\"Step3:Update(?)\")\nax[3].plot(X,y,'o',color='gray',alpha=0.2)\nax[3].plot(X,ensemble(trees,i),'--',color='C1')\n\n\n\n\n- 애니메이션\n\nfig,ax = plt.subplots(1,4,figsize=(8,2))\nplt.close()\n#---#\ndef func(i):\n    for a in ax:\n        a.clear()\n    #--#\n    ax[0].set_title(\"Step0\")\n    ax[0].plot(X,y,'o',color='gray',alpha=0.2)\n    #--#\n    ax[1].set_title(\"Step1:ReSampling\")\n    ax[1].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[1].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\n    #--#\n    ax[2].set_title(\"Step2:Fit\")\n    ax[2].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[2].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\n    ax[2].plot(X,trees[i].predict(X),'--')\n    #--#\n    ax[3].set_title(\"Step3:Update(?)\")\n    ax[3].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[3].plot(X,ensemble(trees,i),'--',color='C1')\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=10)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/13wk-57.html",
    "href": "posts/13wk-57.html",
    "title": "13wk-57: House Prices / 자료분석(Autogluon)",
    "section": "",
    "text": "ref: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview\n\n\n1. 강의영상\n\n\n\n2. Imports\n\n#!pip install autogluon.eda\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n#---#\nfrom autogluon.tabular import TabularPredictor\nimport autogluon.eda.auto as auto\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n3. Data\n\n!kaggle competitions download -c house-prices-advanced-regression-techniques\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\nDownloading house-prices-advanced-regression-techniques.zip to /root/Dropbox/MP\n100%|█████████████████████████████████████████| 199k/199k [00:00&lt;00:00, 820kB/s]\n100%|█████████████████████████████████████████| 199k/199k [00:00&lt;00:00, 819kB/s]\n\n\n\n!unzip house-prices-advanced-regression-techniques.zip\n\nArchive:  house-prices-advanced-regression-techniques.zip\n  inflating: data_description.txt    \n  inflating: sample_submission.csv   \n  inflating: test.csv                \n  inflating: train.csv               \n\n\n\ndf_submission = pd.read_csv(\"sample_submission.csv\")\ndf_train = pd.read_csv(\"train.csv\")\ndf_test = pd.read_csv(\"test.csv\")\n\n\n!rm sample_submission.csv\n!rm train.csv\n!rm test.csv\n!rm data_description.txt\n!rm house-prices-advanced-regression-techniques.zip\n\n\n\n4. 적합\n\nset(df_train.columns) - set(df_test.columns)\n\n{'SalePrice'}\n\n\n\n# step1 -- pass \n# step2 \npredictor = TabularPredictor(label = 'SalePrice')\n# step3 \npredictor.fit(df_train,num_gpus=1)\n# step4 \nyyhat = predictor.predict(df_test)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231130_200819/\"\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231130_200819/\"\nAutoGluon Version:  0.8.1\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #140-Ubuntu SMP Thu Aug 4 02:23:37 UTC 2022\nDisk Space Avail:   1585.50 GB / 1887.51 GB (84.0%)\nTrain Data Rows:    1460\nTrain Data Columns: 80\nLabel Column: SalePrice\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n    Label info (max, min, mean, stddev): (755000, 34900, 180921.19589, 79442.50288)\n    If 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    434212.09 MB\n    Train Data (Original)  Memory Usage: 4.06 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 3 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])  :  3 | ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n        ('int', [])    : 34 | ['Id', 'MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', ...]\n        ('object', []) : 43 | ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', ...]\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])  : 40 | ['MSZoning', 'Alley', 'LotShape', 'LandContour', 'LotConfig', ...]\n        ('float', [])     :  3 | ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n        ('int', [])       : 34 | ['Id', 'MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', ...]\n        ('int', ['bool']) :  3 | ['Street', 'Utilities', 'CentralAir']\n    0.3s = Fit runtime\n    80 features in original data used to generate 80 features in processed data.\n    Train Data (Processed) Memory Usage: 0.52 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.29s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 1168, Val Rows: 292\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models ...\nFitting model: KNeighborsUnif ...\n    -52278.8213  = Validation score   (-root_mean_squared_error)\n    0.01s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: KNeighborsDist ...\n    -51314.2734  = Validation score   (-root_mean_squared_error)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT ...\n    Training LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -27196.7065  = Validation score   (-root_mean_squared_error)\n    9.28s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBM ...\n    Training LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -28692.2871  = Validation score   (-root_mean_squared_error)\n    14.96s   = Training   runtime\n    0.04s    = Validation runtime\nFitting model: RandomForestMSE ...\n    -32785.3519  = Validation score   (-root_mean_squared_error)\n    0.48s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: CatBoost ...\n    Training CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n    Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n    -31739.8443  = Validation score   (-root_mean_squared_error)\n    20.53s   = Training   runtime\n    0.04s    = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -32045.9062  = Validation score   (-root_mean_squared_error)\n    0.45s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    -34966.8734  = Validation score   (-root_mean_squared_error)\n    3.5s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: XGBoost ...\n    -27851.4995  = Validation score   (-root_mean_squared_error)\n    1.47s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    -35503.6949  = Validation score   (-root_mean_squared_error)\n    5.71s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge ...\n    Training LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -32084.1712  = Validation score   (-root_mean_squared_error)\n    24.94s   = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    -26404.7032  = Validation score   (-root_mean_squared_error)\n    0.26s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 83.08s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231130_200819/\")\nWARNING: Int features without null values at train time contain null values at inference time! Imputing nulls to 0. To avoid this, pass the features as floats during fit!\nWARNING: Int features with nulls: ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageCars', 'GarageArea']\n\n\n[1000]  valid_set's rmse: 27505.1\n[2000]  valid_set's rmse: 27240.4\n[3000]  valid_set's rmse: 27201.5\n[4000]  valid_set's rmse: 27197.3\n[5000]  valid_set's rmse: 27197.2\n[1000]  valid_set's rmse: 29499.8\n[2000]  valid_set's rmse: 28896.4\n[3000]  valid_set's rmse: 28752.1\n[4000]  valid_set's rmse: 28705.7\n[5000]  valid_set's rmse: 28695.2\n[6000]  valid_set's rmse: 28693\n[7000]  valid_set's rmse: 28692.5\n[8000]  valid_set's rmse: 28692.3\n[9000]  valid_set's rmse: 28692.3\n[10000] valid_set's rmse: 28692.3\n[1000]  valid_set's rmse: 32134.9\n[2000]  valid_set's rmse: 32087.8\n[3000]  valid_set's rmse: 32084.2\n[4000]  valid_set's rmse: 32084.2\n[5000]  valid_set's rmse: 32084.2\n\n\n\n\n5. 제출\n\ndf_submission['SalePrice'] = yyhat\ndf_submission.to_csv(\"submission.csv\",index=False)\n\n\n#!kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission.csv -m \"오토글루온, 아무것도 안하고 처음 적합\"\n!rm submission.csv \n\n\n\n958/4955\n\n0.19334006054490413\n\n\n나쁘지 않은 순위..\n\n\n6. 해석 및 시각화 (HW)"
  },
  {
    "objectID": "posts/03wk-12.html#a.-데이터-정리",
    "href": "posts/03wk-12.html#a.-데이터-정리",
    "title": "03wk-12: 취업 / 로지스틱 – 추가해설",
    "section": "A. 데이터 정리",
    "text": "A. 데이터 정리\n\nX = pd.get_dummies(df_train[['toeic','gpa']])\ny = df_train[['employment']]\n\n\nX\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n...\n...\n...\n\n\n495\n280\n4.288465\n\n\n496\n310\n2.601212\n\n\n497\n225\n0.042323\n\n\n498\n320\n1.041416\n\n\n499\n375\n3.626883\n\n\n\n\n500 rows × 2 columns"
  },
  {
    "objectID": "posts/03wk-12.html#b.-predictor-starstarstar",
    "href": "posts/03wk-12.html#b.-predictor-starstarstar",
    "title": "03wk-12: 취업 / 로지스틱 – 추가해설",
    "section": "B. Predictor (\\(\\star\\star\\star\\))",
    "text": "B. Predictor (\\(\\star\\star\\star\\))\n- 여기가 중요함. \\(y\\)가 연속형이 아니라 범주형으로 이루어진 경우는 sklearn.linear_model.LogisticRegression() 이용하여 predictor를 만들 것\n\npredictr = sklearn.linear_model.LogisticRegression()"
  },
  {
    "objectID": "posts/03wk-12.html#c.-학습",
    "href": "posts/03wk-12.html#c.-학습",
    "title": "03wk-12: 취업 / 로지스틱 – 추가해설",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-12.html#d.-예측",
    "href": "posts/03wk-12.html#d.-예측",
    "title": "03wk-12: 취업 / 로지스틱 – 추가해설",
    "section": "D. 예측",
    "text": "D. 예측\n\npredictr.predict(X) \n\narray([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1])"
  },
  {
    "objectID": "posts/03wk-12.html#e.-평가",
    "href": "posts/03wk-12.html#e.-평가",
    "title": "03wk-12: 취업 / 로지스틱 – 추가해설",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.882\n\n\n\n(predictr.predict(X) == y.employment).mean()\n\n0.882\n\n\n\nplt.plot(df_train.toeic,df_train.gpa,'o')\ndf_filtered = df_train[predictr.predict(X)==1]\nplt.plot(df_filtered.toeic,df_filtered.gpa,'o') \n\n\n\n\n\n이 정도면 합리적임\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n오버피팅: 물론 정확률이 좀 아쉽긴하지만 여기서 더 맞추려고 한다면 오버핏이다. 모형을 개선하는게 의미가 없을것 같다는 판단을 해야함."
  },
  {
    "objectID": "posts/09wk-mid.html",
    "href": "posts/09wk-mid.html",
    "title": "09wk-mid (ver 1.0) – 풀이업로드",
    "section": "",
    "text": "True/False를 판단하는 문제는 답만 써도 무방함.\n“자료분석” 문제는 “kaggle style score = 50%”, “분석의 논리 = 50%” 의 배점으로 채점한다.\n“자료분석” 유형의 경우 분석의 논리가 매우 우수하거나, 창의적인 접근법으로 분석을 시도할 경우 가산점을 부여한다. (아이디어 단계에서도 가산점 부여가능) 가산점은 문항점수의 최대 100%까지 부여한다.\n“연구” 문항의 경우 세부문항을 정확하게 수행한 경우에만 100%의 점수를 부여하며 이를 어길시 부분점수를 부여하지 않는다. 연구결과의 시각화나 해석이 우수한 답안은 문항점수의 20%까지 가산점을 부여한다.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.linear_model\nimport sklearn.ensemble\nimport sklearn.preprocessing \nimport sklearn.impute\nimport sklearn.model_selection\n\n\n1. True/False (50점)\n\nhttps://www.kaggle.com/t/7911b05108f642e094ce13a77a5a4723\n\n\n링크를 shift + 클릭하면 새 창으로 열려서 문제 풀기 수월합니다.\n\n(1)-(3): 아래는 kaggle에서 titanic 데이터를 불러오는 코드이다.\ndf_train=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_test=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# !kaggle competitions download -c titanic\n# !unzip titanic.zip -d ./titanic\n# df_train = pd.read_csv('titanic/train.csv')\n# df_test = pd.read_csv('titanic/test.csv')\n# !rm titanic.zip\n# !rm -rf titanic/\n\n(1) 학습을 할 때는 df_train만을 이용하며 df_test는 전혀 사용하지 않는다.\n\nTrue\n\nTrue\n\n\n(2) df_train과 df_test의 열의 숫자는 동일하며 행의 숫자만 차이가 난다.\n\nFalse\n\nFalse\n\n\n(3) 여성생존자만 모두 생존한다고 가정하면 df_train에서의 accuracy는 78% 이상이다.\n\nTrue\n\nTrue\n\n\n(4)-(7) 아래는 모듈4에서 소개한 Alexis Cook의 분석코드이다.\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n(4) Alexis Cook의 코드에서는 사용된 설명변수는 [“Pclass”, “Sex”, “SibSp”, “Parch”] 이다.\n\nTrue\n\nTrue\n\n\n(5) Alexis Cook의 코드에서는 로지스틱모형을 이용하여 반응변수를 예측하였다.\n\nFalse\n\nFalse\n\n\n(6) model.fit(X,y)는 train data를 학습하기 위한 코드이다.\n\nTrue\n\nTrue\n\n\n(7) test data에서의 성능을 확인하기 위해서는 아래의 코드를 사용하면 된다.\noutput.score(X_test,y)\n\nFalse\n\nFalse\n\n\n(8)-(10) 아래는 모듈8에 소개된 코드중 일부이다.\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n# load data\n...\n...\n\n# analysis\npredictr = TabularPredictor(\"Survived\")\npredictr.fit(df_train,presets='best_quality')\n(df_train.Survived == predictr.predict(df_train)).mean()\n(8) 위의 코드에서 반응변수는 “Survived”로 설정하였다.\n\nTrue\n\nTrue\n\n\n(9) predictr.fit(df_train,presets='best_quality')은 train data를 학습하는 단계이다.\n\nTrue\n\nTrue\n\n\n(10) (df_train.Survived == predictr.predict(df_train)).mean()은 모형의 성능을 train data에서 검증하는 단계이다.\n\nTrue\n\nTrue\n\n\n(11)-(14) 아래는 모듈11에 제시된 데이터프레임으로 선형모형의 계수값을 표현한 것이다.\n\n(11) 여성은 남성보다 평균적으로 보험료를 65.657180 만큼 더 낸다고 해석할 수 있다.\n\nFalse # 65.657180*2 만큼 더낸다\n\nFalse\n\n\n(12) 나이가 1살 증가하면 보험료가 평균 256.856353 만큼 증가한다고 해석할 수 있다.\n\nTrue\n\nTrue\n\n\n(13) 보험료를 상대적으로 많이 내는 지역과 그렇지 않은 지역이 존재한다.\n\nTrue\n\nTrue\n\n\n(14) 흡연유무는 성별보다 보험료에 미치는 영향이 크다.\n\nTrue\n\nTrue\n\n\n(15)-(19) 아래는 모듈13에 대한 설명이다.\n(15) 4.분석-실패에서 아래와 같은 에러메시지가 나오는 이유는 train data에 결측치가 포함되어 있기 때문이다.\n\n\nTrue\n\nTrue\n\n\n(16) 결측치가 하나라도 포함된 모든 열을 제거한다면 에러메시지를 피할수는 있겠으나 오버피팅이 발생할 가능성이 있다.\n\nFalse\n\nFalse\n\n\n(17) 따라서 결측치를 하나라도 포함된 모든 열을 제거한뒤 Lasso를 쓰면 오버피팅을 피할 수 있다.\n\nFalse\n\nFalse\n\n\n(18) 결측치가 존재할 경우, 결측치가 있는 열을 제외하고 분석하는 것이 유일한 해결방법은 아니다. 예를들면 결측치를 적당한 값으로 impute한 뒤 분석할 수도 있다.\n\nTrue\n\nTrue\n\n\n(19) train data와 test data에서 결측치가 포함된 행을 모두 제거한 뒤, train data 를 이용하여 적합을 시키고 test data 에서 예측한뒤 제출하는 것도 가능한 분석방법 중 하나이다. 즉 아래와 같은 의사코드는 가능한 분석방법 중 하나이다.\ndf_train = 결측치가 포함된 행이 제거된 train data\ndf_test = 결측치가 포함된 행이 제거된 test data\nX,y = df_train 을 적당히 잘 정리한 자료의 튜플형태 \nXX = df_test 를 적당히 잘 정리한 자료\npredictr = sklearn.linear_model.LogisticRegression()\npredictr.fit(X,y)\nsubmission = predictr.predict(XX)의 값을 적당히 제출용으로 정리하여 만든 df\n\nFalse # test에도 결측치가 존재한다면, 해당방법을 쓸 수 없다. 결측치가 포함된 행을 예측할 수 없기때문\n\nFalse\n\n\n(20)-(24) 아래는 모듈19, 모듈20 에 대한 설명이다.\n(20) sklearn.preprocessing.MinMaxScaler()를 이용하여 스케일러를 생성하고 train data에 .fit_transform() method를 적용하면 항상 \\([0,1]\\) 사이의 값이 나온다.\n\nTrue\n\nTrue\n\n\n(21) sklearn.preprocessing.MinMaxScaler()를 이용하여 스케일러는 .inverse_transform() method를 가지고 있으며 이는 변환된 값을 원래로 되돌리는 역할을 한다.\n\nTrue\n\nTrue\n\n\n(22) sklearn.preprocessing.StandardScaler()는 데이터의 평균을 0, 표준편차를 1로 만드는 방식으로 조정한다.\n\nTrue\n\nTrue\n\n\n(23) 만약에 train data와 test data로 나누어진 상황에서 스케일러를 이용하여 데이터를 변환하려면 아래와 같은 순서를 따르는 것이 하나의 방법이다.\n\n적당한 scaler 생성이후 train data 를 입력으로 넣고 .fit_transform() method 사용\n1과 동일한 scaler에 test data 를 입력으로 넣고 .transform() method 사용\n\n\nTrue\n\nTrue\n\n\n(24) (23)과 동일한 상황에서 아래와 같은 변환도 해법이 될 수 있다.\n\n적당한 scaler 생성이후 train data, test data를 합쳐서 입력으로 넣고 .fit_transform() method 사용\n\n\nFalse\n\nFalse\n\n\n(25)-(26) 아래는 모듈21 에 대한 설명이다.\n(25) 밸런스게임을 통하여 많은 변수를 모을수록 train data 에 대한 score를 올릴 수 있다.\n\nTrue\n\nTrue\n\n\n(26) 이 모듈의 예시는 불필요한 변수의 추가가 오히려 학습에 방해되어 test score를 낮추는 현상이 생길 수 있음을 시사한다.\n\nTrue\n\nTrue\n\n\n(27)-(33) 아래는 모듈22 에 대한 설명이다.\n(27) 이 모듈의 데이터에서 toeic과 toeic0의 상관계수값은 매우 높게 나올 것이다.\n\nTrue\n\nTrue\n\n\n(28) 이 모듈의 데이터에서 모든변수를 넣고 적합한다면,toeic, toeic0, ... ,toeic499에 적합된 계수값의 합은 0.01 정도의 값을 가진다.\n\nTrue\n\nTrue\n\n\n(29) 이 모듈의 데이터에서 gpa와 toeic만을 남기고 나머지 변수를 제거한뒤 돌리면 test score가 향상된다.\n\nTrue\n\nTrue\n\n\n(30) 즉 이 모듈의 데이터는 변수를 제거할수록 test score가 향상된다. 예를들면 gpa를 제거하여도 test score가 향상된다.\n\nFalse\n\nFalse\n\n\n(31) 이 예제에서 train score는 모든 데이터를 사용하여 적합하였을 경우 가장 높게 나타난다.\n\nTrue # Train score는 높게나온다. Test score가 문제여서 그렇지..\n\nTrue\n\n\n(32) toeic, toeic1부터 toeic499까지의 변수들은 모두 employment_score와 높은 상관계수를 보인다.\n\nTrue\n\nTrue\n\n\n(33) 모듈22에 소개된 현상을 다중공선성이라고 말하며 이때는 계수값의 해석이 용이하지 않다.\n\nTrue\n\nTrue\n\n\n(34)-(38) 아래는 다중공선성과 Rigde, Lasso에 대한 종합적인 설명이다.\n(34) 다중공선성이 발생할 경우 올바른 변수선택으로 언제나 문제를 해결할 수 있다.\n\nFalse\n\nFalse\n\n\n(35) 다중공선성이 발생할 경우 \\({\\bf X}\\)의 correlation matrix는 단위행렬에 가깝게 나온다.\n\nFalse\n\nFalse\n\n\n(36) 다중공선성이 발생한다면 Ridge, Lasso와 같은 방법을 이용하여 해결할 수 있다.\n\nTrue\n\nTrue\n\n\n(37) 관측치의 수가 매우커진다면 (즉 데이터프레임에서 행의 숫자가 매우 커진다면) 다중공선성 문제가 심화될 가능성이 있다.\n\nFalse\n\nFalse\n\n\n(38) 다중공선성을 아래와 같은 시각화 코드로 진단할 수 있는 경우가 있다.\nsns.heatmap(df_train.corr(),annot=True)\n\nTrue\n\nTrue\n\n\n(39)-(42) 아래는 모듈23 에 대한 설명이다.\n(39) toeic, toiec0, …, toeic499 에 대응하는 계수값의 합은 \\(\\alpha\\) 와 관련없이 항상 일정하다.\n\nFalse\n\nFalse\n\n\n(40) 이 모듈에서 큰 \\(\\alpha\\)는 토익계수들의 값을 점점 비슷하게 만들어 주는 효과가 있다.\n\nTrue\n\nTrue\n\n\n(41) toeic, toiec0, …, toeic499 에 대응하는 계수값들의 분산은 \\(\\alpha\\)가 커짐에 따라 점점 작아진다.\n\nTrue\n\nTrue\n\n\n(42) \\(\\alpha\\)를 매우 잘 선택한다면 모듈23-4.True 에서 소개된 바와 같이 적절한 변수선택을 할 수 있다.\n\nFalse\n\nFalse\n\n\n(43)-(44) 아래는 모듈25 에 대한 설명이다.\n(43) \\(\\alpha\\)가 커질수록 0이 아닌 계수값의 수는 점점 증가한다.\n\nFalse\n\nFalse\n\n\n(44) Lasso를 사용하면, 토익과 유사한 변수의 수가 증가할수록 test_score가 좋아진다.\n\nFalse\n\nFalse\n\n\n(45)-(47) 아래는 모듈27 에 대한 설명이다.\n(45) 강의교재에 소개된 아래의 그림은 train score 보다 test score 가 낮은 상황이다.\n\n\nTrue\n\nTrue\n\n\n(46) 하지만 이러한 적합결과가 오버피팅이라고 보기는 어렵다.\n\nTrue\n\nTrue\n\n\n(47) 아웃라이어를 제거한다면 선형모형의 df_train, df_test에 대한 score가 모두 상승할 것이다.\n\nTrue\n\nTrue\n\n\n(48)-(50) 아래는 모듈29에 대한 설명이다.\n(48) 모듈29, 4.분석에 소개된 아래와 같은 모형은\n\n\\({\\bf X}\\): Supplement, Exercise, Supplement \\(\\times\\) Exercise\n\\({\\bf y}\\): Weight_Loss\n\n운동을 했으며, 보조제를 사용한 경우에 발생하는 추가적인 효과를 고려할 수 있다.\n\nTrue\n\nTrue\n\n\n(49) 이처럼 두 종류 이상의 범주형 범수가 존재하는 경우 범주형 범수간의 교호작용을 항상 고려해야 모형의 성능을 올릴 수 있다.\n\nFalse\n\nFalse\n\n\n(50) 실제로 교호작용이 있지만 이를 고려하지 않아서 모형의 성능이 떨어지는 경우는 언터피팅의 일종으로 볼 수 있다.\n\nTrue\n\nTrue\n\n\n\n\n2. 자료분석 (30점)\n(1) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/icesales_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/icesales_test.csv')\n\n세부지침\n\n반응변수 \\({\\bf y}\\)를 sales로 설정하고 나머지는 설명변수로 설정하라.\ndf_test에 sales에 대한 예측값을 포함하는 열을 추가하라.\n\n(풀이)\n\ndf_train.head()\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\n\n\n\n\n0\n19.4\nchoco\n64.807407\n\n\n1\n0.9\nvanilla\n25.656697\n\n\n2\n7.4\nvanilla\n34.756650\n\n\n3\n4.5\nchoco\n27.265442\n\n\n4\n21.1\nchoco\n70.606946\n\n\n\n\n\n\n\n\n# step1 \nX = pd.get_dummies(df_train[['temp','type']],drop_first=True)\nXX = pd.get_dummies(df_test[['temp','type']],drop_first=True)\ny = df_train['sales']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y) \n# step4 \nyhat = predictr.predict(XX) \ndf_test['sales'] = yhat\n\n(더 좋은 풀이)\n\nsns.scatterplot(df_train, x='temp', y='sales', hue='type')\n\n&lt;Axes: xlabel='temp', ylabel='sales'&gt;\n\n\n\n\n\n시각화를 해보고 type은 y를 예측할때 도움이 되지 않음을 파악하고 type을 제외하고 예측하면 가산점.\n(2) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_test.csv')\n\n세부지침\n\n반응변수 \\({\\bf y}\\)를 height로 설정하고 나머지는 설명변수로 설정하라.\ndf_test에 height에 대한 예측값을 포함하는 열을 추가하라.\n\n(풀이)\n\ndf_train.head()\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n0\n71.169041\nmale\n180.906857\n\n\n1\n69.204748\nmale\n178.123281\n\n\n2\n49.037293\nfemale\n165.106085\n\n\n3\n74.472874\nmale\n177.467439\n\n\n4\n74.239599\nmale\n177.439925\n\n\n\n\n\n\n\n\n# step1 \n## 데이터분리\nX = pd.get_dummies(df_train[['weight','sex']],drop_first=True)\nXX = pd.get_dummies(df_test[['weight','sex']],drop_first=True)\ny = df_train['height']\n## 결측치처리\nimputer = sklearn.impute.SimpleImputer()\nX[['weight']] = imputer.fit_transform(X[['weight']])\nXX[['weight']] = imputer.transform(XX[['weight']])\n## 교호작용고려 \nX['interaction'] = X['weight']*X['sex_male']\nXX['interaction'] = XX['weight']*XX['sex_male']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y) \n# step4 \nyhat = predictr.predict(XX) \ndf_test['height'] = predictr.predict(XX)\n\n적합결과 시각화 (train)\n\nsns.scatterplot(df_train, x='weight',y='height',hue='sex',alpha=0.3)\nsns.lineplot(df_train,x='weight',y=predictr.predict(X),hue='sex',linestyle='--')\n\n&lt;Axes: xlabel='weight', ylabel='height'&gt;\n\n\n\n\n\n\n이정도만 풀어도 만점으로 인정\n\n(더 좋은 풀이)\n남여의 몸무게 평균이 다르므로 그룹별로 몸무게를 예측하는것이 더 올바르다.\n\n# step1 \n## 데이터분리\nX = pd.get_dummies(df_train[['weight','sex']],drop_first=True)*1.0\nXX = pd.get_dummies(df_test[['weight','sex']],drop_first=True)*1.0\ny = df_train['height']\n## 결측치처리\n# -- male \nimputer_male = sklearn.impute.SimpleImputer()\nX[X.sex_male==1] = imputer_male.fit_transform(X[X.sex_male==1])\nXX[XX.sex_male==1] = imputer_male.transform(XX[XX.sex_male==1])\n# -- female \nimputer_female = sklearn.impute.SimpleImputer()\nX[X.sex_male==0] = imputer_female.fit_transform(X[X.sex_male==0])\nXX[XX.sex_male==0] = imputer_female.fit_transform(XX[XX.sex_male==0])\n## 교호작용고려 \nX['interaction'] = X['weight']*X['sex_male']\nXX['interaction'] = XX['weight']*XX['sex_male']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y) \n# step4 \nyhat = predictr.predict(XX) \ndf_test['height'] = predictr.predict(XX)\n\n적합결과 시각화 (train)\n\nsns.scatterplot(df_train, x='weight',y='height',hue='sex',alpha=0.3)\nsns.lineplot(df_train,x='weight',y=predictr.predict(X),hue='sex',linestyle='--')\n\n&lt;Axes: xlabel='weight', ylabel='height'&gt;\n\n\n\n\n\n(3) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_test.csv')\n\n세부지침\n\n반응변수 \\({\\bf y}\\)를 y로 설정하고 나머지 X1,X2,X3,X4는 설명변수로 설정하라.\ndf_test에 y에 대한 예측값을 포함하는 열을 추가하라.\n\n(풀이)\n\n# 생성된 데이터 시각화\nsns.heatmap(df_train.corr())\nfig = plt.gcf()\nfig.suptitle(\"Figure1\")\n\nfig,ax = plt.subplots(2, 2)\nax[0,0].plot(df_train.X1, df_train.y,'o')\nax[0,0].set_title(r'$(X_1,y)$')\nax[0,1].plot(df_train.X2, df_train.y,'o')\nax[0,1].set_title(r'$(X_2,y)$')\nax[1,0].plot(df_train.X3, df_train.y,'o')\nax[1,0].set_title(r'$(X_3,y)$')\nax[1,1].plot(df_train.X4, df_train.y,'o')\nax[1,1].set_title(r'$(X_4,y)$')\nfig.suptitle(\"Figure2\")\nfig.tight_layout()\n\n\nfig, ax = plt.subplots()\nax.plot(df_train.X2, df_train.X3,'o')\nax.set_title(r'Figure3: $(X_2,X_3)$')\n#fig.suptitle(\"Figure3\")\n\nText(0.5, 1.0, 'Figure3: $(X_2,X_3)$')\n\n\n\n\n\n\n\n\n\n\n\n- \\((X_1,X_4)\\)은 \\(y\\)와 관련이 없으므로 제외한다. (Figure1, Figure2)\n- \\((X_2,X_3)\\)은 모두 \\(y\\)와 관련이 있으나 둘 사이의 선형관계가 존재하므로 공선성의 관계가 있다. (Figure1, Figure3) 따라서 둘중하나를 제거하고 분석하거나, Lasso/Ridge중 하나를 쓰는것이 합리적으로 보인다. 본 풀이에서는 둘 중 하나를 제거하고 분석하도록 하겠다.\n\n# step1 \nX = df_train[['X3']]\ny = df_train['y']\nXX = df_test[['X3']]\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_test['y'] = predictr.predict(XX)\n\n- 적합결과 시각화\n\nplt.plot(df_train['y'],df_train['y'],'-',label='True')\nplt.plot(df_train['y'],predictr.predict(X),'o',label='Predictted')\nplt.title(f'score ={predictr.score(X,y):.4f}')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fadc7c7bb50&gt;\n\n\n\n\n\n(더 좋은 풀이) -- 1\n\\((X_3,y)\\) 사이에 비 선형성이 있어보이므로, 의사결정나무를 이용하여 적합하여 보자.\n\n# step1 \nX = df_train[['X3']]\ny = df_train['y']\nXX = df_test[['X3']]\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_test['y'] = predictr.predict(XX)\n\n\nplt.plot(df_train['y'],df_train['y'],'-',label='True')\nplt.plot(df_train['y'],predictr.predict(X),'o',label='Predictted')\nplt.title(f'score ={predictr.score(X,y):.4f}')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fadbb962bf0&gt;\n\n\n\n\n\n\n오버핏이 염려되긴 하지만 해봄직하다.\n\n(더 좋은 풀이) -- 2\n\\((X_3,y)\\) 사이에 비 선형성이 있어보이므로, \\((X_3)^2\\)을 모델링하여 적합하자. (배운내용X, 이러한 방법을 ploynomial regression 혹은 자기자신과의 교호작용을 고려한 모형이라고 한다.)\n\n# step1 \nX = df_train.assign(X3_square = df_train['X3']**2)[['X3','X3_square']]\ny = df_train['y']\nXX = df_test.assign(X3_square = df_test['X3']**2)[['X3','X3_square']]\n#---#\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_test['y'] = predictr.predict(XX)\n\n\nplt.plot(df_train['y'],df_train['y'],'-',label='True')\nplt.plot(df_train['y'],predictr.predict(X),'o',label='Predictted')\nplt.title(f'score ={predictr.score(X,y):.4f}')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fadb9f717b0&gt;\n\n\n\n\n\n\n사실 이게 트루모델임..\n\n\n\n3. 연구 (20점)\n아래의 코드를 이용하여 자료를 불러오라.\n\nnp.random.seed(42)\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n아래의 절차에 따라 자료를 연구하라. 결과를 해석하라.\n\n종속변수 \\({\\bf y}\\)를 employment_score로 설정하고 나머지는 설명변수로 설정하라.\nsklearn.linear_model.Ridge를 이용하여 \\(\\alpha = [10^1, 10^{1.5}, 10^2, 10^{2.5}, \\dots, 10^{9}]\\)에 대해 predictor를 생성하라.\n\\(\\alpha\\)의 변화에 따른 train score 와 test score 의 변화를 시각화하라.\n\\(\\alpha\\)에 변화에 따른 predictor가 학습한 coefficient값 (predictor.coef_) 들의 분산변화와 L2-penalty를 시각화하라.\n\n(풀이) – 생략, 강의노트 참고"
  },
  {
    "objectID": "posts/06wk-26.html",
    "href": "posts/06wk-26.html",
    "title": "06wk-26: 취업(다중공선성) / LassoCV",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n3. Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n\n4. LassoCV\n- LassoCV 클래스에서 모형을 선택해보자.\n\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.LassoCV(alphas= np.linspace(0,2,100))\n## step3\npredictr.fit(X,y)\n## step4 -- pass\n\nLassoCV(alphas=array([0.        , 0.02020202, 0.04040404, 0.06060606, 0.08080808,\n       0.1010101 , 0.12121212, 0.14141414, 0.16161616, 0.18181818,\n       0.2020202 , 0.22222222, 0.24242424, 0.26262626, 0.28282828,\n       0.3030303 , 0.32323232, 0.34343434, 0.36363636, 0.38383838,\n       0.4040404 , 0.42424242, 0.44444444, 0.46464646, 0.48484848,\n       0.50505051, 0.52525253, 0.54545455, 0.56565657, 0.58585859,\n       0.60606061...\n       1.31313131, 1.33333333, 1.35353535, 1.37373737, 1.39393939,\n       1.41414141, 1.43434343, 1.45454545, 1.47474747, 1.49494949,\n       1.51515152, 1.53535354, 1.55555556, 1.57575758, 1.5959596 ,\n       1.61616162, 1.63636364, 1.65656566, 1.67676768, 1.6969697 ,\n       1.71717172, 1.73737374, 1.75757576, 1.77777778, 1.7979798 ,\n       1.81818182, 1.83838384, 1.85858586, 1.87878788, 1.8989899 ,\n       1.91919192, 1.93939394, 1.95959596, 1.97979798, 2.        ]))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoCVLassoCV(alphas=array([0.        , 0.02020202, 0.04040404, 0.06060606, 0.08080808,\n       0.1010101 , 0.12121212, 0.14141414, 0.16161616, 0.18181818,\n       0.2020202 , 0.22222222, 0.24242424, 0.26262626, 0.28282828,\n       0.3030303 , 0.32323232, 0.34343434, 0.36363636, 0.38383838,\n       0.4040404 , 0.42424242, 0.44444444, 0.46464646, 0.48484848,\n       0.50505051, 0.52525253, 0.54545455, 0.56565657, 0.58585859,\n       0.60606061...\n       1.31313131, 1.33333333, 1.35353535, 1.37373737, 1.39393939,\n       1.41414141, 1.43434343, 1.45454545, 1.47474747, 1.49494949,\n       1.51515152, 1.53535354, 1.55555556, 1.57575758, 1.5959596 ,\n       1.61616162, 1.63636364, 1.65656566, 1.67676768, 1.6969697 ,\n       1.71717172, 1.73737374, 1.75757576, 1.77777778, 1.7979798 ,\n       1.81818182, 1.83838384, 1.85858586, 1.87878788, 1.8989899 ,\n       1.91919192, 1.93939394, 1.95959596, 1.97979798, 2.        ]))\n\n\n\npredictr.score(X,y)\n\n0.9483494983570958\n\n\n\npredictr.score(XX,yy)\n\n0.8749691641081606\n\n\n\n\n5. HW\n- 현재 predictr 에 저장된1 alpha값을 조사하고, 그것을 바탕으로 아래의 코드를 수정하여 위와 동일한 train_score, test_score가 나오도록 하라.\n1 LassoCV클래스에서 생성된 predictor수정할코드\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.Lasso(alpha=????)\n## step3\npredictr.fit(X,y)\n## step4 -- pass\n나와야할 결과\n\npredictr.score(X,y)\n\n0.9483494983570958\n\n\n\npredictr.score(XX,yy)\n\n0.8749691641081606"
  },
  {
    "objectID": "posts/06wk-23.html#a.-정확한-설명",
    "href": "posts/06wk-23.html#a.-정확한-설명",
    "title": "06wk-23: 취업(다중공선성) / Ridge – 추가해설",
    "section": "A. 정확한 설명",
    "text": "A. 정확한 설명\n- SVD를 이용하여 이론적인 계산하면 sklearn.linear_model.LinearRegression()로 적합한 결과보다 sklearn.linear_model.Ridge()로 적합한 결과를 더 좋게 만드는 \\(\\alpha\\)가 항상 존재함을 증명할 수 있음."
  },
  {
    "objectID": "posts/06wk-23.html#b.-직관적-설명-엄밀하지-않은-설명",
    "href": "posts/06wk-23.html#b.-직관적-설명-엄밀하지-않은-설명",
    "title": "06wk-23: 취업(다중공선성) / Ridge – 추가해설",
    "section": "B. 직관적 설명 (엄밀하지 않은 설명)",
    "text": "B. 직관적 설명 (엄밀하지 않은 설명)\n\nStep1: LinearRegression은 왜 망했는가?\n- 토익의 계수는 실제로 \\(\\frac{1}{100}\\)이다. 적딩히\n\ntoeic_coef + … + toeic499_coef \\(\\approx\\) 0.01 이라면\n\n대충 맞는 답이다.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.LinearRegression()\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n#---# \nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 1.0000\ntest_score: 0.1171\n\n\n\ns= pd.Series(predictr.coef_)\ns.index = X.columns\ns[1:].sum()\n\n0.010302732920632579\n\n\n- 그런데 사실 저 0.01이라는 값은 몇개의 계수만 있어도 만들 수 있다. (toeic2와 toeic3에 해당하는 계수)\n\ns['toeic2']+s['toeic3']\n\n0.010142832986793252\n\n\n- 이런논리로 치면 toeic2, toeic3에 해당하는 계수만 있다면 사실 \\(y\\)를 설명하는데 충분했고, 나머지는 불필요한 특징이 된다. (그리고 불필요한 특징은 오버피팅을 유발한다)\n\n\nStep2: Ridge의 아이디어\n- Ridge의 아이디어: toeic2, toeic3 와 같이 몇개의 변수로만 0.01이라는 수를 설명할 수 없도록 “강제”하자. 즉 몇개의 변수로만 0.01이라는 수를 설명할 수 없도록 “패널티”를 주자.\n- 패널티: 유사토익들의 계수값을 제곱한뒤 합치고(=L2-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n이러한 패널티를 줄 경우 결과적으로 0.01의 값이 “동일하게 나누어져서” 나오는 값(=\\(\\frac{1}{100}\\frac{1}{501}\\))이 계수값으로 추정된다. (왜? – 요건 정확하게 이해하는게 사실 지금은 힘듬)\n\n- 잘 적용된 Ridge의 결과를 보면 아래와 같이 계수값이 저장되어 있음.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge(alpha=5e8)\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n#---# \nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.7507\ntest_score: 0.7438\n\n\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns\ns\n\ngpa         0.000001\ntoeic       0.000019\ntoeic0      0.000018\ntoeic1      0.000018\ntoeic2      0.000019\n              ...   \ntoeic495    0.000018\ntoeic496    0.000019\ntoeic497    0.000019\ntoeic498    0.000019\ntoeic499    0.000019\nLength: 502, dtype: float64\n\n\n\n0.01/501\n\n1.9960079840319362e-05\n\n\n- 결국 Ridge를 사용하면 계수들의 값이 “동일하게 나누어지는” 효과가 나타남\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n모든 계수가 “엔빵”된다는 의미는 아니에요. 예를들어 X1=toeic, X2≈toeic*2 라면?\n\n\n- 패널티를 주는 정도? \\(\\alpha\\)로 조절함.. \\(\\alpha\\)를 크게 할수록 패널티를 많이줌"
  },
  {
    "objectID": "posts/06wk-23.html#c.-alpha에-따른-계수값-변화",
    "href": "posts/06wk-23.html#c.-alpha에-따른-계수값-변화",
    "title": "06wk-23: 취업(다중공선성) / Ridge – 추가해설",
    "section": "C. \\(\\alpha\\)에 따른 계수값 변화",
    "text": "C. \\(\\alpha\\)에 따른 계수값 변화\n- 여러개의 predictor 학습\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\nalphas = [5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8]\npredictrs = [sklearn.linear_model.Ridge(alpha=alpha) for alpha in alphas]\n## step3 \nfor predictr in predictrs:\n    predictr.fit(X,y)\n## step4 -- pass \n\n- 계수값 시각화\n\npredictrs[0].alpha\n\n500.0\n\n\n\nplt.plot(predictrs[0].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[0].alpha))\nplt.plot(predictrs[3].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[1].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f671255c340&gt;\n\n\n\n\n\n\nplt.plot(predictrs[3].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[3].alpha))\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f67129dd690&gt;\n\n\n\n\n\n\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.plot(predictrs[-1].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[-1].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f6712a4fd30&gt;\n\n\n\n\n\n- 직관: 마지막 predictor의 계수값을 살펴보자.\n\npredictrs[-1].coef_\n\narray([1.10421248e-06, 1.89938091e-05, 1.77768343e-05, 1.82118332e-05,\n       1.90895673e-05, 1.87128138e-05, 1.90343037e-05, 1.82483251e-05,\n       1.90405022e-05, 1.85802628e-05, 1.90021086e-05, 1.88952130e-05,\n       1.96003229e-05, 1.89154663e-05, 1.86638217e-05, 1.92666606e-05,\n       1.97107043e-05, 1.92214868e-05, 1.92961317e-05, 1.93321368e-05,\n       1.92194541e-05, 1.85663279e-05, 1.86805137e-05, 1.81649873e-05,\n       1.78656367e-05, 1.83171419e-05, 1.94428947e-05, 1.89710925e-05,\n       2.00598946e-05, 1.88384883e-05, 1.98903125e-05, 1.81113551e-05,\n       1.85043847e-05, 1.84424971e-05, 1.91508275e-05, 1.97427867e-05,\n       1.93598061e-05, 1.98264264e-05, 1.89934042e-05, 1.84770850e-05,\n       1.83617634e-05, 1.79346774e-05, 1.84943159e-05, 1.89803006e-05,\n       1.78633749e-05, 1.80073666e-05, 1.85664525e-05, 1.97390143e-05,\n       1.86574281e-05, 1.92233226e-05, 1.91281904e-05, 1.85617627e-05,\n       1.83939489e-05, 1.84309427e-05, 1.88142167e-05, 1.84159665e-05,\n       1.94078579e-05, 1.84515402e-05, 1.88107980e-05, 1.85889903e-05,\n       1.89357356e-05, 1.88750847e-05, 1.92107444e-05, 1.81799279e-05,\n       1.92122152e-05, 1.97863670e-05, 1.89851436e-05, 1.88974919e-05,\n       1.88566578e-05, 1.95841935e-05, 1.86398380e-05, 1.95801159e-05,\n       1.87550098e-05, 1.87392625e-05, 1.87462595e-05, 1.96056001e-05,\n       1.80626630e-05, 1.88237701e-05, 1.83108446e-05, 1.88087164e-05,\n       1.84723703e-05, 1.84767748e-05, 1.89267252e-05, 1.87604297e-05,\n       1.86945591e-05, 1.92924236e-05, 1.77843453e-05, 1.85415541e-05,\n       1.91448999e-05, 1.98281375e-05, 1.97994651e-05, 1.86653004e-05,\n       1.87298830e-05, 1.87474975e-05, 1.90018315e-05, 1.92043808e-05,\n       1.88941675e-05, 1.81646176e-05, 1.91508494e-05, 2.04322537e-05,\n       1.92111546e-05, 1.93061022e-05, 1.92088349e-05, 1.80206353e-05,\n       1.89399818e-05, 1.96895533e-05, 1.94410839e-05, 1.92051217e-05,\n       1.84961416e-05, 1.89785667e-05, 1.92235780e-05, 1.86729143e-05,\n       1.88439733e-05, 1.76776615e-05, 1.87493841e-05, 1.86986837e-05,\n       1.81917859e-05, 1.94657238e-05, 1.82063420e-05, 1.78143049e-05,\n       1.88432683e-05, 1.90674860e-05, 1.86411824e-05, 1.93286721e-05,\n       1.75163829e-05, 1.86852659e-05, 2.02343956e-05, 1.82025623e-05,\n       1.89153395e-05, 1.98862774e-05, 1.94775038e-05, 1.90665531e-05,\n       1.94170642e-05, 1.88227118e-05, 1.88792179e-05, 1.89712787e-05,\n       1.87855482e-05, 1.87895464e-05, 2.00798925e-05, 1.97167119e-05,\n       1.91644137e-05, 1.90990710e-05, 1.85836048e-05, 1.82346595e-05,\n       1.85731253e-05, 1.84871242e-05, 1.90728256e-05, 1.90277156e-05,\n       1.93085319e-05, 1.91719254e-05, 1.80097271e-05, 1.82517485e-05,\n       1.90904218e-05, 1.85232604e-05, 1.88184612e-05, 1.84002976e-05,\n       2.00337440e-05, 1.86478638e-05, 1.93507546e-05, 1.85547358e-05,\n       1.97154574e-05, 1.91189346e-05, 1.93320777e-05, 1.85313268e-05,\n       1.91085306e-05, 1.88406812e-05, 1.87444892e-05, 1.96637559e-05,\n       1.83552699e-05, 1.80759243e-05, 1.94662845e-05, 1.93761303e-05,\n       1.98339288e-05, 1.87139235e-05, 1.91131387e-05, 1.85801855e-05,\n       1.91544816e-05, 1.98413649e-05, 1.84027849e-05, 1.81842651e-05,\n       1.95888229e-05, 1.80738476e-05, 1.92457286e-05, 1.91474170e-05,\n       1.88737956e-05, 1.78029998e-05, 1.97734483e-05, 1.92409710e-05,\n       1.97346045e-05, 1.99425451e-05, 1.89157923e-05, 1.82538525e-05,\n       1.87475300e-05, 1.79663692e-05, 1.94360535e-05, 1.93333725e-05,\n       1.81368431e-05, 1.91860664e-05, 2.03648683e-05, 1.92870391e-05,\n       1.92561212e-05, 1.92408929e-05, 1.77556464e-05, 1.89317813e-05,\n       1.95230859e-05, 1.91845519e-05, 1.88923023e-05, 1.88368476e-05,\n       1.89013580e-05, 1.82113056e-05, 1.86295402e-05, 1.92236940e-05,\n       1.80025543e-05, 1.92322271e-05, 1.80917953e-05, 1.87188051e-05,\n       1.93772655e-05, 1.87894009e-05, 1.86773984e-05, 1.96830961e-05,\n       1.94593808e-05, 1.99377297e-05, 1.85707832e-05, 1.88667594e-05,\n       1.85589760e-05, 1.98498326e-05, 1.88878514e-05, 1.90686529e-05,\n       1.86868639e-05, 1.90576790e-05, 1.95494214e-05, 1.86567117e-05,\n       1.85992014e-05, 1.77199587e-05, 1.82193592e-05, 1.90965903e-05,\n       1.96016869e-05, 1.88116657e-05, 1.81131528e-05, 1.85436209e-05,\n       1.92951259e-05, 1.92495993e-05, 1.84570073e-05, 1.94529446e-05,\n       1.92760629e-05, 1.92236816e-05, 1.85750512e-05, 1.95451343e-05,\n       1.82912208e-05, 1.88851896e-05, 1.86295173e-05, 1.84150640e-05,\n       1.95101106e-05, 1.98423439e-05, 1.88687440e-05, 1.91657943e-05,\n       1.89387389e-05, 1.89907539e-05, 1.90653825e-05, 1.80854343e-05,\n       1.86906336e-05, 1.85793308e-05, 1.84992786e-05, 1.93964742e-05,\n       1.83344151e-05, 1.89611068e-05, 1.91457644e-05, 1.88755070e-05,\n       1.98511526e-05, 1.93068196e-05, 1.93316489e-05, 1.89507435e-05,\n       1.89083004e-05, 1.91358509e-05, 1.87803906e-05, 1.78160168e-05,\n       1.94603877e-05, 2.02569965e-05, 1.87423291e-05, 1.94609617e-05,\n       1.91292677e-05, 1.85958571e-05, 1.88629266e-05, 1.90600256e-05,\n       1.82221314e-05, 1.95093258e-05, 1.89176339e-05, 2.00028045e-05,\n       1.94052035e-05, 1.86744967e-05, 1.89125601e-05, 2.02089363e-05,\n       1.80569192e-05, 2.02141130e-05, 1.93147541e-05, 1.89011113e-05,\n       1.93335891e-05, 1.96767360e-05, 1.90364715e-05, 1.94635849e-05,\n       1.90397143e-05, 1.91973258e-05, 1.85857694e-05, 1.91487106e-05,\n       1.92897509e-05, 1.99589223e-05, 1.89690091e-05, 1.90089893e-05,\n       1.80391078e-05, 1.89867708e-05, 1.91430968e-05, 1.92719424e-05,\n       1.95648244e-05, 1.85975115e-05, 1.92077870e-05, 1.84415844e-05,\n       1.88715614e-05, 1.85970322e-05, 1.93261490e-05, 1.86726361e-05,\n       1.97716032e-05, 1.92749150e-05, 2.00954709e-05, 1.90876286e-05,\n       1.89190693e-05, 1.98831620e-05, 1.91612367e-05, 1.86269524e-05,\n       1.89155394e-05, 1.89824518e-05, 1.98347756e-05, 1.86788886e-05,\n       1.83508292e-05, 1.85069060e-05, 1.86909372e-05, 1.85978543e-05,\n       1.88150510e-05, 1.89755849e-05, 1.90099289e-05, 1.90515657e-05,\n       1.93189513e-05, 1.82151178e-05, 1.78471089e-05, 1.91763316e-05,\n       1.84903926e-05, 1.92863572e-05, 1.90497739e-05, 1.87657428e-05,\n       1.87801680e-05, 1.85137448e-05, 1.91226761e-05, 1.94084785e-05,\n       1.81950620e-05, 1.81823646e-05, 1.87513814e-05, 1.97922951e-05,\n       1.87200102e-05, 1.98409879e-05, 1.85874173e-05, 1.90513332e-05,\n       1.85234477e-05, 1.81902197e-05, 1.76367508e-05, 1.90389194e-05,\n       1.85299355e-05, 1.95358518e-05, 1.81772601e-05, 1.93671350e-05,\n       1.91528856e-05, 1.91322975e-05, 1.85830738e-05, 1.85626882e-05,\n       1.86250726e-05, 1.84514809e-05, 1.86800234e-05, 1.89256964e-05,\n       1.90280385e-05, 1.88870537e-05, 1.86929332e-05, 1.95167742e-05,\n       1.86377119e-05, 1.93693632e-05, 1.94429807e-05, 1.90730542e-05,\n       1.86276638e-05, 1.86225787e-05, 1.87333026e-05, 1.94293224e-05,\n       1.87174307e-05, 1.93106731e-05, 1.91898445e-05, 1.91446507e-05,\n       1.83627209e-05, 1.85185991e-05, 1.90680366e-05, 1.88180597e-05,\n       1.86586581e-05, 1.80051184e-05, 1.83329730e-05, 1.82088945e-05,\n       1.87516598e-05, 1.82744310e-05, 1.90219092e-05, 1.89098591e-05,\n       1.89001214e-05, 1.90959896e-05, 1.77157866e-05, 1.91760361e-05,\n       1.80496598e-05, 1.85629242e-05, 1.93527162e-05, 1.85046434e-05,\n       1.97977476e-05, 1.82757747e-05, 1.92849021e-05, 1.86829990e-05,\n       1.86752898e-05, 1.95540241e-05, 1.92250030e-05, 1.84817730e-05,\n       1.94636774e-05, 1.86057300e-05, 1.90096458e-05, 1.91037821e-05,\n       1.98095086e-05, 1.92558748e-05, 1.94175627e-05, 1.86155519e-05,\n       1.91386204e-05, 1.89659072e-05, 1.89507918e-05, 1.88868989e-05,\n       1.91223138e-05, 1.81488441e-05, 1.95885497e-05, 1.87850789e-05,\n       1.90457546e-05, 1.96549561e-05, 1.86983597e-05, 1.89788151e-05,\n       1.98384237e-05, 1.99479277e-05, 1.91275095e-05, 1.89970341e-05,\n       1.85749782e-05, 1.91683345e-05, 1.91850806e-05, 1.97386011e-05,\n       1.93320833e-05, 1.92560345e-05, 1.85426153e-05, 1.85185853e-05,\n       1.85764448e-05, 1.94279426e-05, 1.97685699e-05, 1.91733090e-05,\n       1.84972022e-05, 1.89924907e-05, 1.83467563e-05, 1.95149016e-05,\n       1.84410610e-05, 1.86536281e-05, 1.88181888e-05, 1.85487807e-05,\n       1.88565643e-05, 1.89056942e-05, 1.95082352e-05, 1.91711709e-05,\n       1.91422027e-05, 1.91363321e-05, 1.89114818e-05, 1.85390554e-05,\n       1.92949067e-05, 1.88019353e-05, 1.85332879e-05, 1.86699430e-05,\n       1.96934870e-05, 2.01293426e-05, 1.81411289e-05, 1.86806981e-05,\n       1.90987154e-05, 1.85866377e-05, 1.96875267e-05, 1.88785203e-05,\n       1.94435510e-05, 1.85812461e-05, 1.97178935e-05, 1.90067232e-05,\n       2.02306858e-05, 1.86213361e-05, 1.94255182e-05, 1.86417320e-05,\n       1.95689564e-05, 1.97728792e-05, 1.94352125e-05, 1.93768903e-05,\n       1.90643113e-05, 1.79709383e-05, 1.90573271e-05, 1.85638225e-05,\n       1.91337229e-05, 1.86437625e-05])\n\n\n\n불필요한 변수가 나올 수 없는 구조가 되어버렸음 (한 두개로 0.01을 만들 수 없음)\n모든 변수는 대략 2e-5(\\(\\approx \\frac{1}{100}\\frac{1}{501}\\))정도 만큼 똑같이 중요하다고 생각된다.\n고급: 살짝 1/(100*501)보다 전체적으로 값이 작아보이는데, 이는 기분탓이 아님 (Ridge 특징)\n\n\n1/100*1/501\n\n1.9960079840319362e-05"
  },
  {
    "objectID": "posts/06wk-23.html#d.-alpha에-따른-실험내용-정리",
    "href": "posts/06wk-23.html#d.-alpha에-따른-실험내용-정리",
    "title": "06wk-23: 취업(다중공선성) / Ridge – 추가해설",
    "section": "D. \\(\\alpha\\)에 따른 실험내용 정리",
    "text": "D. \\(\\alpha\\)에 따른 실험내용 정리\n- 예비개념: L2-penalty는 그냥 대충 분산같은것..\n\nx = np.random.randn(5)\nl2_penalty = (x**2).sum()\nl2_penalty, 5*(x.var()+(x.mean()**2))\n\n(7.084638326878741, 7.084638326878739)\n\n\n- \\(\\alpha\\)가 커질수록 생기는 일\n\n크게 느낀것: 계수들의 값이 점점 비슷해짐 –&gt; 계수들의 값들을 모아서 분산을 구하면 작아진다는 의미 –&gt; L2-penalty 가 작아진다는 의미\n미묘하게 느껴지는 점: toeic, 그리고 toeic0~toeic499 까지의 계수총합은 0.01이 되어야 하는데, 그 총합이 미묘하게 작어지는 느낌.\n\n\nfor predictr in predictrs: \n    print(\n        f'alpha={predictr.alpha:.2e}\\t'\n        f'l2_penalty={((predictr.coef_)**2).sum():.6f}\\t'\n        f'sum(toeic_coefs)={((predictr.coef_[1:])).sum():.4f}\\t'\n        f'test_score={predictr.score(XX,yy):.4f}'\n    )\n\nalpha=5.00e+02  l2_penalty=0.046715 sum(toeic_coefs)=0.0103 test_score=0.2026\nalpha=5.00e+03  l2_penalty=0.021683 sum(toeic_coefs)=0.0102 test_score=0.4638\nalpha=5.00e+04  l2_penalty=0.003263 sum(toeic_coefs)=0.0099 test_score=0.6889\nalpha=5.00e+05  l2_penalty=0.000109 sum(toeic_coefs)=0.0099 test_score=0.7407\nalpha=5.00e+06  l2_penalty=0.000002 sum(toeic_coefs)=0.0099 test_score=0.7447\nalpha=5.00e+07  l2_penalty=0.000000 sum(toeic_coefs)=0.0098 test_score=0.7450\nalpha=5.00e+08  l2_penalty=0.000000 sum(toeic_coefs)=0.0095 test_score=0.7438\n\n\n- L2-penalty의 느낌은 대충 아래와 같이 분산으로 이해해도 무방\n\nfor predictr in predictrs: \n    print(\n        f'alpha={predictr.alpha:.2e}\\t'\n        f'var(coefs)={(predictr.coef_).var()*501:.6f}\\t'\n        f'sum(toeic_coefs)={((predictr.coef_[1:])).sum():.4f}\\t'\n        f'test_score={predictr.score(XX,yy):.4f}'\n    )\n\nalpha=5.00e+02  var(coefs)=0.046618 sum(toeic_coefs)=0.0103 test_score=0.2026\nalpha=5.00e+03  var(coefs)=0.021638 sum(toeic_coefs)=0.0102 test_score=0.4638\nalpha=5.00e+04  var(coefs)=0.003256 sum(toeic_coefs)=0.0099 test_score=0.6889\nalpha=5.00e+05  var(coefs)=0.000109 sum(toeic_coefs)=0.0099 test_score=0.7407\nalpha=5.00e+06  var(coefs)=0.000001 sum(toeic_coefs)=0.0099 test_score=0.7447\nalpha=5.00e+07  var(coefs)=0.000000 sum(toeic_coefs)=0.0098 test_score=0.7450\nalpha=5.00e+08  var(coefs)=0.000000 sum(toeic_coefs)=0.0095 test_score=0.7438"
  },
  {
    "objectID": "posts/06wk-23.html#e.-alpha가-크다고-무조건-좋은건-아니다.",
    "href": "posts/06wk-23.html#e.-alpha가-크다고-무조건-좋은건-아니다.",
    "title": "06wk-23: 취업(다중공선성) / Ridge – 추가해설",
    "section": "E. \\(\\alpha\\)가 크다고 무조건 좋은건 아니다.",
    "text": "E. \\(\\alpha\\)가 크다고 무조건 좋은건 아니다.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge(alpha=1e12)\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n\nRidge(alpha=1000000000000.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=1000000000000.0)\n\n\n\nprint(f'train_score={predictr.score(X,y):.4f}')\nprint(f'test_score={predictr.score(XX,yy):.4f}')\n\ntrain_score=0.0191\ntest_score=0.0140\n\n\n\npredictr.coef_[1:].sum() # 이 값이 0.01이어야 하는데, 많이 작아짐\n\n0.00012585319204891574"
  },
  {
    "objectID": "posts/07wk-29.html",
    "href": "posts/07wk-29.html",
    "title": "07wk-29: 체중감량(교호작용) / 회귀분석 – 추가해설",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\nimport sklearn.model_selection\n\n\n\n3. Data\n\n# n = 10000\n# Supplement = np.random.choice([True, False], n)\n# Exercise = np.random.choice([False, True], n)\n# Weight_Loss = np.where(\n#     (~Supplement & (~Exercise)),\n#     np.random.normal(loc=0, scale=1, size=n),  \n#     np.where(\n#         (Supplement & (Exercise)),\n#         np.random.normal(loc=15.00, scale=1, size=n),\n#         np.where(\n#             (~Supplement & (Exercise)),\n#             np.random.normal(loc=5.00, scale=1, size=n),\n#             np.random.normal(loc=0.5, scale=1, size=n)\n#         )\n#     )\n# )\n# df = pd.DataFrame({\n#     'Supplement': Supplement,\n#     'Exercise': Exercise,\n#     'Weight_Loss': Weight_Loss\n# })\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/weightloss.csv')\n\n\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n\n\n1\nTrue\nFalse\n1.604542\n\n\n2\nTrue\nTrue\n13.824148\n\n\n3\nTrue\nTrue\n13.004505\n\n\n4\nTrue\nTrue\n13.701128\n\n\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n\n\n9996\nFalse\nFalse\n-0.217816\n\n\n9997\nFalse\nTrue\n4.072701\n\n\n9998\nTrue\nFalse\n-0.253796\n\n\n9999\nFalse\nFalse\n-1.399092\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n- 운동과 체중감량보조제를 병행하면 시너지가 나는 것 같음\n\n\n4. 분석\n- 분석1: 모형을 아래와 같이 본다. – 언더피팅\n\n\\({\\bf X}\\): Supplement, Exercise\n\\({\\bf y}\\): Weight_Loss\n\n\n# step1\nX = df_train[['Supplement','Exercise']]\ny = df_train['Weight_Loss']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n#---#\nprint(f'train score = {predictr.score(X,y):.4f}')\n\ntrain score = 0.8208\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n-2.373106\n7.374557\n\n\nTrue\n2.845934\n12.593598\n\n\n\n\n\n\n\n\n운동을 하면 10키로 감량효과가 있다고 추정하고 있음.\n보충제를 먹으면 5키로 감량효과가 있다고 추정하고 있음.\n대충 (10,5)의 숫자를 바꿔가면서 적합해봤는데 이게 최선이라는 의미임\n\n- 분석2: 모형을 아래와 같이 본다. – 딱 맞아요\n\n\\({\\bf X}\\): Supplement, Exercise, Supplement \\(\\times\\) Exercise\n\\({\\bf y}\\): Weight_Loss\n\n\nNote: 기본적인 운동의 효과 및 보조제의 효과는 각각 Supplement, Exercise 로 적합하고 운동과 보조제의 시너지는 Supplement\\(\\times\\)Exercise 로 적합한다.\n\n\n# step1 \nX = df_train.eval('Interaction = Supplement * Exercise')[['Supplement','Exercise','Interaction']]\ny = df_train['Weight_Loss']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n#---#\nprint(f'train score = {predictr.score(X,y):.4f}')\n\ntrain score = 0.9728\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\n운동의 효과는 5정도 감량효과가 있다고 추정함.\n보충제를 먹으면 0.5키로 감량효과가 있다고 추정함.\n다만 운동을 하면서 보충제를 함께 먹을 경우 발생하는 추가적인 시너지효과가 9.5정도라고 추정하는 것임.\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n만약에 운동을 안하고, 약만먹을 경우 부작용이 생긴다면? (이것도 교호작용의 일종)\n\n이러한 경우 위의 모형으로 단순적합하기 어렵다. (위의 모형은 “운동O/약O”인 case에서 발생하는 효과만 고려하도록 설계되어있음)\n따라서 이럴 경우 차라리 (운동,약)을 결합하여 새로운 범주형 변수를 만들고 그 변수에서 원핫인코딩을 하는게 좋다. (마지막 더미변수는 제외하는고 좋지만, 파이썬에서는 제외하지 않아도 큰일나는건 아님)\n사실 (운동,약)을 결합하여 모든 새로운 범주를 만들고 이중 필요없는 범주를 또 다시 제거해야하는 과정도 분석에 포함되어야 한다. (\\(p\\)-value를 보면서 뺼수도 있고 다른 방법을 쓸 수도 있고..)\n그런데 범주형 변수가 3개라면? –&gt; 솔직히 이것저것 생각하기 귀찮으니까 이럴떄는 “트리모형”계열을 사용하는게 속편하다. (아니면 교호작용이 없길 기도하거나)"
  },
  {
    "objectID": "posts/10wk-36.html",
    "href": "posts/10wk-36.html",
    "title": "10wk-36: 애니메이션",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation\nimport IPython\n\n\n\n3. FuncAnimation\n- 모티브\n\nk = 6\nx = np.linspace(0,10,100)\ny = np.sin(0.1*k*x) \nplt.plot(x,y)\n\n\n\n\n\\(k=1,2,3,\\dots\\)로 바꾸면서 변화하는 그림을 연속으로 출력되게 하여 애니메이션으로 보고 싶다. 따라서\n\n하나의 고정된 그림을 정의하고\n그림안의 내용물을 frame에 따라 바꾸는 동작을 정의하여\n\n이들을 결합하는 전략을 생각해보자.\n- 위의코드는 아래와 같다.\n\nk = 4\nx = np.linspace(0,10,100)\ny = np.sin(0.1*k*x) \nfig = plt.figure() # 하나의 고정된 그림을 정의하는 코드 \nax = fig.gca() \nax.plot(x,y) # 고정된 그림에서 내용물을 frame에 따라서 바꾸는 동작을 정의하는 코드 \n\n\n\n\n- 애니메이션\n\nfig = plt.figure() # 하나의 고정된 그림을 정의하는 코드 \n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca() \n    ax.clear()\n    x = np.linspace(0,10,100)\n    y = np.sin(0.1*frame*x) \n    ax.plot(x,y) # 고정된 그림에서 내용물을 frame에 따라서 바꾸는 동작을 정의하는 코드 \n\n\nani = matplotlib.animation.FuncAnimation(\n    fig, # 하나의 고정된 그림\n    func, # 고정된 그림에서 내용물을 frame에 따라서 바꾸는 동작을 함수로 정의하고, 그 함수를 넣음\n    frames=50 \n) \n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n4. HW\n\nx = np.linspace(0,10,100)\ny = np.sin(5*x) \nplt.plot(x,y)\n\n\n\n\n위의 그림을 이용하여 애니메이션을 만들어라. 이때 frame이 짝수일경우는 color=’C0’로 frame이 홀수일 경우는 color=’C1’으로 그린 그림이 나오도록 애니메이션을 구성하라."
  },
  {
    "objectID": "posts/06wk-22.html#a.-분석절차",
    "href": "posts/06wk-22.html#a.-분석절차",
    "title": "06wk-22: 취업(다중공선성) / 다중공선성의 개념 – 추가해설",
    "section": "A. 분석절차",
    "text": "A. 분석절차\n- step1: 데이터정리\n\ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n\nX = df_train.loc[:,'gpa':'toeic499']\nXX = df_test.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n\n- step2: predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression()\n\n- step3: 학습\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- step4: 예측: 생략"
  },
  {
    "objectID": "posts/06wk-22.html#b.-계수해석-및-평가",
    "href": "posts/06wk-22.html#b.-계수해석-및-평가",
    "title": "06wk-22: 취업(다중공선성) / 다중공선성의 개념 – 추가해설",
    "section": "B. 계수해석 및 평가",
    "text": "B. 계수해석 및 평가\n- 계수해석\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns \ns\n\ngpa         0.035315\ntoeic       0.002680\ntoeic0      0.009333\ntoeic1     -0.017511\ntoeic2      0.005205\n              ...   \ntoeic495   -0.012811\ntoeic496   -0.007390\ntoeic497   -0.007487\ntoeic498    0.003379\ntoeic499   -0.002187\nLength: 502, dtype: float64\n\n\n\n실제계수값은 토익*1/100, GPA*1.0, 나머지 toeic0~toeic499 는 모두 계수값이 0임\n그러나 학습된 계수값은 그렇지 않음.\n\n- 평가: train/test score 계산\n\npredictr.score(X,y)\n\n1.0\n\n\n\ntrain 에서는 잘맞음 (퍼펙트) – 모의고사는 기가막히게 잘품\n\n\npredictr.score(XX,yy)\n\n0.1170507821249005\n\n\n\ntest 에서는 잘 맞지 않음 – 수능을 보면 망한다."
  },
  {
    "objectID": "posts/06wk-22.html#a.-toeic과-gpa가-유의미한-변수라는걸-눈치챘다면-오라클..",
    "href": "posts/06wk-22.html#a.-toeic과-gpa가-유의미한-변수라는걸-눈치챘다면-오라클..",
    "title": "06wk-22: 취업(다중공선성) / 다중공선성의 개념 – 추가해설",
    "section": "A. toeic과 gpa가 유의미한 변수라는걸 눈치챘다면? (오라클..)",
    "text": "A. toeic과 gpa가 유의미한 변수라는걸 눈치챘다면? (오라클..)\n- 분석절차수행\n\n## step1: 데이터의 정리  \ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic']\nXX = df_test.loc[:,'gpa':'toeic']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n## step2: predictor 생성 \npredictr = sklearn.linear_model.LinearRegression()\n## step3: predictor.fit을 이용하여 predictor 학습\npredictr.fit(X,y)\n## step4: predictor.predict을 이용하여 예측 -- pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 계수해석\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns\ns\n\ngpa      0.972163\ntoeic    0.010063\ndtype: float64\n\n\n\n실제계수값인 GPA*1.0, 토익*1/100이 잘 추정됨\n\n- score도 괜찮음\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.9133\ntest_score: 0.9127"
  },
  {
    "objectID": "posts/06wk-22.html#b.-하다못해-toeic0과-gpa로-적합했다면",
    "href": "posts/06wk-22.html#b.-하다못해-toeic0과-gpa로-적합했다면",
    "title": "06wk-22: 취업(다중공선성) / 다중공선성의 개념 – 추가해설",
    "section": "B. 하다못해 toeic0과 gpa로 적합했다면?",
    "text": "B. 하다못해 toeic0과 gpa로 적합했다면?\n- 분석절차\n\n## step1: 데이터의 정리  \ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,['gpa','toeic0']]\nXX = df_test.loc[:,['gpa','toeic0']]\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n## step2: predictor 생성 \npredictr = sklearn.linear_model.LinearRegression()\n## step3: predictor.fit을 이용하여 predictor 학습\npredictr.fit(X,y)\n## step4: predictor.predict을 이용하여 예측 -- pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 계수해석\n\npredictr.coef_\n\narray([0.98130228, 0.0101011 ])\n\n\n\n합리적으로 추정된것 같음\n\n- 평가\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.9121\ntest_score: 0.9115\n\n\n\n오라클 만큼은 아니지만 이정도만 되어도 합리적임"
  },
  {
    "objectID": "posts/07wk-31.html",
    "href": "posts/07wk-31.html",
    "title": "07wk-31: 체중감량(교호작용) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport sklearn.linear_model\nimport sklearn.tree\n\n\n\n3. Data\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/weightloss.csv')\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n\n\n1\nTrue\nFalse\n1.604542\n\n\n2\nTrue\nTrue\n13.824148\n\n\n3\nTrue\nTrue\n13.004505\n\n\n4\nTrue\nTrue\n13.701128\n\n\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n\n\n9996\nFalse\nFalse\n-0.217816\n\n\n9997\nFalse\nTrue\n4.072701\n\n\n9998\nTrue\nFalse\n-0.253796\n\n\n9999\nFalse\nFalse\n-1.399092\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n- 운동과 체중감량보조제를 병행하면 시너지가 나는 것 같음\n\n\n4. 분석\n- 분석1: 선형회귀 (교호작용 고려 X)\n\n# step 1\nX,y = df_train[['Supplement','Exercise']], df_train['Weight_Loss']\n# step 2 \npredictr = sklearn.linear_model.LinearRegression()\n# step 3 \npredictr.fit(X,y)\n# step 4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n-2.373106\n7.374557\n\n\nTrue\n2.845934\n12.593598\n\n\n\n\n\n\n\n- 분석2: 의사결정나무\n\n# step 1\nX,y = df_train[['Supplement','Exercise']], df_train['Weight_Loss']\n# step 2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step 3 \npredictr.fit(X,y)\n# step 4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n\n\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\nWeight_Loss_hat\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n0.021673\n\n\n1\nTrue\nFalse\n1.604542\n0.497573\n\n\n2\nTrue\nTrue\n13.824148\n14.966363\n\n\n3\nTrue\nTrue\n13.004505\n14.966363\n\n\n4\nTrue\nTrue\n13.701128\n14.966363\n\n\n...\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n0.497573\n\n\n9996\nFalse\nFalse\n-0.217816\n0.021673\n\n\n9997\nFalse\nTrue\n4.072701\n4.991314\n\n\n9998\nTrue\nFalse\n-0.253796\n0.497573\n\n\n9999\nFalse\nFalse\n-1.399092\n0.021673\n\n\n\n\n10000 rows × 4 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363"
  },
  {
    "objectID": "posts/13wk-52.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-52.html#a.-y의-분포-xy의-관계-시각화",
    "title": "13wk-52: 취업(오버피팅) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train, \n    label='employment',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nunique\ntop\nfreq\ndtypes\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nemployment\n1500\n2\nYes\n753\nobject\n\n\nobject\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for employment &gt;= 0.5\n\n\n\n\n\nFeature interaction between toiec/employment in train_data"
  },
  {
    "objectID": "posts/13wk-52.html#target-variable-analysis",
    "href": "posts/13wk-52.html#target-variable-analysis",
    "title": "13wk-52: 취업(오버피팅) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-52.html#b.-중요한-설명변수",
    "href": "posts/13wk-52.html#b.-중요한-설명변수",
    "title": "13wk-52: 취업(오버피팅) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data = df_train, \n    label='employment',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231201_083641/\"\n\n\nModel Prediction for employment\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n0.811111\n0.809524\n0.000758\n0.001052\n0.161432\n0.000758\n0.001052\n0.161432\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\ngpa\n0.184444\n0.009813\n9.578572e-07\n5\n0.204650\n0.164239\n\n\ntoiec\n0.180444\n0.011590\n2.031000e-06\n5\n0.204308\n0.156581\n\n\nbalance5\n0.004889\n0.004554\n3.716217e-02\n5\n0.014266\n-0.004488\n\n\nbalance7\n0.004444\n0.005879\n8.311638e-02\n5\n0.016550\n-0.007661\n\n\nbalance3\n0.002667\n0.004818\n1.417555e-01\n5\n0.012586\n-0.007253\n\n\nbalance8\n0.002222\n0.005666\n2.149867e-01\n5\n0.013888\n-0.009443\n\n\nbalance2\n0.001778\n0.004275\n2.025117e-01\n5\n0.010579\n-0.007024\n\n\nbalance4\n0.000889\n0.003718\n3.106541e-01\n5\n0.008545\n-0.006768\n\n\nbalance6\n0.000889\n0.004037\n3.241306e-01\n5\n0.009201\n-0.007423\n\n\nbalance1\n-0.000444\n0.004818\n5.766784e-01\n5\n0.009475\n-0.010364\n\n\nbalance9\n-0.004000\n0.003651\n9.647580e-01\n5\n0.003518\n-0.011518\n\n\nbalance0\n-0.007111\n0.005305\n9.799713e-01\n5\n0.003813\n-0.018035\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\ntoiec\ngpa\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\nemployment\nNo\nYes\nerror\n\n\n\n\n160\n440\n3.12\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nNo\n0.403186\n0.596814\n0.193628\n\n\n223\n870\n3.02\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nNo\n0.405564\n0.594436\n0.188872\n\n\n232\n190\n2.49\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\nYes\n0.594199\n0.405801\n0.188399\n\n\n722\n315\n4.03\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\nNo\n0.406428\n0.593572\n0.187144\n\n\n4187\n610\n3.91\nTrue\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\nNo\n0.413631\n0.586369\n0.172738\n\n\n15\n415\n2.47\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nYes\n0.584466\n0.415534\n0.168932\n\n\n750\n305\n2.63\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nYes\n0.580264\n0.419736\n0.160529\n\n\n4276\n555\n3.58\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nNo\n0.421053\n0.578947\n0.157895\n\n\n3863\n345\n2.43\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\nYes\n0.575093\n0.424907\n0.150185\n\n\n823\n670\n3.13\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nNo\n0.427126\n0.572874\n0.145748\n\n\n\n\n\n\n\nRows with the least distance vs other class\n\n\nRows in this category are the closest to the decision boundary vs the other class and are good candidates for additional labeling\n\n\n\n\n\n\n\n\n\ntoiec\ngpa\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\nemployment\nNo\nYes\nerror\n\n\n\n\n2930\n665\n2.73\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nYes\n0.500071\n0.499929\n0.000142\n\n\n3251\n100\n4.08\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nNo\n0.499858\n0.500142\n0.000283\n\n\n3769\n990\n2.11\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nYes\n0.501805\n0.498195\n0.003611\n\n\n2485\n105\n3.70\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nNo\n0.497379\n0.502621\n0.005243\n\n\n2941\n170\n4.35\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nFalse\nTrue\nYes\n0.504219\n0.495781\n0.008438\n\n\n1304\n825\n1.93\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nNo\n0.495184\n0.504816\n0.009633\n\n\n2165\n805\n1.27\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nYes\n0.504844\n0.495156\n0.009689\n\n\n2435\n975\n1.48\nTrue\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nNo\n0.495000\n0.505000\n0.010000\n\n\n1120\n315\n2.99\nTrue\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\nNo\n0.494613\n0.505387\n0.010775\n\n\n4051\n640\n2.96\nTrue\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nYes\n0.505709\n0.494291\n0.011418"
  },
  {
    "objectID": "posts/13wk-52.html#c.-관측치별-해석",
    "href": "posts/13wk-52.html#c.-관측치별-해석",
    "title": "13wk-52: 취업(오버피팅) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n- 0번관측치\n\npredictr.predict_proba(df_train.iloc[[0]])\n\n\n\n\n\n\n\n\nNo\nYes\n\n\n\n\n4431\n0.793558\n0.206442\n\n\n\n\n\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[0]]*1\n)\n\n\n\n\n\n\n\n\ntoiec\ngpa\nemployment\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\n\n\n\n\n4431\n460\n1.98\nNo\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n# 떨어진이유\n\n- 1번관측치\n\npredictr.predict_proba(df_train.iloc[[1]])\n\n\n\n\n\n\n\n\nNo\nYes\n\n\n\n\n2162\n0.006966\n0.993034\n\n\n\n\n\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[1]]*1\n)\n\n\n\n\n\n\n\n\ntoiec\ngpa\nemployment\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\n\n\n\n\n2162\n940\n3.7\nYes\n1\n0\n1\n1\n1\n1\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n# 합격한이유"
  },
  {
    "objectID": "posts/13wk-55.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-55.html#a.-y의-분포-xy의-관계-시각화",
    "title": "13wk-55: Medical Cost / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train, \n    label='charges',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\ncharges\n1338\n13270.422265\n12110.011237\n1121.8739\n4740.28715\n9382.033\n16639.912515\n63770.42801\nfloat64\n1337\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for charges &gt;= 0.5\n\n\n\n\n\nFeature interaction between smoker/charges in train_data\n\n\n\n\n\nFeature interaction between age/charges in train_data"
  },
  {
    "objectID": "posts/13wk-55.html#target-variable-analysis",
    "href": "posts/13wk-55.html#target-variable-analysis",
    "title": "13wk-55: Medical Cost / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-55.html#b.-중요한-설명변수",
    "href": "posts/13wk-55.html#b.-중요한-설명변수",
    "title": "13wk-55: Medical Cost / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data = df_train, \n    label='charges',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231130_133734/\"\n\n\nModel Prediction for charges\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-4250.255585\n-4759.009823\n0.002568\n0.002867\n0.343868\n0.002568\n0.002867\n0.343868\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\nsmoker\n11076.822803\n324.831922\n8.864565e-08\n5\n11745.656221\n10407.989386\n\n\nbmi\n3192.326797\n150.875820\n5.969502e-07\n5\n3502.982196\n2881.671399\n\n\nage\n2328.261296\n82.062303\n1.848886e-07\n5\n2497.228712\n2159.293881\n\n\nchildren\n77.495531\n20.011431\n4.892476e-04\n5\n118.699345\n36.291717\n\n\nregion\n39.652780\n26.510840\n1.435724e-02\n5\n94.238965\n-14.933406\n\n\nsex\n-21.116027\n18.796163\n9.670434e-01\n5\n17.585533\n-59.817587\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\ncharges_pred\nerror\n\n\n\n\n1019\n21\nfemale\n32.680\n2\nno\nnorthwest\n26018.95052\n5010.268555\n21008.681965\n\n\n140\n34\nmale\n22.420\n2\nno\nnortheast\n27375.90478\n6853.432129\n20522.472651\n\n\n1027\n23\nmale\n18.715\n0\nno\nnorthwest\n21595.38229\n2316.816650\n19278.565640\n\n\n526\n19\nfemale\n30.590\n2\nno\nnorthwest\n24059.68019\n5120.955078\n18938.725112\n\n\n1039\n19\nmale\n27.265\n2\nno\nnorthwest\n22493.65964\n4008.429199\n18485.230441\n\n\n959\n48\nmale\n36.670\n1\nno\nnorthwest\n28468.91901\n10121.377930\n18347.541080\n\n\n1008\n25\nmale\n24.985\n2\nno\nnortheast\n23241.47453\n5985.646484\n17255.828046\n\n\n539\n53\nmale\n31.350\n0\nno\nsoutheast\n27346.04207\n11388.868164\n15957.173906\n\n\n9\n60\nfemale\n25.840\n0\nno\nnorthwest\n28923.13692\n14113.998047\n14809.138873\n\n\n443\n59\nfemale\n36.520\n1\nno\nsoutheast\n28287.89766\n14497.187500\n13790.710160"
  },
  {
    "objectID": "posts/13wk-55.html#c.-관측치별-해석",
    "href": "posts/13wk-55.html#c.-관측치별-해석",
    "title": "13wk-55: Medical Cost / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n\npredictr.predict(df_train.iloc[[1]])\n\n1    3754.451172\nName: charges, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    display_rows=True,\n    plot='waterfall',\n    rows=df_train.iloc[[1]]*1\n)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n1\n18\nmale\n33.77\n1\nno\nsoutheast\n1725.5523"
  },
  {
    "objectID": "posts/03wk-09.html#a.-질문",
    "href": "posts/03wk-09.html#a.-질문",
    "title": "03wk-09: 아이스크림 / 회귀분석 – 추가해설",
    "section": "A. 질문",
    "text": "A. 질문\n- 질문: 기온이 \\(x=-2.0\\) 일 때 아이스크림을 얼마정도 판다고 보는게 타당할까?"
  },
  {
    "objectID": "posts/03wk-09.html#b.-답1",
    "href": "posts/03wk-09.html#b.-답1",
    "title": "03wk-09: 아이스크림 / 회귀분석 – 추가해설",
    "section": "B. 답1",
    "text": "B. 답1\n- \\(x=-2.0\\) 근처의 데이터를 살펴보자.\n\ndf_train[(-3.0 &lt; df_train.temp) & (df_train.temp &lt; -1.0)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n3\n-1.3\n17.673681\n\n\n\n\n\n\n\n대충 17.67 근처이지 않을까?.."
  },
  {
    "objectID": "posts/03wk-09.html#c.-답2",
    "href": "posts/03wk-09.html#c.-답2",
    "title": "03wk-09: 아이스크림 / 회귀분석 – 추가해설",
    "section": "C. 답2",
    "text": "C. 답2\n- 자료를 바탕으로 그림을 그려보자.\n\nplt.plot(df_train.temp,df_train.sales,'o')\nplt.plot([-2.0],[17.67],'x')\n\n\n\n\n\n저거 보다 못팔 것 같은데?"
  },
  {
    "objectID": "posts/03wk-09.html#d.-아이디어",
    "href": "posts/03wk-09.html#d.-아이디어",
    "title": "03wk-09: 아이스크림 / 회귀분석 – 추가해설",
    "section": "D. 아이디어",
    "text": "D. 아이디어\n- 선을 기가 막히게 그어서 추세선을 만들고, 그 추세선 위의 점으로 예측하자.\n- 속마음: 사실 추세선을 알고 있긴함\n\nplt.plot(df_train.temp,df_train.sales,'o')\nplt.plot(df_train.temp,20+df_train.temp*2.5,'--')\n\n\n\n\n- 사실 \\(y=20+2.5x\\) 라는 추세선을 그으면 된다는 것을 알고 있다.\n- 그래서 \\(x=-2\\) 이라면 \\(y=20-2.5\\times 2=15\\) 라고 보는게 합리적임. (물론 오차가 있을 수 있지만 그건 운이므로 어쩔수 없는것임, 랜덤으로 뭐가 나올지까지 맞출 수는 없음1)\n1 만약 그렇다면 랜덤이 아니겠지?\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n오버피팅: 랜덤으로 뭐가 나올지 맞춘다는 것은 오차항을 적합하겠다는 의미임.\n\n\n- 그렇지만 우리는 사실 \\(20, 2.5\\) 라는 숫자를 모른다. (이 숫자만 안다면 임의의 \\(x\\)에 대한 \\(y\\)값을 알 수 있을 텐데…)\n- 게임셋팅\n\n원래게임: 임의의 \\(x\\)에 대하여 합리적인 \\(y\\)를 잘 찾는 게임\n변형된게임: \\(20,2.5\\) 라는 숫자를 잘 찾는 게임, 즉 데이터를 보고 최대한 \\(y_i \\approx ax_i+b\\) 이 되도록 \\(a,b\\)를 잘 선택하는 게임"
  },
  {
    "objectID": "posts/03wk-09.html#a.-데이터",
    "href": "posts/03wk-09.html#a.-데이터",
    "title": "03wk-09: 아이스크림 / 회귀분석 – 추가해설",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 변수 설정\n\nX = df_train[['temp']] # 독립변수, 설명변수, 피쳐\ny = df_train[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nplt.plot(X,y,'o')\n\n\n\n\n- 질문: 기온이 \\(x=-2.0\\) 일 때 아이스크림을 얼마정도 판다고 보는게 타당할까?\n\ndf_train[(-3.0 &lt; df_train.temp) & (df_train.temp &lt; -1.0)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n3\n-1.3\n17.673681\n\n\n\n\n\n\n\n- 답1: 대충 17.67 근처이지 않을까?..\n- 답2: 17.67 보다 작지 않을까?\n- 아이디어: 추세선을 그리고 거기서 예측해보면 어떨까?\n- 데이터를 학습하여 추세선을 적절히 그릴 수 있고, 그려진 추세선으로 예측까지 해줄수 있는 아이(predictor)를 만들자."
  },
  {
    "objectID": "posts/03wk-09.html#b.-predictor-생성",
    "href": "posts/03wk-09.html#b.-predictor-생성",
    "title": "03wk-09: 아이스크림 / 회귀분석 – 추가해설",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression() \npredictr \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlinear regression 이라는 방법으로 추세선을 만들고 예측하는 아이(predictor)를 만드는 코드"
  },
  {
    "objectID": "posts/03wk-09.html#c.-학습-fit-learn",
    "href": "posts/03wk-09.html#c.-학습-fit-learn",
    "title": "03wk-09: 아이스크림 / 회귀분석 – 추가해설",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-09.html#d.-예측-predict",
    "href": "posts/03wk-09.html#d.-예측-predict",
    "title": "03wk-09: 아이스크림 / 회귀분석 – 추가해설",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- Predictor: 데이터를 살펴보니 원래 true는 이럴것 같아요\n\nyhat = predictr.predict(X)\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat, 'o--',alpha=0.5)\n\n\n\n\n- 최규빈: 저런 추세선을 그었다면, \\(y=ax+b\\) 꼴의 식에서 \\(a\\), \\(b\\)를 적당한 값으로 찾았다는 의미인데, 그 값은 어디있지?\n- Predictor: 아래에 있어요\n\na = predictr.coef_,\nb = predictr.intercept_\n\n\na,b\n\n((array([[2.51561216]]),), array([19.66713127]))\n\n\n- 최규빈: 확인해보자..\n\n(df_train.temp * 2.51561216 + 19.66713127)[:5], yhat[:5]\n\n(0     9.353121\n 1    10.359366\n 2    12.120295\n 3    16.396835\n 4    18.409325\n Name: temp, dtype: float64,\n array([[ 9.35312141],\n        [10.35936628],\n        [12.12029479],\n        [16.39683546],\n        [18.40932519]]))\n\n\n- 새로운 데이터 \\(x=-2\\) 에 대한 예측 (1) – 수식위주로\n\n2.51561216*(-2) + 19.66713127\n\n14.635906949999999\n\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat,'--',alpha=0.5)\nplt.plot([-2],[14.635906949999999],'xr')\n\n\n\n\n- 새로운 데이터 \\(x=-2\\) 에 대한 예측 (2) – 코드위주로 (\\(\\star\\))\n\nXX = pd.DataFrame({'temp':[-2.0]})\n\n\npredictr.predict(XX)\n\narray([[14.63590695]])\n\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat,'--',alpha=0.5)\nplt.plot(XX, predictr.predict(XX),'xr')"
  },
  {
    "objectID": "posts/02wk-06.html#a.-데이터",
    "href": "posts/02wk-06.html#a.-데이터",
    "title": "02wk-06: 타이타닉 / Autogluon (Fsize)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\n!kaggle competitions download -c titanic\n!unzip titanic.zip -d ./titanic\ndf_train = TabularDataset('titanic/train.csv')\ndf_test = TabularDataset('titanic/test.csv')\n!rm titanic.zip\n!rm -rf titanic/\n\nDownloading titanic.zip to /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\n  0%|                                               | 0.00/34.1k [00:00&lt;?, ?B/s]\n100%|███████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 395kB/s]\nArchive:  titanic.zip\n  inflating: ./titanic/gender_submission.csv  \n  inflating: ./titanic/test.csv      \n  inflating: ./titanic/train.csv     \n\n\n- 피처엔지니어링\n\ndf_train.eval('Fsize = SibSp + Parch')\ndf_test.eval('Fsize = SibSp + Parch')\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n0\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n1\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n0\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n0\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\nSpector, Mr. Woolf\nmale\nNaN\n0\n0\nA.5. 3236\n8.0500\nNaN\nS\n0\n\n\n414\n1306\n1\nOliva y Ocana, Dona. Fermina\nfemale\n39.0\n0\n0\nPC 17758\n108.9000\nC105\nC\n0\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\n0\n0\nSOTON/O.Q. 3101262\n7.2500\nNaN\nS\n0\n\n\n416\n1308\n3\nWare, Mr. Frederick\nmale\nNaN\n0\n0\n359309\n8.0500\nNaN\nS\n0\n\n\n417\n1309\n3\nPeter, Master. Michael J\nmale\nNaN\n1\n1\n2668\n22.3583\nNaN\nC\n2\n\n\n\n\n418 rows × 12 columns"
  },
  {
    "objectID": "posts/02wk-06.html#b.-predictor-생성",
    "href": "posts/02wk-06.html#b.-predictor-생성",
    "title": "02wk-06: 타이타닉 / Autogluon (Fsize)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231024_084218\""
  },
  {
    "objectID": "posts/02wk-06.html#c.-적합fit",
    "href": "posts/02wk-06.html#c.-적합fit",
    "title": "02wk-06: 타이타닉 / Autogluon (Fsize)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(df_train.eval('Fsize = SibSp + Parch'))\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231024_084218\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   265.27 GB / 490.57 GB (54.1%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    124116.32 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    12 features in original data used to generate 29 features in processed data.\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2417.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Train Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.22s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8156   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8212   = Validation score   (accuracy)\n    0.19s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.14s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.11s    = Validation runtime\nFitting model: CatBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8268   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.84s    = Training   runtime\n    0.24s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.77s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n    0.8324   = Validation score   (accuracy)\n    1.82s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n    0.8268   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8324   = Validation score   (accuracy)\n    3.7s     = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBMLarge ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8324   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8547   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 11.61s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231024_084218\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f29d8593b80&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.016608  2.934600                0.000400           0.236214            2       True         14\n1        NeuralNetTorch   0.837989       0.008671  1.382263                0.008671           1.382263            1       True         12\n2         LightGBMLarge   0.832402       0.003077  0.377827                0.003077           0.377827            1       True         13\n3       NeuralNetFastAI   0.832402       0.007537  1.316123                0.007537           1.316123            1       True         10\n4              CatBoost   0.826816       0.003649  0.528946                0.003649           0.528946            1       True          7\n5               XGBoost   0.826816       0.004545  0.150251                0.004545           0.150251            1       True         11\n6              LightGBM   0.821229       0.003294  0.180805                0.003294           0.180805            1       True          4\n7      RandomForestEntr   0.821229       0.194986  0.478185                0.194986           0.478185            1       True          6\n8            LightGBMXT   0.815642       0.003372  0.198229                0.003372           0.198229            1       True          3\n9      RandomForestGini   0.815642       0.097445  0.306731                0.097445           0.306731            1       True          5\n10       ExtraTreesEntr   0.810056       0.076015  0.512563                0.076015           0.512563            1       True          9\n11       ExtraTreesGini   0.804469       0.118582  0.881634                0.118582           0.881634            1       True          8\n12       KNeighborsDist   0.648045       0.001869  0.007722                0.001869           0.007722            1       True          2\n13       KNeighborsUnif   0.648045       0.004271  0.092099                0.004271           0.092099            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.016608\n2.934600\n0.000400\n0.236214\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.008671\n1.382263\n0.008671\n1.382263\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.832402\n0.003077\n0.377827\n0.003077\n0.377827\n1\nTrue\n13\n\n\n3\nNeuralNetFastAI\n0.832402\n0.007537\n1.316123\n0.007537\n1.316123\n1\nTrue\n10\n\n\n4\nCatBoost\n0.826816\n0.003649\n0.528946\n0.003649\n0.528946\n1\nTrue\n7\n\n\n5\nXGBoost\n0.826816\n0.004545\n0.150251\n0.004545\n0.150251\n1\nTrue\n11\n\n\n6\nLightGBM\n0.821229\n0.003294\n0.180805\n0.003294\n0.180805\n1\nTrue\n4\n\n\n7\nRandomForestEntr\n0.821229\n0.194986\n0.478185\n0.194986\n0.478185\n1\nTrue\n6\n\n\n8\nLightGBMXT\n0.815642\n0.003372\n0.198229\n0.003372\n0.198229\n1\nTrue\n3\n\n\n9\nRandomForestGini\n0.815642\n0.097445\n0.306731\n0.097445\n0.306731\n1\nTrue\n5\n\n\n10\nExtraTreesEntr\n0.810056\n0.076015\n0.512563\n0.076015\n0.512563\n1\nTrue\n9\n\n\n11\nExtraTreesGini\n0.804469\n0.118582\n0.881634\n0.118582\n0.881634\n1\nTrue\n8\n\n\n12\nKNeighborsDist\n0.648045\n0.001869\n0.007722\n0.001869\n0.007722\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.648045\n0.004271\n0.092099\n0.004271\n0.092099\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-06.html#d.-예측-predict",
    "href": "posts/02wk-06.html#d.-예측-predict",
    "title": "02wk-06: 타이타닉 / Autogluon (Fsize)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(df_train.Survived == predictr.predict(df_train.eval('Fsize = SibSp + Parch'))).mean()\n\n0.9169472502805837\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\n# df_test.assign(Survived = predictr.predict(df_test.eval('Fsize = SibSp + Parch'))).loc[:,['PassengerId','Survived']]\\\n# .to_csv(\"autogluon(Fsize)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/02wk-04.html#part-1-get-started",
    "href": "posts/02wk-04.html#part-1-get-started",
    "title": "02wk-04: 타이타닉 / Alexis Cook의 코드",
    "section": "Part 1: Get started",
    "text": "Part 1: Get started\nIn this section, you’ll learn more about the competition and make your first submission.\n\nJoin the competition!\nThe first thing to do is to join the competition! Open a new window with the competition page, and click on the “Join Competition” button, if you haven’t already. (If you see a “Submit Predictions” button instead of a “Join Competition” button, you have already joined the competition, and don’t need to do so again.)\n\nThis takes you to the rules acceptance page. You must accept the competition rules in order to participate. These rules govern how many submissions you can make per day, the maximum team size, and other competition-specific details. Then, click on “I Understand and Accept” to indicate that you will abide by the competition rules.\n\n\nThe challenge\nThe competition is simple: we want you to use the Titanic passenger data (name, age, price of ticket, etc) to try to predict who will survive and who will die.\n\n\nThe data\nTo take a look at the competition data, click on the Data tab at the top of the competition page. Then, scroll down to find the list of files.\nThere are three files in the data: (1) train.csv, (2) test.csv, and (3) gender_submission.csv.\n\n(1) train.csv\ntrain.csv contains the details of a subset of the passengers on board (891 passengers, to be exact – where each passenger gets a different row in the table). To investigate this data, click on the name of the file on the left of the screen. Once you’ve done this, you can view all of the data in the window.\n\nThe values in the second column (“Survived”) can be used to determine whether each passenger survived or not: - if it’s a “1”, the passenger survived. - if it’s a “0”, the passenger died.\nFor instance, the first passenger listed in train.csv is Mr. Owen Harris Braund. He was 22 years old when he died on the Titanic.\n\n\n(2) test.csv\nUsing the patterns you find in train.csv, you have to predict whether the other 418 passengers on board (in test.csv) survived.\nClick on test.csv (on the left of the screen) to examine its contents. Note that test.csv does not have a “Survived” column - this information is hidden from you, and how well you do at predicting these hidden values will determine how highly you score in the competition!\n\n\n(3) gender_submission.csv\nThe gender_submission.csv file is provided as an example that shows how you should structure your predictions. It predicts that all female passengers survived, and all male passengers died. Your hypotheses regarding survival will probably be different, which will lead to a different submission file. But, just like this file, your submission should have: - a “PassengerId” column containing the IDs of each passenger from test.csv. - a “Survived” column (that you will create!) with a “1” for the rows where you think the passenger survived, and a “0” where you predict that the passenger died."
  },
  {
    "objectID": "posts/02wk-04.html#part-2-your-coding-environment",
    "href": "posts/02wk-04.html#part-2-your-coding-environment",
    "title": "02wk-04: 타이타닉 / Alexis Cook의 코드",
    "section": "Part 2: Your coding environment",
    "text": "Part 2: Your coding environment\nIn this section, you’ll train your own machine learning model to improve your predictions. If you’ve never written code before or don’t have any experience with machine learning, don’t worry! We don’t assume any prior experience in this tutorial.\n\nThe Notebook\nThe first thing to do is to create a Kaggle Notebook where you’ll store all of your code. You can use Kaggle Notebooks to getting up and running with writing code quickly, and without having to install anything on your computer. (If you are interested in deep learning, we also offer free GPU access!)\nBegin by clicking on the Code tab on the competition page. Then, click on “New Notebook”.\n\nYour notebook will take a few seconds to load. In the top left corner, you can see the name of your notebook – something like “kernel2daed3cd79”.\n\nYou can edit the name by clicking on it. Change it to something more descriptive, like “Getting Started with Titanic”.\n\n\n\nYour first lines of code\nWhen you start a new notebook, it has two gray boxes for storing code. We refer to these gray boxes as “code cells”.\n\nThe first code cell already has some code in it. To run this code, put your cursor in the code cell. (If your cursor is in the right place, you’ll notice a blue vertical line to the left of the gray box.) Then, either hit the play button (which appears to the left of the blue line), or hit [Shift] + [Enter] on your keyboard.\nIf the code runs successfully, three lines of output are returned. Below, you can see the same code that you just ran, along with the output that you should see in your notebook.\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nThis shows us where the competition data is stored, so that we can load the files into the notebook. We’ll do that next.\n\n\nLoad the data\nThe second code cell in your notebook now appears below the three lines of output with the file locations.\n\nType the two lines of code below into your second code cell. Then, once you’re done, either click on the blue play button, or hit [Shift] + [Enter].\n\n#train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nYour code should return the output above, which corresponds to the first five rows of the table in train.csv. It’s very important that you see this output in your notebook before proceeding with the tutorial! &gt; If your code does not produce this output, double-check that your code is identical to the two lines above. And, make sure your cursor is in the code cell before hitting [Shift] + [Enter].\nThe code that you’ve just written is in the Python programming language. It uses a Python “module” called pandas (abbreviated as pd) to load the table from the train.csv file into the notebook. To do this, we needed to plug in the location of the file (which we saw was /kaggle/input/titanic/train.csv).\n&gt; If you’re not already familiar with Python (and pandas), the code shouldn’t make sense to you – but don’t worry! The point of this tutorial is to (quickly!) make your first submission to the competition. At the end of the tutorial, we suggest resources to continue your learning.\nAt this point, you should have at least three code cells in your notebook.\n\nCopy the code below into the third code cell of your notebook to load the contents of the test.csv file. Don’t forget to click on the play button (or hit [Shift] + [Enter])!\n\n#test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nAs before, make sure that you see the output above in your notebook before continuing.\nOnce all of the code runs successfully, all of the data (in train.csv and test.csv) is loaded in the notebook. (The code above shows only the first 5 rows of each table, but all of the data is there – all 891 rows of train.csv and all 418 rows of test.csv!)"
  },
  {
    "objectID": "posts/02wk-04.html#part-3-your-first-submission",
    "href": "posts/02wk-04.html#part-3-your-first-submission",
    "title": "02wk-04: 타이타닉 / Alexis Cook의 코드",
    "section": "Part 3: Your first submission",
    "text": "Part 3: Your first submission\nRemember our goal: we want to find patterns in train.csv that help us predict whether the passengers in test.csv survived.\nIt might initially feel overwhelming to look for patterns, when there’s so much data to sort through. So, we’ll start simple.\n\nExplore a pattern\nRemember that the sample submission file in gender_submission.csv assumes that all female passengers survived (and all male passengers died).\nIs this a reasonable first guess? We’ll check if this pattern holds true in the data (in train.csv).\nCopy the code below into a new code cell. Then, run the cell.\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n\n% of women who survived: 0.7420382165605095\n\n\n\n여성의 생존률을 구하는 코드입니다, 이전에 accuracy를 구하던 테크닉을 활용하면 아래의 코드도 가능합니다\n\ntrain_data[train_data.Sex == 'female'].Survived.mean()\n\n0.7420382165605095\n\n\n\nBefore moving on, make sure that your code returns the output above. The code above calculates the percentage of female passengers (in train.csv) who survived.\nThen, run the code below in another code cell:\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n\n% of men who survived: 0.18890814558058924\n\n\nThe code above calculates the percentage of male passengers (in train.csv) who survived.\nFrom this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in gender_submission.csv is not a bad first guess!\nBut at the end of the day, this gender-based submission bases its predictions on only a single column. As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions. Since it is quite difficult to consider several columns at once (or, it would take a long time to consider all possible patterns in many different columns simultaneously), we’ll use machine learning to automate this for us."
  },
  {
    "objectID": "posts/02wk-04.html#your-first-machine-learning-model",
    "href": "posts/02wk-04.html#your-first-machine-learning-model",
    "title": "02wk-04: 타이타닉 / Alexis Cook의 코드",
    "section": "Your first machine learning model",
    "text": "Your first machine learning model\nWe’ll build what’s known as a random forest model. This model is constructed of several “trees” (there are three trees in the picture below, but we’ll construct 100!) that will individually consider each passenger’s data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\n\nThe code cell below looks for patterns in four different columns (“Pclass”, “Sex”, “SibSp”, and “Parch”) of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file submission.csv.\nCopy this code into your notebook, and run it in a new code cell.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\nMake sure that your notebook outputs the same message above (Your submission was successfully saved!) before moving on. &gt; Again, don’t worry if this code doesn’t make sense to you! For now, we’ll focus on how to generate and submit predictions.\nOnce you’re ready, click on the “Save Version” button in the top right corner of your notebook. This will generate a pop-up window.\n- Ensure that the “Save and Run All” option is selected, and then click on the “Save” button. - This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the “Save Version” button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (…) to the right of the most recent version, and select Open in Viewer.\n- Click on the Data tab on the top of the screen. Then, click on the “Submit” button to submit your results.\n\nCongratulations for making your first submission to a Kaggle competition! Within ten minutes, you should receive a message providing your spot on the leaderboard. Great work!"
  },
  {
    "objectID": "posts/02wk-04.html#part-4-learn-more",
    "href": "posts/02wk-04.html#part-4-learn-more",
    "title": "02wk-04: 타이타닉 / Alexis Cook의 코드",
    "section": "Part 4: Learn more!",
    "text": "Part 4: Learn more!\nIf you’re interested in learning more, we strongly suggest our (3-hour) Intro to Machine Learning course, which will help you fully understand all of the code that we’ve presented here. You’ll also know enough to generate even better predictions!"
  },
  {
    "objectID": "posts/02wk-04.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "href": "posts/02wk-04.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "title": "02wk-04: 타이타닉 / Alexis Cook의 코드",
    "section": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?",
    "text": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?\n- 원래코드\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\n\nlen(predictions), len(X_test)\n\n(418, 418)\n\n\n- 이렇게 수정하면 될 듯\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\nlen(predictions),len(y)\n\n(891, 891)\n\n\n\n(predictions == y).mean()\n\n0.8159371492704826"
  },
  {
    "objectID": "posts/02wk-04.html#b.-alexis-cook의-코드를-수정해보자",
    "href": "posts/02wk-04.html#b.-alexis-cook의-코드를-수정해보자",
    "title": "02wk-04: 타이타닉 / Alexis Cook의 코드",
    "section": "B. Alexis Cook의 코드를 수정해보자!",
    "text": "B. Alexis Cook의 코드를 수정해보자!\n- 코드를 수정해보자.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=5000, max_depth=1000, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\n(predictions == y).mean()\n\n0.8170594837261503\n\n\n\n내가 만든게 더 좋은데??\n\n- 이것도 제출결과로 만들어보자.\n\npredictions = model.predict(X_test)\n\n- 아래와 같이 제출하면 에러가 발생\n\n# pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\\\n# .assign(Survived=predictions)\\\n# .to_csv(\"AlexisCook수정_submission.csv\")\n\n- 아래와 같이 제출파일을 저장해야 한다.\n\n# pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\\\n# .assign(Survived=predictions)\\\n# .to_csv(\"AlexisCook수정2_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/04wk-18.html#a.-학습-이후에-예측평가-가능",
    "href": "posts/04wk-18.html#a.-학습-이후에-예측평가-가능",
    "title": "04wk-18: Predictor의 깊은 이해, 기호정리",
    "section": "A. 학습 이후에 예측/평가 가능",
    "text": "A. 학습 이후에 예측/평가 가능\n- Predictor의 list생성\n\npredictors = [sklearn.linear_model.LinearRegression() for i in range(2)]\npredictors\n\n[LinearRegression(), LinearRegression()]\n\n\n\n두개의 predictor를 만들어서 리스트로 정리함\n\n- 첫번째 predictor에 접근\n\npredictors[0]\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 두번째 predictor에 접근\n\npredictors[1]\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 첫번째 predictor를 학습\n\npredictors[0].fit(df_train_X,df_train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 학습이후에는 .coef_, .intercept_ 값이 생성됨\n\npredictors[0].coef_, predictors[0].intercept_\n\n(array([[-0.99420514]]), array([20.86235044]))\n\n\n\npredictors[1].coef_, predictors[1].intercept_\n\nAttributeError: 'LinearRegression' object has no attribute 'coef_'\n\n\n- .coef_와 .intercept_값이 생겨야 .predict(X)를 통하여 예측을 할 수 있음\n\npredictors[0].predict(df_train_X)\n\narray([[ 0.97824769],\n       [-0.01595745],\n       [-1.01016259],\n       [-2.00436772],\n       [-2.99857286],\n       [-3.992778  ],\n       [-4.98698314],\n       [-5.98118827]])\n\n\n\npredictors[1].predict(df_train_X)\n\nNotFittedError: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n\n\n- 예측을 해야 평가를 할 수 있음\n\npredictors[0].score(df_train_X,df_train_y)\n\n0.9996857763812423\n\n\n\npredictors[1].score(df_train_X,df_train_y)\n\nNotFittedError: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
  },
  {
    "objectID": "posts/04wk-18.html#b.-.fitxy에서-xy의-형식",
    "href": "posts/04wk-18.html#b.-.fitxy에서-xy의-형식",
    "title": "04wk-18: Predictor의 깊은 이해, 기호정리",
    "section": "B. .fit(X,y)에서 X,y의 형식",
    "text": "B. .fit(X,y)에서 X,y의 형식\n\npredictr = predictors[0]\n\n\nXs = {'DataFrame(2d)': df_train_X, \n      'Seires(1d)': df_train_X.X,\n      'ndarray(2d)': np.array(df_train_X),\n      'ndarray(1d)': np.array(df_train_X).reshape(-1),\n      'list(2d)': np.array(df_train_X).tolist(),\n      'list(1d)': np.array(df_train_X).reshape(-1).tolist()}\n\n\nys = {'DataFrame(2d)': df_train_y, \n      'Seires(1d)': df_train_y.y,\n      'ndarray(2d)': np.array(df_train_y),\n      'ndarray(1d)': np.array(df_train_y).reshape(-1),\n      'list(2d)': np.array(df_train_y).tolist(),\n      'list(1d)': np.array(df_train_y).reshape(-1).tolist()}\n\n\ndef test(X,y):\n    try: \n        predictr.fit(X,y)\n        return 'no error'\n    except:\n        return 'error'\n\n\n{('X='+i,'y='+j): test(Xs[i],ys[j]) for i,j in itertools.product(Xs.keys(),ys.keys())}\n\n{('X=DataFrame(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=Seires(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(1d)'): 'no error',\n ('X=Seires(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=Seires(1d)', 'y=Seires(1d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(2d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(1d)'): 'error',\n ('X=Seires(1d)', 'y=list(2d)'): 'error',\n ('X=Seires(1d)', 'y=list(1d)'): 'error',\n ('X=ndarray(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=Seires(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(1d)'): 'no error',\n ('X=ndarray(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=ndarray(1d)', 'y=Seires(1d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(2d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(1d)'): 'error',\n ('X=ndarray(1d)', 'y=list(2d)'): 'error',\n ('X=ndarray(1d)', 'y=list(1d)'): 'error',\n ('X=list(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=list(2d)', 'y=Seires(1d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=list(2d)', 'y=list(2d)'): 'no error',\n ('X=list(2d)', 'y=list(1d)'): 'no error',\n ('X=list(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=list(1d)', 'y=Seires(1d)'): 'error',\n ('X=list(1d)', 'y=ndarray(2d)'): 'error',\n ('X=list(1d)', 'y=ndarray(1d)'): 'error',\n ('X=list(1d)', 'y=list(2d)'): 'error',\n ('X=list(1d)', 'y=list(1d)'): 'error'}\n\n\n- 결론: X는 2d만 가능, y는 2d,1d 모두 가능"
  },
  {
    "objectID": "posts/02wk-05.html#a.-데이터",
    "href": "posts/02wk-05.html#a.-데이터",
    "title": "02wk-05: 타이타닉 / Autogluon",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\n!kaggle competitions download -c titanic\n!unzip titanic.zip -d ./titanic\ndf_train = TabularDataset('titanic/train.csv')\ndf_test = TabularDataset('titanic/test.csv')\n!rm titanic.zip\n!rm -rf titanic/\n\nDownloading titanic.zip to /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\n  0%|                                               | 0.00/34.1k [00:00&lt;?, ?B/s]\n100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 5.82MB/s]\nArchive:  titanic.zip\n  inflating: ./titanic/gender_submission.csv  \n  inflating: ./titanic/test.csv      \n  inflating: ./titanic/train.csv"
  },
  {
    "objectID": "posts/02wk-05.html#b.-predictor-생성",
    "href": "posts/02wk-05.html#b.-predictor-생성",
    "title": "02wk-05: 타이타닉 / Autogluon",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231024_084026\""
  },
  {
    "objectID": "posts/02wk-05.html#c.-적합fit",
    "href": "posts/02wk-05.html#c.-적합fit",
    "title": "02wk-05: 타이타닉 / Autogluon",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(df_train) \n# 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231024_084026\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   265.30 GB / 490.57 GB (54.1%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    124529.88 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2417.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.19s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBMXT ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8156   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8212   = Validation score   (accuracy)\n    0.16s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.2s     = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.61s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8268   = Validation score   (accuracy)\n    0.56s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.82s    = Training   runtime\n    0.22s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.7s     = Training   runtime\n    0.23s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\nNo improvement since epoch 9: early stopping\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n    0.8324   = Validation score   (accuracy)\n    2.15s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n    0.8101   = Validation score   (accuracy)\n    0.23s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8212   = Validation score   (accuracy)\n    3.4s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8324   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8324   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 12.03s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231024_084026\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fd1a92d4af0&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0         LightGBMLarge   0.832402       0.003578  0.422645                0.003578           0.422645            1       True         13\n1       NeuralNetFastAI   0.832402       0.011829  2.153854                0.011829           2.153854            1       True         10\n2   WeightedEnsemble_L2   0.832402       0.012396  2.629154                0.000567           0.475301            2       True         14\n3              CatBoost   0.826816       0.007282  0.558903                0.007282           0.558903            1       True          7\n4              LightGBM   0.821229       0.003267  0.163059                0.003267           0.163059            1       True          4\n5        NeuralNetTorch   0.821229       0.014016  3.402719                0.014016           3.402719            1       True         12\n6            LightGBMXT   0.815642       0.003467  0.479265                0.003467           0.479265            1       True          3\n7      RandomForestEntr   0.815642       0.175467  0.606128                0.175467           0.606128            1       True          6\n8      RandomForestGini   0.815642       0.204887  0.539022                0.204887           0.539022            1       True          5\n9        ExtraTreesGini   0.815642       0.224873  0.821764                0.224873           0.821764            1       True          8\n10              XGBoost   0.810056       0.005781  0.229575                0.005781           0.229575            1       True         11\n11       ExtraTreesEntr   0.810056       0.229908  0.699347                0.229908           0.699347            1       True          9\n12       KNeighborsDist   0.653631       0.016264  0.011564                0.016264           0.011564            1       True          2\n13       KNeighborsUnif   0.653631       0.028849  0.011524                0.028849           0.011524            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMLarge\n0.832402\n0.003578\n0.422645\n0.003578\n0.422645\n1\nTrue\n13\n\n\n1\nNeuralNetFastAI\n0.832402\n0.011829\n2.153854\n0.011829\n2.153854\n1\nTrue\n10\n\n\n2\nWeightedEnsemble_L2\n0.832402\n0.012396\n2.629154\n0.000567\n0.475301\n2\nTrue\n14\n\n\n3\nCatBoost\n0.826816\n0.007282\n0.558903\n0.007282\n0.558903\n1\nTrue\n7\n\n\n4\nLightGBM\n0.821229\n0.003267\n0.163059\n0.003267\n0.163059\n1\nTrue\n4\n\n\n5\nNeuralNetTorch\n0.821229\n0.014016\n3.402719\n0.014016\n3.402719\n1\nTrue\n12\n\n\n6\nLightGBMXT\n0.815642\n0.003467\n0.479265\n0.003467\n0.479265\n1\nTrue\n3\n\n\n7\nRandomForestEntr\n0.815642\n0.175467\n0.606128\n0.175467\n0.606128\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.815642\n0.204887\n0.539022\n0.204887\n0.539022\n1\nTrue\n5\n\n\n9\nExtraTreesGini\n0.815642\n0.224873\n0.821764\n0.224873\n0.821764\n1\nTrue\n8\n\n\n10\nXGBoost\n0.810056\n0.005781\n0.229575\n0.005781\n0.229575\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.229908\n0.699347\n0.229908\n0.699347\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.016264\n0.011564\n0.016264\n0.011564\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.028849\n0.011524\n0.028849\n0.011524\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-05.html#d.-예측-predict",
    "href": "posts/02wk-05.html#d.-예측-predict",
    "title": "02wk-05: 타이타닉 / Autogluon",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(df_train.Survived == predictr.predict(df_train)).mean()\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n\n\n0.8810325476992144\n\n\n\n(df_train.Survived == (df_train.Sex == \"female\")).mean() # 예전점수와 비교\n\n0.7867564534231201\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ndf_test.assign(Survived = predictr.predict(df_test)).loc[:,['PassengerId','Survived']]\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n0\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n0\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\n# df_test.assign(Survived = predictr.predict(df_test)).loc[:,['PassengerId','Survived']]\\\n# .to_csv(\"autogluon_submission.csv\",index=False)"
  },
  {
    "objectID": "offline.html",
    "href": "offline.html",
    "title": "대면수업",
    "section": "",
    "text": "기계학습활용은 비대면수업과 대면수업을 병행합니다. 본 페이지는 대면수업을 위한 강의자료입니다.\n공지사항\n\n대면수업은 학점과 무관한 수업입니다. (대면수업에 참여하지 않아도 학점에 불이익이 없습니다)\n9월7일 대면수업: Quarto를 이용한 기술블로그 만들기 + 타이타닉 자료에 대한 발표/토론\n9월12일 대면수업: 9월12일 대면수업은 Quarto를 이용한 기술블로그 만들기를 이어서 설명하겠습니다. 9월7일 대면수업 내용을 따라오신 상태이어야 합니다. (가능하면 지난시간에 실습했던 자리를 이어서 하는게 좋을거에요)\n9월21일 대면수업: Anaconda를 이용한 개발환경만들기\n9월26일 대면수업: 코드스페이스를 이용한 리눅스명령어 실습\n11월14일 대면수업: 개발환경의 변천사\n\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 5, 2023\n\n\n2023-12-05 – 원격컴퓨터에서 개발환경 셋팅하기 (3)\n\n\n최규빈 \n\n\n\n\nNov 30, 2023\n\n\n2023-11-30 – 원격컴퓨터에서 개발환경 셋팅하기 (2)\n\n\n최규빈 \n\n\n\n\nNov 28, 2023\n\n\n2023-11-28 – 원격컴퓨터에서 개발환경 셋팅하기 (1)\n\n\n최규빈 \n\n\n\n\nNov 14, 2023\n\n\n2023-11-14 – 개발환경의 변천사\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n2023-09-26 – 코드스페이스, 리눅스명령어, kaggle.json\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n2023-09-21 – Anaconda를 이용하여 autogluon 설치하기\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n2023-09-12 – Quarto Blog 만들기\n\n\n최규빈 \n\n\n\n\nSep 7, 2023\n\n\n2023-09-07 – 준비, 가상환경설정, Local ↔︎ Github\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "offline/231128.html",
    "href": "offline/231128.html",
    "title": "2023-11-28 – 원격컴퓨터에서 개발환경 셋팅하기 (1)",
    "section": "",
    "text": "- 원격컴퓨터에서 간단한 개발환경 셋팅"
  },
  {
    "objectID": "offline/231128.html#a.-ssh-연결",
    "href": "offline/231128.html#a.-ssh-연결",
    "title": "2023-11-28 – 원격컴퓨터에서 개발환경 셋팅하기 (1)",
    "section": "A. SSH 연결",
    "text": "A. SSH 연결\n- 연결하는 방법\nssh root@210.117.173.186 -p {학번끝5자리}\n\n예를들어 학번끝5자리가 43052라면 root@210.117.173.186 -p 43052\n\n\n초기비밀번호는 s+학번끝5자리이다. 즉 학번 끝 5자리가 43052라면 초기비밀번호는 s43052이다."
  },
  {
    "objectID": "offline/231128.html#b.-password-변경",
    "href": "offline/231128.html#b.-password-변경",
    "title": "2023-11-28 – 원격컴퓨터에서 개발환경 셋팅하기 (1)",
    "section": "B. password 변경",
    "text": "B. password 변경\n- 아래를 실행하여 비밀번호를 바꿀 것\npasswd root \n- 이후에는 아래를 입력하여 바뀐 password를 입력한다.\nssh root@210.117.173.186 -p {학번끝5자리}"
  },
  {
    "objectID": "offline/231128.html#c.-아나콘다-설치",
    "href": "offline/231128.html#c.-아나콘다-설치",
    "title": "2023-11-28 – 원격컴퓨터에서 개발환경 셋팅하기 (1)",
    "section": "C. 아나콘다 설치",
    "text": "C. 아나콘다 설치\n- 아나콘다 홈페이지 접속 \\(\\to\\) 다운로드 링크 확보 \\(\\to\\) 아나콘다 다운로드\n# 아나콘다 홈페 접속\n# 다운로드링크 확보 (쉬프트+클릭으로 열면 홈페이지 주소 복사가능) \nwget 복사한주소\n\n- (아나콘다 설치) 아나콘다를 다운받은 폴더로 가서 아래와 같이 실행한다.\nbash Anaconda3-2019.03-Linux-x86_64.sh\n대충 bash Ana 정도까지만 치고 tab을 누르면 알아서 완성된다. (스페이스를 누르면서 라이센스텀을 넘긴다)"
  },
  {
    "objectID": "offline/231128.html#d.-주피터랩-설치-및-실행",
    "href": "offline/231128.html#d.-주피터랩-설치-및-실행",
    "title": "2023-11-28 – 원격컴퓨터에서 개발환경 셋팅하기 (1)",
    "section": "D. 주피터랩 설치 및 실행",
    "text": "D. 주피터랩 설치 및 실행\n- 주피터랩 설치\nconda install -c conda-forge jupyterlab \nconda install -c conda-forge notebook \n- 패스워드 설정\njupyter notebook password\n## 패스워드 치고\n## 확인해야함\n- 실행\njupyter lab --ip=\"0.0.0.0\" --port={MY-PORT} --no-browser --allow-root\n\n\n\n\n\n\nNote\n\n\n\nport는 {1,2,3,4 중 학번끝5자리의 첫 글자 제외} + {학번끝4자리} 조합으로 쓸 수 있음. 예를들어 학번끝 5자리가 43052라면\n13052\n23052\n33052\n에 해당하는 port번호를 쓸 수 있음.\n\n\n\n- 백엔드에서 주피터랩 실행 (터미널을 껐을때 주피터도 꺼지는게 싫다면..)\n\n- 백엔드에서 실행된 주피터랩 종료"
  },
  {
    "objectID": "offline/231128.html#e.-오토글루온-설치",
    "href": "offline/231128.html#e.-오토글루온-설치",
    "title": "2023-11-28 – 원격컴퓨터에서 개발환경 셋팅하기 (1)",
    "section": "E. 오토글루온 설치",
    "text": "E. 오토글루온 설치\n- ref: https://auto.gluon.ai/stable/install.html\nconda activate ag \nconda install -c conda-forge mamba\nmamba install -c conda-forge autogluon"
  },
  {
    "objectID": "offline/231128.html#f.-git을-이용하여-강의노트-다운로드-업데이트",
    "href": "offline/231128.html#f.-git을-이용하여-강의노트-다운로드-업데이트",
    "title": "2023-11-28 – 원격컴퓨터에서 개발환경 셋팅하기 (1)",
    "section": "F. Git을 이용하여 강의노트 다운로드, 업데이트",
    "text": "F. Git을 이용하여 강의노트 다운로드, 업데이트\n- 예를들어 데이터시각화 강의노트를 받는다면 아래와 같이 입력한다.\ngit clone https://github.com/guebin/DV2023.git\n\n- updated된 데이터시각화 강의노트를 받으려면 “해당폴더로 이동한 후” 아래와 같이 입력한다.\ngit pull"
  },
  {
    "objectID": "offline/231128.html#g.-kaggle-설정하기",
    "href": "offline/231128.html#g.-kaggle-설정하기",
    "title": "2023-11-28 – 원격컴퓨터에서 개발환경 셋팅하기 (1)",
    "section": "G. Kaggle 설정하기",
    "text": "G. Kaggle 설정하기\n- 아래를 실행\nconda activate ag\npip install kaggle \n- kaggle.json 파일 업로드 + /root/.kaggle로 복사\ncp kaggle.json /root/.kaggle"
  },
  {
    "objectID": "offline/231205.html",
    "href": "offline/231205.html",
    "title": "2023-12-05 – 원격컴퓨터에서 개발환경 셋팅하기 (3)",
    "section": "",
    "text": "1. R과 Jupyter의 연결\n- 아래를 이용하여 가상환경생성, 가상환경활성화, 가상환경에R설치,R실행\nconda create -n r \nconda activate r \nconda install -c conda-forge r-essentials \nR\n- R이 실행되면 아래를 입력\nIRkernel::installspec()\nq() # 이후에 n을 누르고 빠져나간다.\n- 주피터에서 R이 실행되는지 확인한다."
  },
  {
    "objectID": "offline/230912.html",
    "href": "offline/230912.html",
    "title": "2023-09-12 – Quarto Blog 만들기",
    "section": "",
    "text": "0. 꼭 해야할까?\n1. https://www.youtube.com/watch?v=SZcjvjrdomE\n\nrepo도 확인함\ncommit 은 거의 기본임\n\n2. https://www.youtube.com/watch?v=Yc56NpYW1DM&t=23s\n\n3:00 - 3:40: 깃헙과 블로그가 중요함 / commit 을 빼곡하게, repo 확인\n3:40 - : 블로그\n4:00 - : commit 링크가 없으면 X, 필수라고 해서 넣은 느낌 (영상에서는 필수 아니라고 하지만 반 필수임)\n\n\n\n1. 블로그 생성\n- 깃헙레포지토리로 이동한뒤에 아래의 명령어를 입력:\nquarto create-project --type website --template blog\n위의 명령어를 실행하면 여러가지 설정파일들이 생성된다.\n- 블로그 publish\nquarto publish gh-pages\n- 깃헙으로 이동하여 확인\n\nwindow는 자동으로 퍼블리쉬 된다.\nmac에서는 수동으로 gh-pages로 바꿔야함.\n\n\n\n2. 새로운 포스트 생성\n- 아무 노트북이나 만들어본다. (혹은 이미 만들어진 노트북을 넣어본다.)\n\n예시1: https://guebin.github.io/PP2023/ 에서 아무거나 다운로드\n예시2: git clone https://github.com/guebin/PP2023.git\n\n- 노트북을 posts 폴더에 넣는다.\n- 아래를 통하여 preview 이미지를 확인.\nquarto preview \n- 퍼블리쉬\nquarto publish --no-prompt\n\n\n3. 블로그 설정변경\n- index.qmd 메인에 보이는 화면\n---\ntitle: \"기계학습활용 (2023)\"\nlisting:\n  contents: posts\n  sort: \"date\"\n  type: table\n  categories: true\n  sort-ui: false\n  filter-ui: false\npage-layout: full\ntitle-block-banner: false\n---\n\n**질문하는 방법**\n\n- 카카오톡: [질문하러 가기](http://pf.kakao.com/_skxnxdG/chat) // 학기종료이후 폐쇄함\n- 이메일: &lt;guebin@jbnu.ac.kr&gt;\n- 직접방문: 자연과학대학 본관 205호 \n- Zoom: *카카오톡이나 이메일로 미리 시간을 정할 것*\n- LMS쪽지: &lt;https://ieilms.jbnu.ac.kr/&gt;\n\n**references**\n\n**공지사항** \n\n- 강의영상의 경우 시간이 지나면 고화질로 시청할 수 있습니다. (인코딩중에는 유튜브에서 저화질로 송출됩니다)\n- **9월7일 대면수업**: Quarto를 이용한 기술블로그 만들기 + 타이타닉 자료에 대한 발표/토론 \n\n**강의노트** \n- _quarto.yml\nproject:\n  type: website\n\nwebsite:\n  title: \"MP2023\"\n  navbar:\n    right:\n      - icon: github    \n        href: https://github.com/guebin/MP2023\n      - icon: youtube\n        href: https://www.youtube.com/channel/UCQk9RyBNgXc7ORIsYlOfQrg/playlists?view=50&sort=dd&shelf_id=2\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n- posts 폴더에서 _metadata.yml\n# options specified here will apply to all posts in this folder\n\n# freeze computational output\n# (see https://quarto.org/docs/projects/code-execution.html#freeze)\nexecute:\n  freeze: auto  # re-render only when source changes\n\n# Enable banner style title blocks\ntitle-block-banner: false\n\nnotebook-links: true\ncap-location: margin\nreference-location: margin\ncitation-location: margin\n\nformat:\n  html:\n    toc: true\n    code-fold: false\n    code-line-numbers: false\n    code-copy: true\n  ipynb: default\n- 기타 자세한 옵션은 아래에서 뒤져볼 것\n\nref: https://quarto.org/\n\n- 다른 사람들의 블로그 소개 (설정파일 구경용)\n\nhttps://seoyeonc.github.io/sy_hub/\nhttps://boram-coco.github.io/coco/\nhttps://pinkocto.github.io/Quarto-Blog/\nhttps://pinkocto.github.io/noteda/\n\n\n\n4. 이해\n- 아래를 입력\ngit add .\ngit commit -m .\ngit switch gh-pages\n\n폴더내용 확인\n\n- 다시 원래대로 돌아오기\ngit switch main\n- quarto publish 의 역할\n\n.ipynb 파일을 어떠한 규칙에 따라 변환하여 .html파일로 바꿈\n바꾼 파일들을 gh-pages라는 이름의 브랜치로 옮김\ngh-pages 브랜치를 github 으로 push\n이후에는 github이 알아서 gh-pages의 내용을 웹이 뿌려줌.\n\n- 블로그가 깨졌을때 대응하는 방법\n\ngh-pages로 이동하여 모든 파일 및 폴더 삭제\ngit add ., git commit -m . git push를 순서대로 입력\n다시 main으로 돌아와서 quarto publish --no-prompt 입력\n\n\n\n5. 추천하는 루틴\ngit add .\ngit commit -m .\ngit push \nquarto publish --no-prompt"
  },
  {
    "objectID": "offline/230921.html",
    "href": "offline/230921.html",
    "title": "2023-09-21 – Anaconda를 이용하여 autogluon 설치하기",
    "section": "",
    "text": "1. 아나콘다 설치\n\n아나콘다 설치: https://www.anaconda.com/download\n\n\n\n2. 가상환경 mp 생성 + activate\nconda create -n mp python=3.10 \nconda activate mp \n\n\n3. autogluon 설치\nconda install -c conda-forge mamba\nmamba install -c conda-forge autogluon\n\n\n4. jupyterlab 설치 + 실행\nconda install -c conda-forge jupyterlab \njupyter lab \n\n\n5. pip install vs conda install\n- pip vs conda\n\npip은 특정 패키지만 설치, conda는 관련된 패키지를 모두 설치.\npip은 패키지간의 충돌이 발생할 수 있으며 conda는 이러한 충돌이 발생하지 않음.\npip은 최신버전을 설치하는 경향이 있으며 conda는 최신버전보다 약간 낮은 버전이 설치되는 경향이 있음 (최신버전의 경우 호환성여부가 조사되지 않았으므로).\n공식문서에는 pip과 conda를 이용한 설치가 모두 나와있으며, 보통은 conda를 이용하여 설치하는게 좋다. (scikit-learn, autogluon)\n\n\n\n6. 가상환경\n- 보통은 가상환경을 따로 만들고 패키지를 설치하는게 좋다.\n\n하나의 환경에 여러가지 패키지를 동시에 까는 경우, 패키지 숫자가 많아질수록 결국에는 충돌이 일어남.\n따라서 프로젝트별로 가상환경을 분리하여 관련 패키지를 설치하는 것이 좋다.\n\n- 요약\n\n본인컴퓨터로 작업하는 경우: 아나콘다 or 도커1를 이용하여 가상환경에서 작업할 것! 그렇지 않으면 컴퓨터가 더러워짐.2\n\n아나콘다나 도커를 사용하고 싶지 않다면, 코랩이나 캐글노트북 등 클라우드 컴퓨터에서 작업하는걸 추천\n클라우드 컴퓨터에서 작업하면 본인의 컴퓨터를 정리된 상태로 유지될 수 있다는 장점이 있지만, 새로운 컴퓨터를 할당받을때 마다 autogluon같은 패키지를 계속 설치해야하는 불편함이 있음.\n\n\n\n1 이건 아직 설명안했어요2 패키지 간의 종속성때문에 결국 고장남"
  }
]